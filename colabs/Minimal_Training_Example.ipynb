{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bX6hBigQc-AA"
      },
      "outputs": [],
      "source": [
        "# !pip install rlds dm-reverb[tensorflow]\n",
        "# !pip install flax jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wIwwvkYwpqld"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-20 19:21:01.783680: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-20 19:21:01.783796: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-20 19:21:01.807726: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-20 19:21:02.442322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-07-20 19:21:03.514846: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "# @title Imports\n",
        "\n",
        "from typing import Any, Callable, Dict, Optional, Sequence, Union, NamedTuple, Tuple\n",
        "\n",
        "import copy\n",
        "import enum\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "from jax.experimental import mesh_utils\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import rlds\n",
        "import reverb\n",
        "from rlds import transformations\n",
        "import tensorflow_datasets as tfds\n",
        "import tree\n",
        "import time\n",
        "\n",
        "import abc\n",
        "import dataclasses\n",
        "import math\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from rlds import rlds_types\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import tensorflow_datasets as tfds\n",
        "import functools\n",
        "from typing import Callable, Sequence\n",
        "import matplotlib.pyplot as plt\n",
        "from flax.training import checkpoints\n",
        "from flax import traverse_util\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import csv\n",
        "\n",
        "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
        "\n",
        "jax.config.update(\"jax_disable_jit\", False)\n",
        "\n",
        "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"true\"\n",
        "# os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".95\"\n",
        "# os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj_StgNflaJh"
      },
      "source": [
        "# Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sFD03KnSxvV8"
      },
      "outputs": [],
      "source": [
        "# @title Transformation definitions\n",
        "\n",
        "# For an example usage of the code in this code cell, please take a look at the\n",
        "# dataset colab at the link below:\n",
        "# https://colab.research.google.com/github/google-deepmind/open_x_embodiment/blob/main/colabs/Open_X_Embodiment_Datasets.ipynb\n",
        "\n",
        "def _features_to_tensor_spec(\n",
        "    feature: tfds.features.FeatureConnector\n",
        ") -> tf.TensorSpec:\n",
        "  \"\"\"Converts a tfds Feature into a TensorSpec.\"\"\"\n",
        "\n",
        "  def _get_feature_spec(nested_feature: tfds.features.FeatureConnector):\n",
        "    if isinstance(nested_feature, tf.DType):\n",
        "      return tf.TensorSpec(shape=(), dtype=nested_feature)\n",
        "    else:\n",
        "      return nested_feature.get_tensor_spec()\n",
        "\n",
        "  # FeaturesDict can sometimes be a plain dictionary, so we use tf.nest to\n",
        "  # make sure we deal with the nested structure.\n",
        "  return tf.nest.map_structure(_get_feature_spec, feature)\n",
        "\n",
        "\n",
        "def _encoded_feature(feature: Optional[tfds.features.FeatureConnector],\n",
        "                     image_encoding: Optional[str],\n",
        "                     tensor_encoding: Optional[tfds.features.Encoding]):\n",
        "  \"\"\"Adds encoding to Images and/or Tensors.\"\"\"\n",
        "  def _apply_encoding(feature: tfds.features.FeatureConnector,\n",
        "                      image_encoding: Optional[str],\n",
        "                      tensor_encoding: Optional[tfds.features.Encoding]):\n",
        "    if image_encoding and isinstance(feature, tfds.features.Image):\n",
        "      return tfds.features.Image(\n",
        "          shape=feature.shape,\n",
        "          dtype=feature.dtype,\n",
        "          use_colormap=feature.use_colormap,\n",
        "          encoding_format=image_encoding)\n",
        "    if tensor_encoding and isinstance(\n",
        "        feature, tfds.features.Tensor) and feature.dtype != tf.string:\n",
        "      return tfds.features.Tensor(\n",
        "          shape=feature.shape, dtype=feature.dtype, encoding=tensor_encoding)\n",
        "    return feature\n",
        "\n",
        "  if not feature:\n",
        "    return None\n",
        "  return tf.nest.map_structure(\n",
        "      lambda x: _apply_encoding(x, image_encoding, tensor_encoding), feature)\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class RLDSSpec(metaclass=abc.ABCMeta):\n",
        "  \"\"\"Specification of an RLDS Dataset.\n",
        "\n",
        "  It is used to hold a spec that can be converted into a TFDS DatasetInfo or\n",
        "  a `tf.data.Dataset` spec.\n",
        "  \"\"\"\n",
        "  observation_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  action_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  reward_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  discount_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  step_metadata_info: Optional[tfds.features.FeaturesDict] = None\n",
        "  episode_metadata_info: Optional[tfds.features.FeaturesDict] = None\n",
        "\n",
        "  def step_tensor_spec(self) -> Dict[str, tf.TensorSpec]:\n",
        "    \"\"\"Obtains the TensorSpec of an RLDS step.\"\"\"\n",
        "    step = {}\n",
        "    if self.observation_info:\n",
        "      step[rlds_types.OBSERVATION] = _features_to_tensor_spec(\n",
        "          self.observation_info)\n",
        "    if self.action_info:\n",
        "      step[rlds_types.ACTION] = _features_to_tensor_spec(\n",
        "          self.action_info)\n",
        "    if self.discount_info:\n",
        "      step[rlds_types.DISCOUNT] = _features_to_tensor_spec(\n",
        "          self.discount_info)\n",
        "    if self.reward_info:\n",
        "      step[rlds_types.REWARD] = _features_to_tensor_spec(\n",
        "          self.reward_info)\n",
        "    if self.step_metadata_info:\n",
        "      for k, v in self.step_metadata_info.items():\n",
        "        step[k] = _features_to_tensor_spec(v)\n",
        "\n",
        "    step[rlds_types.IS_FIRST] = tf.TensorSpec(shape=(), dtype=bool)\n",
        "    step[rlds_types.IS_LAST] = tf.TensorSpec(shape=(), dtype=bool)\n",
        "    step[rlds_types.IS_TERMINAL] = tf.TensorSpec(shape=(), dtype=bool)\n",
        "    return step\n",
        "\n",
        "  def episode_tensor_spec(self) -> Dict[str, tf.TensorSpec]:\n",
        "    \"\"\"Obtains the TensorSpec of an RLDS step.\"\"\"\n",
        "    episode = {}\n",
        "    episode[rlds_types.STEPS] = tf.data.DatasetSpec(\n",
        "        element_spec=self.step_tensor_spec())\n",
        "    if self.episode_metadata_info:\n",
        "      for k, v in self.episode_metadata_info.items():\n",
        "        episode[k] = _features_to_tensor_spec(v)\n",
        "    return episode\n",
        "\n",
        "  def to_dataset_config(\n",
        "      self,\n",
        "      name: str,\n",
        "      image_encoding: Optional[str] = None,\n",
        "      tensor_encoding: Optional[tfds.features.Encoding] = None,\n",
        "      citation: Optional[str] = None,\n",
        "      homepage: Optional[str] = None,\n",
        "      description: Optional[str] = None,\n",
        "      overall_description: Optional[str] = None,\n",
        "  ) -> tfds.rlds.rlds_base.DatasetConfig:\n",
        "    \"\"\"Obtains the DatasetConfig for TFDS from the Spec.\"\"\"\n",
        "    return tfds.rlds.rlds_base.DatasetConfig(\n",
        "        name=name,\n",
        "        description=description,\n",
        "        overall_description=overall_description,\n",
        "        homepage=homepage,\n",
        "        citation=citation,\n",
        "        observation_info=_encoded_feature(self.observation_info, image_encoding,\n",
        "                                          tensor_encoding),\n",
        "        action_info=_encoded_feature(self.action_info, image_encoding,\n",
        "                                     tensor_encoding),\n",
        "        reward_info=_encoded_feature(self.reward_info, image_encoding,\n",
        "                                     tensor_encoding),\n",
        "        discount_info=_encoded_feature(self.discount_info, image_encoding,\n",
        "                                       tensor_encoding),\n",
        "        step_metadata_info=_encoded_feature(self.step_metadata_info,\n",
        "                                            image_encoding, tensor_encoding),\n",
        "        episode_metadata_info=_encoded_feature(self.episode_metadata_info,\n",
        "                                               image_encoding, tensor_encoding))\n",
        "\n",
        "  def to_features_dict(self):\n",
        "    \"\"\"Returns a TFDS FeaturesDict representing the dataset config.\"\"\"\n",
        "    step_config = {\n",
        "        rlds_types.IS_FIRST: tf.bool,\n",
        "        rlds_types.IS_LAST: tf.bool,\n",
        "        rlds_types.IS_TERMINAL: tf.bool,\n",
        "    }\n",
        "\n",
        "    if self.observation_info:\n",
        "      step_config[rlds_types.OBSERVATION] = self.observation_info\n",
        "    if self.action_info:\n",
        "      step_config[rlds_types.ACTION] = self.action_info\n",
        "    if self.discount_info:\n",
        "      step_config[rlds_types.DISCOUNT] = self.discount_info\n",
        "    if self.reward_info:\n",
        "      step_config[rlds_types.REWARD] = self.reward_info\n",
        "\n",
        "    if self.step_metadata_info:\n",
        "      for k, v in self.step_metadata_info.items():\n",
        "        step_config[k] = v\n",
        "\n",
        "    if self.episode_metadata_info:\n",
        "      return tfds.features.FeaturesDict({\n",
        "          rlds_types.STEPS: tfds.features.Dataset(step_config),\n",
        "          **self.episode_metadata_info,\n",
        "      })\n",
        "    else:\n",
        "      return tfds.features.FeaturesDict({\n",
        "          rlds_types.STEPS: tfds.features.Dataset(step_config),\n",
        "      })\n",
        "\n",
        "RLDS_SPEC = RLDSSpec\n",
        "TENSOR_SPEC = Union[tf.TensorSpec, dict[str, tf.TensorSpec]]\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class TrajectoryTransform(metaclass=abc.ABCMeta):\n",
        "  \"\"\"Specification the TrajectoryTransform applied to a dataset of episodes.\n",
        "\n",
        "  A TrajectoryTransform is a set of rules transforming a dataset\n",
        "  of RLDS episodes to a dataset of trajectories.\n",
        "  This involves three distinct stages:\n",
        "  - An optional `episode_to_steps_map_fn(episode)` is called at the episode\n",
        "    level, and can be used to select or modify steps.\n",
        "    - Augmentation: an `episode_key` could be propagated to `steps` for\n",
        "      debugging.\n",
        "    - Selection: Particular steps can be selected.\n",
        "    - Stripping: Features can be removed from steps. Prefer using `step_map_fn`.\n",
        "  - An optional `step_map_fn` is called at the flattened steps dataset for each\n",
        "    step, and can be used to featurize a step, e.g. add/remove features, or\n",
        "    augument images\n",
        "  - A `pattern` leverages DM patterns to set a rule of slicing an episode to a\n",
        "    dataset of overlapping trajectories.\n",
        "\n",
        "  Importantly, each TrajectoryTransform must define a `expected_tensor_spec`\n",
        "  which specifies a nested TensorSpec of the resulting dataset. This is what\n",
        "  this TrajectoryTransform will produce, and can be used as an interface with\n",
        "  a neural network.\n",
        "  \"\"\"\n",
        "  episode_dataset_spec: RLDS_SPEC\n",
        "  episode_to_steps_fn_dataset_spec: RLDS_SPEC\n",
        "  steps_dataset_spec: Any\n",
        "  pattern: reverb.structured_writer.Pattern\n",
        "  episode_to_steps_map_fn: Any\n",
        "  expected_tensor_spec: TENSOR_SPEC\n",
        "  step_map_fn: Optional[Any] = None\n",
        "\n",
        "  def get_for_cached_trajectory_transform(self):\n",
        "    \"\"\"Creates a copy of this traj transform to use with caching.\n",
        "\n",
        "    The returned TrajectoryTransfrom copy will be initialized with the default\n",
        "    version of the `episode_to_steps_map_fn`, because the effect of that\n",
        "    function has already been materialized in the cached copy of the dataset.\n",
        "    Returns:\n",
        "      trajectory_transform: A copy of the TrajectoryTransform with overridden\n",
        "        `episode_to_steps_map_fn`.\n",
        "    \"\"\"\n",
        "    traj_copy = dataclasses.replace(self)\n",
        "    traj_copy.episode_dataset_spec = traj_copy.episode_to_steps_fn_dataset_spec\n",
        "    traj_copy.episode_to_steps_map_fn = lambda e: e[rlds_types.STEPS]\n",
        "    return traj_copy\n",
        "\n",
        "  def transform_episodic_rlds_dataset(self, episodes_dataset: tf.data.Dataset):\n",
        "    \"\"\"Applies this TrajectoryTransform to the dataset of episodes.\"\"\"\n",
        "\n",
        "    # Convert the dataset of episodes to the dataset of steps.\n",
        "    steps_dataset = episodes_dataset.map(\n",
        "        self.episode_to_steps_map_fn, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).flat_map(lambda x: x)\n",
        "\n",
        "    return self._create_pattern_dataset(steps_dataset)\n",
        "\n",
        "  def transform_steps_rlds_dataset(\n",
        "      self, steps_dataset: tf.data.Dataset\n",
        "  ) -> tf.data.Dataset:\n",
        "    \"\"\"Applies this TrajectoryTransform to the dataset of episode steps.\"\"\"\n",
        "\n",
        "    return self._create_pattern_dataset(steps_dataset)\n",
        "\n",
        "  def create_test_dataset(\n",
        "      self,\n",
        "  ) -> tf.data.Dataset:\n",
        "    \"\"\"Creates a test dataset of trajectories.\n",
        "\n",
        "    It is guaranteed that the structure of this dataset will be the same as\n",
        "    when flowing real data. Hence this is a useful construct for tests or\n",
        "    initialization of JAX models.\n",
        "    Returns:\n",
        "      dataset: A test dataset made of zeros structurally identical to the\n",
        "        target dataset of trajectories.\n",
        "    \"\"\"\n",
        "    zeros = transformations.zeros_from_spec(self.expected_tensor_spec)\n",
        "\n",
        "    return tf.data.Dataset.from_tensors(zeros)\n",
        "\n",
        "  def _create_pattern_dataset(\n",
        "      self, steps_dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
        "    \"\"\"Create PatternDataset from the `steps_dataset`.\"\"\"\n",
        "    config = create_structured_writer_config('temp', self.pattern)\n",
        "\n",
        "    # Further transform each step if the `step_map_fn` is provided.\n",
        "    if self.step_map_fn:\n",
        "      steps_dataset = steps_dataset.map(self.step_map_fn)\n",
        "    pattern_dataset = reverb.PatternDataset(\n",
        "        input_dataset=steps_dataset,\n",
        "        configs=[config],\n",
        "        respect_episode_boundaries=True,\n",
        "        is_end_of_episode=lambda x: x[rlds_types.IS_LAST])\n",
        "    return pattern_dataset\n",
        "\n",
        "\n",
        "class TrajectoryTransformBuilder(object):\n",
        "  \"\"\"Facilitates creation of the `TrajectoryTransform`.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               dataset_spec: RLDS_SPEC,\n",
        "               episode_to_steps_map_fn=lambda e: e[rlds_types.STEPS],\n",
        "               step_map_fn=None,\n",
        "               pattern_fn=None,\n",
        "               expected_tensor_spec=None):\n",
        "    self._rds_dataset_spec = dataset_spec\n",
        "    self._steps_spec = None\n",
        "    self._episode_to_steps_map_fn = episode_to_steps_map_fn\n",
        "    self._step_map_fn = step_map_fn\n",
        "    self._pattern_fn = pattern_fn\n",
        "    self._expected_tensor_spec = expected_tensor_spec\n",
        "\n",
        "  def build(self,\n",
        "            validate_expected_tensor_spec: bool = True) -> TrajectoryTransform:\n",
        "    \"\"\"Creates `TrajectoryTransform` from a `TrajectoryTransformBuilder`.\"\"\"\n",
        "\n",
        "    if validate_expected_tensor_spec and self._expected_tensor_spec is None:\n",
        "      raise ValueError('`expected_tensor_spec` must be set.')\n",
        "\n",
        "    episode_ds = zero_episode_dataset_from_spec(self._rds_dataset_spec)\n",
        "\n",
        "    steps_ds = episode_ds.flat_map(self._episode_to_steps_map_fn)\n",
        "\n",
        "    episode_to_steps_fn_dataset_spec = self._rds_dataset_spec\n",
        "\n",
        "    if self._step_map_fn is not None:\n",
        "      steps_ds = steps_ds.map(self._step_map_fn)\n",
        "\n",
        "    zeros_spec = transformations.zeros_from_spec(steps_ds.element_spec)  # pytype: disable=wrong-arg-types\n",
        "\n",
        "    ref_step = reverb.structured_writer.create_reference_step(zeros_spec)\n",
        "\n",
        "    pattern = self._pattern_fn(ref_step)\n",
        "\n",
        "    steps_ds_spec = steps_ds.element_spec\n",
        "\n",
        "    target_tensor_structure = create_reverb_table_signature(\n",
        "        'temp_table', steps_ds_spec, pattern)\n",
        "\n",
        "    if (validate_expected_tensor_spec and\n",
        "        self._expected_tensor_spec != target_tensor_structure):\n",
        "      raise RuntimeError(\n",
        "          'The tensor spec of the TrajectoryTransform doesn\\'t '\n",
        "          'match the expected spec.\\n'\n",
        "          'Expected:\\n%s\\nActual:\\n%s\\n' %\n",
        "          (str(self._expected_tensor_spec).replace('TensorSpec',\n",
        "                                                   'tf.TensorSpec'),\n",
        "           str(target_tensor_structure).replace('TensorSpec', 'tf.TensorSpec')))\n",
        "\n",
        "    return TrajectoryTransform(\n",
        "        episode_dataset_spec=self._rds_dataset_spec,\n",
        "        episode_to_steps_fn_dataset_spec=episode_to_steps_fn_dataset_spec,\n",
        "        steps_dataset_spec=steps_ds_spec,\n",
        "        pattern=pattern,\n",
        "        episode_to_steps_map_fn=self._episode_to_steps_map_fn,\n",
        "        step_map_fn=self._step_map_fn,\n",
        "        expected_tensor_spec=target_tensor_structure)\n",
        "\n",
        "def zero_episode_dataset_from_spec(rlds_spec: RLDS_SPEC):\n",
        "  \"\"\"Creates a zero valued dataset of episodes for the given RLDS Spec.\"\"\"\n",
        "\n",
        "  def add_steps(episode, step_spec):\n",
        "    episode[rlds_types.STEPS] = transformations.zero_dataset_like(\n",
        "        tf.data.DatasetSpec(step_spec))\n",
        "    if 'fake' in episode:\n",
        "      del episode['fake']\n",
        "    return episode\n",
        "\n",
        "  episode_without_steps_spec = {\n",
        "      k: v\n",
        "      for k, v in rlds_spec.episode_tensor_spec().items()\n",
        "      if k != rlds_types.STEPS\n",
        "  }\n",
        "\n",
        "  if episode_without_steps_spec:\n",
        "    episodes_dataset = transformations.zero_dataset_like(\n",
        "        tf.data.DatasetSpec(episode_without_steps_spec))\n",
        "  else:\n",
        "    episodes_dataset = tf.data.Dataset.from_tensors({'fake': ''})\n",
        "\n",
        "  episodes_dataset_with_steps = episodes_dataset.map(\n",
        "      lambda episode: add_steps(episode, rlds_spec.step_tensor_spec()))\n",
        "  return episodes_dataset_with_steps\n",
        "\n",
        "\n",
        "def create_reverb_table_signature(table_name: str, steps_dataset_spec,\n",
        "                                  pattern: reverb.structured_writer.Pattern) -> reverb.reverb_types.SpecNest:\n",
        "  config = create_structured_writer_config(table_name, pattern)\n",
        "  reverb_table_spec = reverb.structured_writer.infer_signature(\n",
        "      [config], steps_dataset_spec)\n",
        "  return reverb_table_spec\n",
        "\n",
        "\n",
        "def create_structured_writer_config(table_name: str,\n",
        "                                    pattern: reverb.structured_writer.Pattern) -> Any:\n",
        "  config = reverb.structured_writer.create_config(\n",
        "      pattern=pattern, table=table_name, conditions=[])\n",
        "  return config\n",
        "\n",
        "def n_step_pattern_builder(n: int) -> Any:\n",
        "  \"\"\"Creates trajectory of length `n` from all fields of a `ref_step`.\"\"\"\n",
        "\n",
        "  def transform_fn(ref_step):\n",
        "    traj = {}\n",
        "    for key in ref_step:\n",
        "      if isinstance(ref_step[key], dict):\n",
        "        transformed_entry = tree.map_structure(lambda ref_node: ref_node[-n:],\n",
        "                                               ref_step[key])\n",
        "        traj[key] = transformed_entry\n",
        "      else:\n",
        "        traj[key] = ref_step[key][-n:]\n",
        "\n",
        "    return traj\n",
        "\n",
        "  return transform_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "3yS0kizwpaLS"
      },
      "outputs": [],
      "source": [
        "# @title Shared map functions\n",
        "\n",
        "StepFnMapType = Callable[[rlds.Step, rlds.Step], None]\n",
        "\n",
        "\n",
        "def resize_to_resolution(\n",
        "    image: Union[tf.Tensor, np.ndarray],\n",
        "    target_width: int = 320,\n",
        "    target_height: int = 256,\n",
        "    to_numpy: bool = True,\n",
        ") -> Union[tf.Tensor, np.ndarray]:\n",
        "  \"\"\"Resizes image and casts to uint8.\"\"\"\n",
        "  image = tf.image.resize_with_pad(\n",
        "      image,\n",
        "      target_width=target_width,\n",
        "      target_height=target_height,\n",
        "  )\n",
        "  image = tf.cast(image, tf.uint8)\n",
        "  if to_numpy:\n",
        "    image = image.numpy()\n",
        "  return image\n",
        "\n",
        "\n",
        "def map_observation(\n",
        "    to_step: rlds.Step,\n",
        "    from_step: rlds.Step,\n",
        "    from_image_feature_names: tuple[str, ...] = ('image',),\n",
        "    to_image_feature_names: tuple[str, ...] = ('image',),\n",
        "    resize: bool = True,\n",
        ") -> None:\n",
        "  \"\"\"Map observation to model observation spec.\"\"\"\n",
        "\n",
        "  to_step[rlds.OBSERVATION]['natural_language_embedding'] = from_step[\n",
        "      rlds.OBSERVATION\n",
        "  ]['natural_language_embedding']\n",
        "\n",
        "  for from_feature_name, to_feature_name in zip(\n",
        "      from_image_feature_names, to_image_feature_names\n",
        "  ):\n",
        "    if resize:\n",
        "      to_step['observation'][to_feature_name] = resize_to_resolution(\n",
        "          from_step['observation'][from_feature_name],\n",
        "          to_numpy=False,\n",
        "          target_width=320,\n",
        "          target_height=256,\n",
        "      )\n",
        "\n",
        "\n",
        "def terminate_bool_to_act(terminate_episode: tf.Tensor) -> tf.Tensor:\n",
        "  return tf.cond(\n",
        "      terminate_episode == tf.constant(1.0),\n",
        "      lambda: tf.constant([1, 0, 0], dtype=tf.int32),\n",
        "      lambda: tf.constant([0, 1, 0], dtype=tf.int32),\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "A0IBN-1Oo-ur"
      },
      "outputs": [],
      "source": [
        "# @title RT-1 action map function\n",
        "\n",
        "\n",
        "def rt_1_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  to_step[rlds.ACTION] = from_step[rlds.ACTION]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "idQIobyEuuAx"
      },
      "outputs": [],
      "source": [
        "# @title Bridge action map function\n",
        "\n",
        "def rescale_action_with_bound(\n",
        "    actions: tf.Tensor,\n",
        "    low: float,\n",
        "    high: float,\n",
        "    safety_margin: float = 0,\n",
        "    post_scaling_max: float = 1.0,\n",
        "    post_scaling_min: float = -1.0,\n",
        ") -> tf.Tensor:\n",
        "  \"\"\"Formula taken from https://stats.stackexchange.com/questions/281162/scale-a-number-between-a-range.\"\"\"\n",
        "  resc_actions = (actions - low) / (high - low) * (\n",
        "      post_scaling_max - post_scaling_min\n",
        "  ) + post_scaling_min\n",
        "  return tf.clip_by_value(\n",
        "      resc_actions,\n",
        "      post_scaling_min + safety_margin,\n",
        "      post_scaling_max - safety_margin,\n",
        "  )\n",
        "\n",
        "\n",
        "def _rescale_action(action):\n",
        "  \"\"\"Rescales action.\"\"\"\n",
        "\n",
        "  # Values taken from\n",
        "  # https://github.com/Asap7772/rt1_eval/blob/2fad77e9bf4def2ef82604d445270f83475e9726/kitchen_eval/rt1_wrapper.py#L39\n",
        "  action['world_vector'] = rescale_action_with_bound(\n",
        "      action['world_vector'],\n",
        "      low=-0.05,\n",
        "      high=0.05,\n",
        "      safety_margin=0.01,\n",
        "      post_scaling_max=1.75,\n",
        "      post_scaling_min=-1.75,\n",
        "  )\n",
        "  action['rotation_delta'] = rescale_action_with_bound(\n",
        "      action['rotation_delta'],\n",
        "      low=-0.25,\n",
        "      high=0.25,\n",
        "      safety_margin=0.01,\n",
        "      post_scaling_max=1.4,\n",
        "      post_scaling_min=-1.4,\n",
        "  )\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "def bridge_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps Bridge dataset action to action expected by the model.\"\"\"\n",
        "\n",
        "  to_step['action']['world_vector'] = from_step['action']['world_vector']\n",
        "  to_step['action']['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step['action']['terminate_episode']\n",
        "  )\n",
        "\n",
        "  to_step['action']['rotation_delta'] = from_step['action']['rotation_delta']\n",
        "\n",
        "  open_gripper = from_step['action']['open_gripper']\n",
        "\n",
        "  possible_values = tf.constant([True, False], dtype=tf.bool)\n",
        "  eq = tf.equal(possible_values, open_gripper)\n",
        "\n",
        "  assert_op = tf.Assert(tf.reduce_any(eq), [open_gripper])\n",
        "\n",
        "  with tf.control_dependencies([assert_op]):\n",
        "    to_step['action']['gripper_closedness_action'] = tf.cond(\n",
        "        # for open_gripper in bridge dataset,\n",
        "        # 0 is fully closed and 1 is fully open\n",
        "        open_gripper,\n",
        "        # for Fractal data,\n",
        "        # gripper_closedness_action = -1 means opening the gripper and\n",
        "        # gripper_closedness_action = 1 means closing the gripper.\n",
        "        lambda: tf.constant([-1.0], dtype=tf.float32),\n",
        "        lambda: tf.constant([1.0], dtype=tf.float32),\n",
        "    )\n",
        "\n",
        "  to_step['action'] = _rescale_action(to_step['action'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "J6BAjveUpNsR"
      },
      "outputs": [],
      "source": [
        "# @title Task Agnostic Robot Play map function\n",
        "\n",
        "def taco_play_rescale_actions_by_bounds(actions, lows, highs, safety_margin=0.01):\n",
        "  # Actions is SymbolicTensor, shape (N,)\n",
        "  resc_actions = (actions - lows) / (highs - lows) * 2 - 1\n",
        "  return tf.clip_by_value(resc_actions, -1 + safety_margin, 1 - safety_margin)\n",
        "\n",
        "\n",
        "\n",
        "def taco_play_rescale_action(action):\n",
        "  \"\"\"Rescales actions based on measured per dimension ranges.\"\"\"\n",
        "  # Rotation Delta\n",
        "  rd_lows = tf.constant([-3.2, -0.8, -1.8])\n",
        "  rd_highs = tf.constant([3.2, 0.2, 2.5])\n",
        "  action['rotation_delta'] = taco_play_rescale_actions_by_bounds(\n",
        "      action['rotation_delta'], lows=rd_lows, highs=rd_highs\n",
        "  )\n",
        "\n",
        "  # World Vector\n",
        "  wv_lows = tf.constant([0.0, -0.5, 0.0])\n",
        "  wv_highs = tf.constant([0.8, 0.7, 0.6])\n",
        "  action['world_vector'] = taco_play_rescale_actions_by_bounds(\n",
        "      action['world_vector'], lows=wv_lows, highs=wv_highs\n",
        "  )\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "def taco_play_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps Taco Play Panda action to action expected by the model.\"\"\"\n",
        "\n",
        "  # 'actions' is absolute, and not relative action. There is relative action in\n",
        "  # the materialized dataset that can be used for training (not yet supported).\n",
        "  actions = from_step[rlds.ACTION]['actions']\n",
        "  to_step[rlds.ACTION]['world_vector'] = actions[:3]\n",
        "  to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step[rlds.ACTION]['terminate_episode']\n",
        "  )\n",
        "\n",
        "  to_step[rlds.ACTION]['rotation_delta'] = actions[3:6]\n",
        "  to_step[rlds.ACTION]['gripper_closedness_action'] = tf.expand_dims(\n",
        "      actions[6], axis=-1\n",
        "  )\n",
        "\n",
        "  to_step[rlds.ACTION] = _rescale_action(to_step[rlds.ACTION])\n",
        "\n",
        "\n",
        "taco_play_map_observation = functools.partial(\n",
        "    map_observation,\n",
        "    from_image_feature_names=('rgb_static',),\n",
        "    to_image_feature_names=('image',))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "Lv5YtakupoN4"
      },
      "outputs": [],
      "source": [
        "# @title Jaco Play map function\n",
        "\n",
        "def _normalize(value, mean, std):\n",
        "  return (value - mean) / std\n",
        "\n",
        "\n",
        "def jaco_play_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  to_step['action']['world_vector'] = _normalize(\n",
        "      from_step['action']['world_vector'],\n",
        "      mean=tf.constant(\n",
        "          [0.00096585, -0.00580069, -0.00395066], dtype=tf.float32\n",
        "      ),\n",
        "      std=tf.constant([0.12234575, 0.09676983, 0.11155209], dtype=tf.float32),\n",
        "  )\n",
        "  to_step['action']['gripper_closedness_action'] = from_step['action'][\n",
        "      'gripper_closedness_action'\n",
        "  ]\n",
        "  to_step['action']['terminate_episode'] = from_step['action'][\n",
        "      'terminate_episode'\n",
        "  ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "_6F3XPY-qFbf"
      },
      "outputs": [],
      "source": [
        "# @title Cable Routing map function\n",
        "\n",
        "def berkeley_cable_routing_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "\n",
        "  to_step['action']['world_vector'] = from_step['action']['world_vector']\n",
        "  to_step['action']['rotation_delta'] = from_step['action']['rotation_delta']\n",
        "\n",
        "  to_step['action']['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step['action']['terminate_episode']\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "caiR409BqLz-"
      },
      "outputs": [],
      "source": [
        "# @title RoboTurk map function\n",
        "\n",
        "def roboturk_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  to_step[rlds.ACTION]['world_vector'] = from_step[rlds.ACTION]['world_vector']\n",
        "  to_step[rlds.ACTION]['gripper_closedness_action'] = from_step[rlds.ACTION][\n",
        "      'gripper_closedness_action'\n",
        "  ]\n",
        "  to_step[rlds.ACTION]['rotation_delta'] = from_step[rlds.ACTION]['rotation_delta']\n",
        "  to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step[rlds.ACTION]['terminate_episode']\n",
        "  )\n",
        "\n",
        "\n",
        "roboturk_map_observation = functools.partial(\n",
        "    map_observation,\n",
        "    from_image_feature_names=('front_rgb',),\n",
        "    to_image_feature_names=('image',)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "1b4OUQ7Wqf6-"
      },
      "outputs": [],
      "source": [
        "# @title NYU VINN map function\n",
        "\n",
        "def nyu_door_opening_surprising_effectiveness_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps dataset action to action expected by the model.\"\"\"\n",
        "\n",
        "  # The world vector as existed in the dataset on disk ranges from -0.07 to 0.07\n",
        "  # We scale by 20.0 so that the action spans the limit of the world_vector\n",
        "  # action, from -2.0 to 2.0.\n",
        "  to_step['action']['world_vector'] = from_step['action']['world_vector'] * 20.0\n",
        "\n",
        "  # Similarly, the rotation_delta in the dataset on disk ranges from -0.07 to\n",
        "  # 0.07.\n",
        "  # We scale by 15.0 so that the rotation_delta almost spans the limit of\n",
        "  # rotation_delta, from -pi/2 to pi/2.\n",
        "  to_step['action']['rotation_delta'] = (\n",
        "      from_step['action']['rotation_delta'] * 15.0\n",
        "  )\n",
        "\n",
        "  to_step['action']['gripper_closedness_action'] = (\n",
        "      from_step['action']['gripper_closedness_action']\n",
        "  )\n",
        "\n",
        "  to_step['action']['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step['action']['terminate_episode']\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "sseg7Qyiqjw2"
      },
      "outputs": [],
      "source": [
        "# @title Austin VIOLA map function\n",
        "\n",
        "def viola_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps dataset action to action expected by the model.\"\"\"\n",
        "\n",
        "  # The world vector as existed in the dataset on disk ranges from -1.0 to 1.0\n",
        "  # We scale by 1.75 so that the action better spans the limit of the\n",
        "  # world_vector action, from -2.0 to 2.0.\n",
        "  to_step[rlds.ACTION]['world_vector'] = from_step[rlds.ACTION]['world_vector'] * 1.75\n",
        "  to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step[rlds.ACTION]['terminate_episode']\n",
        "  )\n",
        "\n",
        "  # Similarly, the rotation_delta in the dataset on disk ranges from -0.4 to 0.4\n",
        "  # We scale by 3.0 so that the rotation_delta almost spans the limit of\n",
        "  # rotation_delta, from -pi/2 to pi/2.\n",
        "  to_step[rlds.ACTION]['rotation_delta'] = (\n",
        "      from_step[rlds.ACTION]['rotation_delta'] * 3.0\n",
        "  )\n",
        "\n",
        "  gripper_closedness_action = from_step[rlds.ACTION]['gripper_closedness_action']\n",
        "\n",
        "  # There can be 0.0 values because of zero padding\n",
        "  possible_values = tf.constant([-1.0, 1.0, 0.0], dtype=tf.float32)\n",
        "  eq = tf.equal(possible_values, gripper_closedness_action)\n",
        "\n",
        "  # Assert that gripper_closedness_action is one of possible_values\n",
        "  assert_op = tf.Assert(tf.reduce_any(eq), [gripper_closedness_action])\n",
        "\n",
        "  with tf.control_dependencies([assert_op]):\n",
        "    gripper_closedness_action = tf.expand_dims(\n",
        "        gripper_closedness_action, axis=-1\n",
        "    )\n",
        "    to_step[rlds.ACTION]['gripper_closedness_action'] = gripper_closedness_action\n",
        "\n",
        "\n",
        "viola_map_observation = functools.partial(\n",
        "    map_observation,\n",
        "    from_image_feature_names = ('agentview_rgb',),\n",
        "    to_image_feature_names = ('image',),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "8hv4j5e0qnRm"
      },
      "outputs": [],
      "source": [
        "# @title Berkeley Autolab UR5 map function\n",
        "\n",
        "def berkeley_autolab_ur5_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps Berkeley Autolab UR5 action to action expected by the model.\"\"\"\n",
        "\n",
        "  # The world vector as existed in the dataset on disk ranges from -0.02 to 0.02\n",
        "  # We scale by 100.0 so that the action spans the limit of the world_vector\n",
        "  # action, from -2.0 to 2.0.\n",
        "  to_step[rlds.ACTION]['world_vector'] = (\n",
        "      from_step[rlds.ACTION]['world_vector'] * 100.0\n",
        "  )\n",
        "  to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step[rlds.ACTION]['terminate_episode']\n",
        "  )\n",
        "\n",
        "  # Similarly, the rotation_delta in the dataset on disk ranges from -0.07 to\n",
        "  # 0.07\n",
        "  # We scale by 15.0 so that the rotation_delta almost spans the limit of\n",
        "  # rotation_delta, from -pi/2 to pi/2.\n",
        "  to_step[rlds.ACTION]['rotation_delta'] = (\n",
        "      from_step[rlds.ACTION]['rotation_delta'] * 15.0\n",
        "  )\n",
        "  to_step[rlds.ACTION]['gripper_closedness_action'] = tf.expand_dims(\n",
        "      from_step[rlds.ACTION]['gripper_closedness_action'], axis=0\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "9bljPl3Eqp7y"
      },
      "outputs": [],
      "source": [
        "# @title TOTO\n",
        "\n",
        "def toto_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps TOTO action to action expected by the model.\"\"\"\n",
        "\n",
        "  print(from_step['action'].keys())\n",
        "\n",
        "  # The world vector as existed in the dataset on disk ranges from -0.7 to 0.7\n",
        "  # We scale by 2.0 so that the action better spans the limit of the\n",
        "  # world_vector action, from -2.0 to 2.0.\n",
        "  to_step['action']['world_vector'] = from_step['action']['world_vector'] * 2.0\n",
        "  to_step['action']['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step['action']['terminate_episode']\n",
        "  )\n",
        "\n",
        "  to_step['action']['rotation_delta'] = from_step['action']['rotation_delta']\n",
        "  to_step['action']['gripper_closedness_action'] = tf.expand_dims(\n",
        "      from_step['action']['open_gripper'], axis=0\n",
        "  )\n",
        "  to_step['action']['gripper_closedness_action'] = tf.cast(\n",
        "      to_step['action']['gripper_closedness_action'], tf.float32\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title UMI\n",
        "\n",
        "def rescale_value(value, value_low, value_high, output_low, output_high, safety_margin=0.01):\n",
        "  \"\"\"Rescale value from [value_low, value_high] to [output_low, output_high].\"\"\"\n",
        "  scaled = (value - value_low) / (value_high - value_low) * (\n",
        "      output_high - output_low\n",
        "  ) + output_low\n",
        "  return min(max(scaled, output_low + safety_margin), output_high - safety_margin)\n",
        "  \n",
        "\n",
        "# def umi_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "#   \"\"\"Maps UMI action to action expected by the model.\"\"\"\n",
        "\n",
        "#   # The world vector as existed in the dataset on disk ranges from -0.3 to 0.3\n",
        "#   # We scale by 6.0 so that the action better spans the limit of the\n",
        "#   # world_vector action, from -2.0 to 2.0.\n",
        "#   to_step['action']['world_vector'] = from_step['action']['world_vector'] * 6.0\n",
        "#   to_step['action']['terminate_episode'] = terminate_bool_to_act(\n",
        "#       from_step['action']['terminate_episode']\n",
        "#   )\n",
        "\n",
        "#   # Similarly, the rotation_delta in the dataset on disk ranges from -60.0 to\n",
        "#   # 60.0\n",
        "#   # We scale by 0.025 so that the rotation_delta almost spans the limit of\n",
        "#   # rotation_delta, from -pi/2 to pi/2.\n",
        "#   to_step['action']['rotation_delta'] = from_step['action']['rotation_delta'] * 0.025\n",
        "\n",
        "#   # scale grip from space 0.02 to 0.08, to 1 to -1, with 0.02 being 1, and 0.08 being -1\n",
        "#   to_step['action']['gripper_closedness_action'] = tf.expand_dims(\n",
        "#       \n",
        "# (\n",
        "#           from_step['action']['gripper_closedness_action'],\n",
        "#           value_low=0.02,\n",
        "#           value_high=0.08,\n",
        "#           output_low=1.0,\n",
        "#           output_high=-1.0,\n",
        "#       ),\n",
        "#       axis=0,\n",
        "#   )\n",
        "  \n",
        "#   to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "#       from_step[rlds.ACTION]['terminate_episode']\n",
        "#   )\n",
        "\n",
        "def umi_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps UMI action to action expected by the model.\"\"\"\n",
        "\n",
        "  pos_x = rescale_value(\n",
        "        from_step['action']['world_vector'][0],\n",
        "        value_low=-0.5,\n",
        "        value_high=0.5,\n",
        "        output_low=-1.75,\n",
        "        output_high=1.75,\n",
        "  )\n",
        "  \n",
        "  pos_y = rescale_value(\n",
        "        from_step['action']['world_vector'][1],\n",
        "        value_low=0.2,\n",
        "        value_high=0.7,\n",
        "        output_low=-1.75,\n",
        "        output_high=1.75,\n",
        "    )\n",
        "  \n",
        "  pos_z = rescale_value(\n",
        "        from_step['action']['world_vector'][2],\n",
        "        value_low=0.2,\n",
        "        value_high=0.7,\n",
        "        output_low=-1.75,\n",
        "        output_high=1.75,\n",
        "    )\n",
        "  \n",
        "  to_step['action']['world_vector'] = from_step['action']['world_vector']\n",
        "  \n",
        "  rot_x = rescale_value(\n",
        "        from_step['action']['rotation_delta'][0],\n",
        "        value_low=45,\n",
        "        value_high=135,\n",
        "        output_low=-1.4,\n",
        "        output_high=1.4,\n",
        "    )\n",
        "  \n",
        "  rot_y = rescale_value(\n",
        "        from_step['action']['rotation_delta'][1],\n",
        "        value_low=0,\n",
        "        value_high=90,\n",
        "        output_low=-1.4,\n",
        "        output_high=1.4,\n",
        "    )\n",
        "  \n",
        "  rot_z = rescale_value(\n",
        "        from_step['action']['rotation_delta'][2],\n",
        "        value_low=0,\n",
        "        value_high=90,\n",
        "        output_low=-1.4,\n",
        "        output_high=1.4,\n",
        "    )\n",
        "  \n",
        "  to_step['action']['rotation_delta'] = [rot_x, rot_y, rot_z]\n",
        "  \n",
        "  to_step['action']['gripper_closedness_action'] = tf.expand_dims(\n",
        "      rescale_value(\n",
        "          from_step['action']['gripper_closedness_action'],\n",
        "          value_low=0.02,\n",
        "          value_high=0.08,\n",
        "          output_low=1.0,\n",
        "          output_high=-1.0,\n",
        "      ),\n",
        "      axis=0,\n",
        "  )\n",
        "  \n",
        "  to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step[rlds.ACTION]['terminate_episode']\n",
        "  )\n",
        "\n",
        "  print(\"PRINTING RESCALED ACTION...\")\n",
        "  print(to_step['action']['world_vector'][0])\n",
        "  \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0500000000000003\n"
          ]
        }
      ],
      "source": [
        "def rescale_value(value, value_low, value_high, output_low, output_high):\n",
        "  \"\"\"Rescale value from [value_low, value_high] to [output_low, output_high].\"\"\"\n",
        "  return (value - value_low) / (value_high - value_low) * (\n",
        "      output_high - output_low\n",
        "  ) + output_low\n",
        "\n",
        "test = rescale_value(0.3, -0.5, 0.5, -1.75, 1.75)\n",
        "\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "id": "ni56wdvAuv17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRINTING RESCALED ACTION...\n",
            "Tensor(\"strided_slice_6:0\", shape=(), dtype=float32)\n",
            "PRINTING RESCALED ACTION...\n",
            "Tensor(\"strided_slice_6:0\", shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# @title Create trajectory datasets\n",
        "\n",
        "def pad_initial_zero_steps(\n",
        "    steps: tf.data.Dataset, num_zero_step: int\n",
        ") -> tf.data.Dataset:\n",
        "  zero_steps = steps.take(1)\n",
        "  zero_steps = zero_steps.map(lambda x: tf.nest.map_structure(tf.zeros_like, x),\n",
        "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  zero_steps = zero_steps.repeat(num_zero_step)\n",
        "  return rlds.transformations.concatenate(zero_steps, steps)\n",
        "\n",
        "\n",
        "def pad_initial_zero_episode(episode: tf.data.Dataset, num_zero_step: int) -> tf.data.Dataset:\n",
        "  episode[rlds.STEPS] = pad_initial_zero_steps(episode[rlds.STEPS], num_zero_step)\n",
        "  return episode\n",
        "\n",
        "\n",
        "def get_trajectory_dataset(builder_dir: str, step_map_fn, trajectory_length: int, split='train[:10]'):\n",
        "  dataset_builder = tfds.builder_from_directory(builder_dir=builder_dir)\n",
        "\n",
        "  dataset_builder_episodic_dataset = dataset_builder.as_dataset(split=split)\n",
        "\n",
        "  # We need pad_initial_zero_episode because reverb.PatternDataset will skip\n",
        "  # constructing trajectories where the first trajectory_length - 1 steps are\n",
        "  # the final step in a trajectory. As such, without padding, the policies will\n",
        "  # not be trained to predict the actions in the first trajectory_length - 1\n",
        "  # steps.\n",
        "  # We are padding with num_zero_step=trajectory_length-1 steps.\n",
        "  dataset_builder_episodic_dataset = dataset_builder_episodic_dataset.map(\n",
        "      functools.partial(pad_initial_zero_episode, num_zero_step=trajectory_length-1), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  rlds_spec = RLDSSpec(\n",
        "      observation_info=dataset_builder.info.features[rlds.STEPS][rlds.OBSERVATION],\n",
        "      action_info=dataset_builder.info.features[rlds.STEPS][rlds.ACTION],\n",
        "  )\n",
        "\n",
        "  trajectory_transform = TrajectoryTransformBuilder(rlds_spec,\n",
        "                                                    step_map_fn=step_map_fn,\n",
        "                                                    pattern_fn=n_step_pattern_builder(trajectory_length)).build(validate_expected_tensor_spec=False)\n",
        "\n",
        "  trajectory_dataset = trajectory_transform.transform_episodic_rlds_dataset(dataset_builder_episodic_dataset)\n",
        "\n",
        "  return trajectory_dataset\n",
        "\n",
        "\n",
        "def step_map_fn(step, map_observation: StepFnMapType, map_action: StepFnMapType):\n",
        "  transformed_step = {}\n",
        "  transformed_step[rlds.IS_FIRST] = step[rlds.IS_FIRST]\n",
        "  transformed_step[rlds.IS_LAST] = step[rlds.IS_LAST]\n",
        "  transformed_step[rlds.IS_TERMINAL] = step[rlds.IS_TERMINAL]\n",
        "\n",
        "  transformed_step[rlds.OBSERVATION] = {}\n",
        "  transformed_step[rlds.ACTION] = {\n",
        "    'gripper_closedness_action': tf.zeros(1, dtype=tf.float32),\n",
        "    'rotation_delta': tf.zeros(3, dtype=tf.float32),\n",
        "    'terminate_episode': tf.zeros(3, dtype=tf.int32),\n",
        "    'world_vector': tf.zeros(3, dtype=tf.float32),\n",
        "    'base_displacement_vertical_rotation': tf.zeros(1, dtype=tf.float32),\n",
        "    'base_displacement_vector': tf.zeros(2, dtype=tf.float32)\n",
        "  }\n",
        "\n",
        "  map_observation(transformed_step, step)\n",
        "  map_action(transformed_step, step)\n",
        "\n",
        "  return transformed_step\n",
        "\n",
        "\n",
        "DATASET_NAME_TO_TRAJECTORY_DATASET_KWARGS = {\n",
        "    # RT-1\n",
        "    # 'rt_1': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/fractal20220817_data/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=map_observation,\n",
        "    #                                     map_action=rt_1_map_action)\n",
        "    # },\n",
        "    # # TODO: (add Qt-Opt)\n",
        "    # # Bridge\n",
        "    # 'bridge': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/bridge/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=map_observation,\n",
        "    #                                     map_action=bridge_map_action)\n",
        "    # },\n",
        "    # #  Task Agnostic Robot Play\n",
        "    # 'taco_play': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/taco_play/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=taco_play_map_observation,\n",
        "    #                                     map_action=taco_play_map_action)\n",
        "    # },\n",
        "    # # Jaco Play\n",
        "    # 'jaco_play': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/jaco_play/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=map_observation,\n",
        "    #                                     map_action=jaco_play_map_action)\n",
        "    # },\n",
        "    # # Cable Routing\n",
        "    # 'berkeley_cable_routing': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/berkeley_cable_routing/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=map_observation,\n",
        "    #                                     map_action=berkeley_cable_routing_map_action)\n",
        "    # },\n",
        "    # # Roboturk\n",
        "    # 'roboturk': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/roboturk/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=roboturk_map_observation,\n",
        "    #                                     map_action=roboturk_map_action)\n",
        "    # },\n",
        "    # # NYU VINN\n",
        "    # 'nyu_door_opening_surprising_effectiveness': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/nyu_door_opening_surprising_effectiveness/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=map_observation,\n",
        "    #                                     map_action=nyu_door_opening_surprising_effectiveness_map_action)\n",
        "    # },\n",
        "    # # Austin VIOLA\n",
        "    # 'viola': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/viola/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=viola_map_observation,\n",
        "    #                                     map_action=viola_map_action)\n",
        "    # },\n",
        "    # # Berkeley Autolab UR5\n",
        "    # 'berkeley_autolab_ur5': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/berkeley_autolab_ur5/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=map_observation,\n",
        "    #                                     map_action=berkeley_autolab_ur5_map_action)\n",
        "    # },\n",
        "    # # TODO: (add Language Table)\n",
        "    # 'toto': {\n",
        "    #     'builder_dir': 'gs://gresearch/robotics/toto/0.1.0',\n",
        "    #     'trajectory_length': 15,\n",
        "    #     'step_map_fn':functools.partial(step_map_fn,\n",
        "    #                                     map_observation=map_observation,\n",
        "    #                                     map_action=toto_map_action)\n",
        "    # },\n",
        "    'umi': {\n",
        "        'builder_dir': '~/tensorflow_datasets/episodes/1.0.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=map_observation,\n",
        "                                        map_action=umi_map_action)\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "DATASET_NAME_TO_TRAJECTORY_DATASET = {k: get_trajectory_dataset(**v) for k, v in DATASET_NAME_TO_TRAJECTORY_DATASET_KWARGS.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "id": "nNB3PBAdJr_-"
      },
      "outputs": [],
      "source": [
        "# @title Dataset weights\n",
        "\n",
        "# DATASET_NAME_TO_WEIGHTS = {\n",
        "#     'rt_1': 150,\n",
        "#     # 'rlds.kuka': 20,\n",
        "#     'bridge': 50,\n",
        "#     'taco_play': 5,\n",
        "#     'jaco_play': 20,\n",
        "#     'berkeley_cable_routing': 20,\n",
        "#     'roboturk': 10,\n",
        "#     'nyu_door_opening_surprising_effectiveness': 5,\n",
        "#     'viola': 3,\n",
        "#     'berkeley_autolab_ur5': 5,\n",
        "#     # 'language_table.language_table': 30,\n",
        "#     'toto': 5,\n",
        "# }\n",
        "\n",
        "DATASET_NAME_TO_WEIGHTS = {\n",
        "    # 'rt_1': 0,\n",
        "    # # 'rlds.kuka': 0,\n",
        "    # 'bridge': 0,\n",
        "    # 'taco_play': 0,\n",
        "    # 'jaco_play': 0,\n",
        "    # 'berkeley_cable_routing': 0,\n",
        "    # 'roboturk': 0,\n",
        "    # 'nyu_door_opening_surprising_effectiveness': 0,\n",
        "    # 'viola': 0,\n",
        "    # 'berkeley_autolab_ur5': 0,\n",
        "    # # 'language_table.language_table': 0,\n",
        "    # 'toto': 0,\n",
        "    'umi': 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "id": "TrmGytLNyxeQ"
      },
      "outputs": [],
      "source": [
        "# # @title Batch, and sample one training sample\n",
        "\n",
        "BATCH_SIZE = 6\n",
        "\n",
        "# Larger shuffle buffer leads to better performance, but consumes more RAM\n",
        "datasets = []\n",
        "weights = []\n",
        "\n",
        "for name, dataset in DATASET_NAME_TO_TRAJECTORY_DATASET.items():\n",
        "\n",
        "  # print number of episodes in each dataset\n",
        "#   print(f\"Number of episodes in {name} dataset: {len(list(dataset))}\")\n",
        "#   for i in dataset:\n",
        "#     print(i.keys())\n",
        "\n",
        "  datasets.append(dataset.shuffle(10000))\n",
        "  weights.append(float(DATASET_NAME_TO_WEIGHTS[name]))\n",
        "\n",
        "# dataset = tf.data.Dataset.sample_from_datasets(datasets, weights=weights)\n",
        "\n",
        "# size = 0\n",
        "# for i in dataset:\n",
        "#   size += 1\n",
        "# print(size)\n",
        "\n",
        "# # Larger shuffle buffer leads to better performance, but consumes more RAM\n",
        "# dataset = dataset.shuffle(1)\n",
        "\n",
        "# dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# # print(dataset.reduce(0, lambda x,_: x+1).numpy())\n",
        "\n",
        "# trajectory_dataset_iter = iter(dataset)\n",
        "\n",
        "# # get length of iter\n",
        "# # print(sum(1 for _ in trajectory_dataset_iter))\n",
        "\n",
        "# sample = next(trajectory_dataset_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "height": 273
        },
        "executionInfo": {
          "elapsed": 138,
          "status": "ok",
          "timestamp": 1702072517841,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "lfNnoFj0y8I7",
        "outputId": "c00a3f07-8e18-45a5-a0b4-199b3f0a5661"
      },
      "outputs": [],
      "source": [
        "# Image.fromarray(sample[rlds.OBSERVATION]['image'].numpy()[0][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "executionInfo": {
          "elapsed": 26,
          "status": "ok",
          "timestamp": 1702072517976,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "wfoUdbF8JGLl",
        "outputId": "611b719b-2fa7-43e0-fdea-dbe98f637b3d"
      },
      "outputs": [],
      "source": [
        "# sample[rlds.OBSERVATION]['image'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "colab": {
          "height": 572
        },
        "executionInfo": {
          "elapsed": 2551,
          "status": "ok",
          "timestamp": 1702072520605,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "JfVV1LbzJNE4",
        "outputId": "1be2e44f-20f4-42b1-82b7-48cd52ef89e4"
      },
      "outputs": [],
      "source": [
        "# @title Visualize one batch of training data\n",
        "\n",
        "# batch_size = sample[rlds.OBSERVATION]['image'].shape[0]\n",
        "# trajectory_length = sample[rlds.OBSERVATION]['image'].shape[1]\n",
        "\n",
        "# fig, axs = plt.subplots(nrows=batch_size,\n",
        "#                         ncols=trajectory_length,\n",
        "#                         figsize=(30, 10))\n",
        "\n",
        "# for batch_index in range(batch_size):\n",
        "#   for trajectory_index in range(trajectory_length):\n",
        "#     print(rlds.OBSERVATION)\n",
        "#     axs[batch_index, trajectory_index].imshow(\n",
        "#         sample[rlds.OBSERVATION]['image'][batch_index, trajectory_index])\n",
        "#     axs[batch_index, trajectory_index].axis('off')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE-UQNNZlc51"
      },
      "source": [
        "# RT-1 Model Code\n",
        "\n",
        "In this section we:\n",
        "\n",
        "* Add some model dependency code, which contains layers used by the RT-1 model including the FiLM layers and EfficientNet, as well as the main RT-1 flax module.\n",
        "* Initialize random variables for the model and run a forward pass as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cellView": "form",
        "id": "P1GKKVdTMZy8"
      },
      "outputs": [],
      "source": [
        "# @title Model dependencies code\n",
        "\n",
        "\n",
        "MEAN_RGB = [0.485, 0.456, 0.406]\n",
        "STDDEV_RGB = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "Initializer = Callable[[jnp.ndarray, Sequence[int], jnp.dtype], jnp.ndarray]\n",
        "\n",
        "\n",
        "conv_kernel_init_fn = nn.initializers.variance_scaling(2.0, 'fan_out', 'normal')\n",
        "\n",
        "dense_kernel_init_fn = nn.initializers.variance_scaling(\n",
        "    1 / 3.0, 'fan_out', 'uniform'\n",
        ")\n",
        "\n",
        "\n",
        "class FilmConditioning(nn.Module):\n",
        "  \"\"\"FiLM conditioning layer.\"\"\"\n",
        "\n",
        "  num_channels: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, conv_filters, context):\n",
        "    \"\"\"Applies FiLM conditioning to the input.\n",
        "\n",
        "    Args:\n",
        "      conv_filters: array of shape (B, H, W, C), usually an output conv feature\n",
        "        map.\n",
        "      context: array of shape (B, context_size).\n",
        "\n",
        "    Returns:\n",
        "      array of shape (B, H, W, C) with the FiLM conditioning applied.\n",
        "    \"\"\"\n",
        "    zero_init = nn.initializers.zeros_init()\n",
        "    project_cond_add = nn.Dense(\n",
        "        self.num_channels, kernel_init=zero_init, bias_init=zero_init\n",
        "    )(context)\n",
        "    project_cond_mul = nn.Dense(\n",
        "        self.num_channels, kernel_init=zero_init, bias_init=zero_init\n",
        "    )(context)\n",
        "\n",
        "    project_cond_add = project_cond_add[:, None, None, :]\n",
        "    project_cond_mul = project_cond_mul[:, None, None, :]\n",
        "\n",
        "    result = (1 + project_cond_mul) * conv_filters + project_cond_add\n",
        "    return result\n",
        "\n",
        "class DepthwiseConv(nn.Module):\n",
        "  \"\"\"Depthwise convolution that matches tensorflow's conventions.\n",
        "\n",
        "  In Tensorflow, the shapes of depthwise kernels don't match the shapes of a\n",
        "  regular convolutional kernel of appropriate feature_group_count.\n",
        "  It is safer to use this class instead of the regular Conv (easier port of\n",
        "  tensorflow checkpoints, fan_out initialization of the previous layer will\n",
        "  match the tensorflow behavior, etc...).\n",
        "\n",
        "  Attributes:\n",
        "    features: Number of convolution filters.\n",
        "    kernel_size: Shape of the convolutional kernel.\n",
        "    strides: A sequence of `n` integers, representing the inter-window strides.\n",
        "    padding: Either the string `'SAME'`, the string `'VALID'`, or a sequence of\n",
        "      `n` `(low, high)` integer pairs that give the padding to apply before and\n",
        "      after each spatial dimension.\n",
        "    input_dilation: `None`, or a sequence of `n` integers, giving the dilation\n",
        "      factor to apply in each spatial dimension of `inputs`. Convolution with\n",
        "      input dilation `d` is equivalent to transposed convolution with stride\n",
        "      `d`.\n",
        "    kernel_dilation: `None`, or a sequence of `n` integers, giving the dilation\n",
        "      factor to apply in each spatial dimension of the convolution kernel.\n",
        "      Convolution with kernel dilation is also known as 'atrous convolution'.\n",
        "    feature_group_count: Unused attribute present in nn.Conv. Declare it to\n",
        "      match the nn.Conv API.\n",
        "    use_bias: Whether to add a bias to the output (default: True).\n",
        "    dtype: The dtype of the computation (default: float32).\n",
        "    precision: Numerical precision of the computation see `jax.lax.Precision`\n",
        "      for details.\n",
        "    kernel_init: Initializer for the convolutional kernel.\n",
        "    bias_init: Initializer for the bias.\n",
        "  \"\"\"\n",
        "\n",
        "  features: int\n",
        "  kernel_size: Tuple[int, int]\n",
        "  strides: Optional[Tuple[int, int]] = None\n",
        "  padding: Union[str, Sequence[int]] = 'SAME'\n",
        "  input_dilation: Optional[Sequence[int]] = None\n",
        "  kernel_dilation: Optional[Sequence[int]] = None\n",
        "  feature_group_count: int = 1\n",
        "  use_bias: bool = True\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "  precision: Any = None\n",
        "  kernel_init: Any = nn.initializers.lecun_normal()\n",
        "  bias_init: Any = nn.initializers.zeros\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Applies a convolution to the inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Input data with dimensions (batch, spatial_dims..., features).\n",
        "\n",
        "    Returns:\n",
        "      The convolved data.\n",
        "    \"\"\"\n",
        "    inputs = jnp.asarray(inputs, self.dtype)\n",
        "    in_features = inputs.shape[-1]\n",
        "    strides = self.strides\n",
        "\n",
        "    if strides is None:\n",
        "      strides = (1,) * (inputs.ndim - 2)\n",
        "\n",
        "    kernel_shape = self.kernel_size + (self.features, 1)\n",
        "    # Naming convention follows tensorflow.\n",
        "    kernel = self.param('depthwise_kernel', self.kernel_init, kernel_shape)\n",
        "    kernel = jnp.asarray(kernel, self.dtype)\n",
        "\n",
        "    # Need to transpose to convert tensorflow-shaped kernel to lax-shaped kernel\n",
        "    kernel = jnp.transpose(kernel, [0, 1, 3, 2])\n",
        "\n",
        "    dimension_numbers = nn.linear._conv_dimension_numbers(inputs.shape)  # pylint:disable=protected-access\n",
        "\n",
        "    y = jax.lax.conv_general_dilated(\n",
        "        inputs,\n",
        "        kernel,\n",
        "        strides,\n",
        "        self.padding,\n",
        "        lhs_dilation=self.input_dilation,\n",
        "        rhs_dilation=self.kernel_dilation,\n",
        "        dimension_numbers=dimension_numbers,\n",
        "        feature_group_count=in_features,\n",
        "        precision=self.precision,\n",
        "    )\n",
        "\n",
        "    if self.use_bias:\n",
        "      bias = self.param('bias', self.bias_init, (self.features,))\n",
        "      bias = jnp.asarray(bias, self.dtype)\n",
        "      y = y + bias\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "# pytype: disable=attribute-error\n",
        "# pylint:disable=unused-argument\n",
        "class BlockConfig(object):\n",
        "  \"\"\"Class that contains configuration parameters for a single block.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_filters: int = 0,\n",
        "      output_filters: int = 0,\n",
        "      kernel_size: int = 3,\n",
        "      num_repeat: int = 1,\n",
        "      expand_ratio: int = 1,\n",
        "      strides: Tuple[int, int] = (1, 1),\n",
        "      se_ratio: Optional[float] = None,\n",
        "      id_skip: bool = True,\n",
        "      fused_conv: bool = False,\n",
        "      conv_type: str = 'depthwise',\n",
        "  ):\n",
        "    for arg in locals().items():\n",
        "      setattr(self, *arg)\n",
        "\n",
        "\n",
        "class ModelConfig(object):\n",
        "  \"\"\"Class that contains configuration parameters for the model.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      width_coefficient: float = 1.0,\n",
        "      depth_coefficient: float = 1.0,\n",
        "      resolution: int = 224,\n",
        "      dropout_rate: float = 0.2,\n",
        "      blocks: Tuple[BlockConfig, ...] = (\n",
        "          # (input_filters, output_filters, kernel_size, num_repeat,\n",
        "          #  expand_ratio, strides, se_ratio)\n",
        "          # pylint: disable=bad-whitespace\n",
        "          BlockConfig(32, 16, 3, 1, 1, (1, 1), 0.25),\n",
        "          BlockConfig(16, 24, 3, 2, 6, (2, 2), 0.25),\n",
        "          BlockConfig(24, 40, 5, 2, 6, (2, 2), 0.25),\n",
        "          BlockConfig(40, 80, 3, 3, 6, (2, 2), 0.25),\n",
        "          BlockConfig(80, 112, 5, 3, 6, (1, 1), 0.25),\n",
        "          BlockConfig(112, 192, 5, 4, 6, (2, 2), 0.25),\n",
        "          BlockConfig(192, 320, 3, 1, 6, (1, 1), 0.25),\n",
        "          # pylint: enable=bad-whitespace\n",
        "      ),\n",
        "      stem_base_filters: int = 32,\n",
        "      top_base_filters: int = 1280,\n",
        "      activation: str = 'swish',\n",
        "      batch_norm: str = 'default',\n",
        "      bn_momentum: float = 0.99,\n",
        "      bn_epsilon: float = 1e-3,\n",
        "      # While the original implementation used a weight decay of 1e-5,\n",
        "      # tf.nn.l2_loss divides it by 2, so we halve this to compensate in Keras\n",
        "      weight_decay: float = 5e-6,\n",
        "      drop_connect_rate: float = 0.2,\n",
        "      depth_divisor: int = 8,\n",
        "      min_depth: Optional[int] = None,\n",
        "      use_se: bool = True,\n",
        "      input_channels: int = 3,\n",
        "      num_classes: int = 1000,\n",
        "      model_name: str = 'efficientnet',\n",
        "      rescale_input: bool = True,\n",
        "      data_format: str = 'channels_last',\n",
        "      final_projection_size: int = 0,\n",
        "      classifier_head: bool = True,\n",
        "      dtype: jnp.dtype = jnp.float32,\n",
        "  ):\n",
        "    \"\"\"Default Config for Efficientnet-B0.\"\"\"\n",
        "    for arg in locals().items():\n",
        "      setattr(self, *arg)\n",
        "\n",
        "\n",
        "# pylint:enable=unused-argument\n",
        "\n",
        "\n",
        "EN_MODEL_CONFIGS = {\n",
        "    # (width, depth, resolution, dropout)\n",
        "    'efficientnet-b3': ModelConfig(1.2, 1.4, 300, 0.3),\n",
        "}\n",
        "\n",
        "\n",
        "def round_filters(filters: int, config: ModelConfig) -> int:\n",
        "  \"\"\"Returns rounded number of filters based on width coefficient.\"\"\"\n",
        "  width_coefficient = config.width_coefficient\n",
        "  min_depth = config.min_depth\n",
        "  divisor = config.depth_divisor\n",
        "\n",
        "  if not width_coefficient:\n",
        "    return filters\n",
        "\n",
        "  filters *= width_coefficient\n",
        "  min_depth = min_depth or divisor\n",
        "  new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
        "  # Make sure that round down does not go down by more than 10%.\n",
        "  if new_filters < 0.9 * filters:\n",
        "    new_filters += divisor\n",
        "  return int(new_filters)\n",
        "\n",
        "\n",
        "def round_repeats(repeats: int, depth_coefficient: float) -> int:\n",
        "  \"\"\"Returns rounded number of repeats based on depth coefficient.\"\"\"\n",
        "  return int(math.ceil(depth_coefficient * repeats))\n",
        "\n",
        "\n",
        "def conv2d(\n",
        "    inputs: jnp.ndarray,\n",
        "    num_filters: int,\n",
        "    config: ModelConfig,\n",
        "    kernel_size: Union[int, Tuple[int, int]] = (1, 1),\n",
        "    strides: Tuple[int, int] = (1, 1),\n",
        "    use_batch_norm: bool = True,\n",
        "    use_bias: bool = False,\n",
        "    activation: Any = None,\n",
        "    depthwise: bool = False,\n",
        "    train: bool = True,\n",
        "    conv_name: Optional[str] = None,\n",
        "    bn_name: Optional[str] = None,\n",
        "    dtype: jnp.dtype = jnp.float32,\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Convolutional layer with possibly batch norm and activation.\n",
        "\n",
        "  Args:\n",
        "    inputs: Input data with dimensions (batch, spatial_dims..., features).\n",
        "    num_filters: Number of convolution filters.\n",
        "    config: Configuration for the model.\n",
        "    kernel_size: Size of the kernel, as a tuple of int.\n",
        "    strides: Strides for the convolution, as a tuple of int.\n",
        "    use_batch_norm: Whether batch norm should be applied to the output.\n",
        "    use_bias: Whether we should add bias to the output of the first convolution.\n",
        "    activation: Name of the activation function to use.\n",
        "    depthwise: If true, will use depthwise convolutions.\n",
        "    train: Whether the model should behave in training or inference mode.\n",
        "    conv_name: Name to give to the convolution layer.\n",
        "    bn_name: Name to give to the batch norm layer.\n",
        "    dtype: dtype for the computation.\n",
        "\n",
        "  Returns:\n",
        "    The output of the convolutional layer.\n",
        "  \"\"\"\n",
        "  conv_fn = DepthwiseConv if depthwise else nn.Conv\n",
        "  kernel_size = (\n",
        "      (kernel_size, kernel_size)\n",
        "      if isinstance(kernel_size, int)\n",
        "      else tuple(kernel_size)\n",
        "  )\n",
        "  conv_name = conv_name if conv_name else 'conv2d'\n",
        "  bn_name = bn_name if bn_name else 'batch_normalization'\n",
        "\n",
        "  x = conv_fn(\n",
        "      num_filters,\n",
        "      kernel_size,\n",
        "      tuple(strides),\n",
        "      padding='SAME',\n",
        "      use_bias=use_bias,\n",
        "      kernel_init=conv_kernel_init_fn,\n",
        "      name=conv_name,\n",
        "      dtype=dtype,\n",
        "  )(inputs)\n",
        "\n",
        "  if use_batch_norm:\n",
        "    x = nn.BatchNorm(\n",
        "        use_running_average=not train,\n",
        "        momentum=config.bn_momentum,\n",
        "        epsilon=config.bn_epsilon,\n",
        "        name=bn_name,\n",
        "        dtype=dtype,\n",
        "    )(x)\n",
        "\n",
        "  if activation is not None:\n",
        "    x = getattr(nn.activation, activation.lower())(x)\n",
        "  return x\n",
        "\n",
        "\n",
        "def stochastic_depth(\n",
        "    inputs: jnp.ndarray,\n",
        "    rng: jnp.ndarray,\n",
        "    survival_probability: float,\n",
        "    deterministic: bool = False,\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Applies stochastic depth.\n",
        "\n",
        "  Args:\n",
        "    inputs: The inputs that should be randomly masked.\n",
        "    rng: A `jax.random.PRNGKey`.\n",
        "    survival_probability: 1 - the probability of masking out a value.\n",
        "    deterministic: If false the inputs are scaled by `1 / (1 - rate)` and\n",
        "      masked, whereas if true, no mask is applied and the inputs are returned as\n",
        "      is.\n",
        "\n",
        "  Returns:\n",
        "    The masked inputs.\n",
        "  \"\"\"\n",
        "  if survival_probability == 1.0 or deterministic:\n",
        "    return inputs\n",
        "\n",
        "  mask_shape = [inputs.shape[0]] + [1 for _ in inputs.shape[1:]]\n",
        "  mask = jax.random.bernoulli(rng, p=survival_probability, shape=mask_shape)\n",
        "  mask = jnp.tile(mask, [1] + list(inputs.shape[1:]))\n",
        "  return jax.lax.select(\n",
        "      mask, inputs / survival_probability, jnp.zeros_like(inputs)\n",
        "  )\n",
        "\n",
        "\n",
        "class SqueezeExcite(nn.Module):\n",
        "  \"\"\"SqueezeExite block (See: https://arxiv.org/abs/1709.01507.)\n",
        "\n",
        "  Attributes:\n",
        "    num_filters: Number of convolution filters.\n",
        "    block: Configuration for this block.\n",
        "    config: Configuration for the model.\n",
        "    train: Whether the model is in training or inference mode.\n",
        "  \"\"\"\n",
        "\n",
        "  num_filters: int\n",
        "  block: BlockConfig\n",
        "  config: ModelConfig\n",
        "  train: bool\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Applies a convolution to the inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Input data with dimensions (batch, spatial_dims..., features).\n",
        "\n",
        "    Returns:\n",
        "      The output of the squeeze excite block.\n",
        "    \"\"\"\n",
        "    block = self.block\n",
        "    config = self.config\n",
        "    train = self.train\n",
        "    dtype = config.dtype\n",
        "    num_reduced_filters = max(1, int(block.input_filters * block.se_ratio))\n",
        "\n",
        "    se = nn.avg_pool(inputs, inputs.shape[1:3])\n",
        "    se = conv2d(\n",
        "        se,\n",
        "        num_reduced_filters,\n",
        "        config,\n",
        "        use_bias=True,\n",
        "        use_batch_norm=False,\n",
        "        activation=config.activation,\n",
        "        conv_name='reduce_conv2d_0',\n",
        "        train=train,\n",
        "        dtype=dtype,\n",
        "    )\n",
        "\n",
        "    se = conv2d(\n",
        "        se,\n",
        "        self.num_filters,\n",
        "        config,\n",
        "        use_bias=True,\n",
        "        use_batch_norm=False,\n",
        "        activation='sigmoid',\n",
        "        conv_name='expand_conv2d_0',\n",
        "        train=train,\n",
        "        dtype=dtype,\n",
        "    )\n",
        "\n",
        "    return inputs * se\n",
        "\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "  \"\"\"Main building component of Efficientnet.\n",
        "\n",
        "  Attributes:\n",
        "    block: BlockConfig, arguments to create a Block.\n",
        "    config: ModelConfig, a set of model parameters.\n",
        "    train: Whether we are training or predicting.\n",
        "  \"\"\"\n",
        "\n",
        "  block: BlockConfig\n",
        "  config: ModelConfig\n",
        "  train: bool = False\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Mobile Inverted Residual Bottleneck.\n",
        "\n",
        "    Args:\n",
        "      inputs: Input to the block.\n",
        "\n",
        "    Returns:\n",
        "      The output of the block.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "    block = self.block\n",
        "    train = self.train\n",
        "    use_se = config.use_se\n",
        "    activation = config.activation\n",
        "    drop_connect_rate = config.drop_connect_rate\n",
        "    use_depthwise = block.conv_type != 'no_depthwise'\n",
        "    dtype = config.dtype\n",
        "\n",
        "    rng = self.make_rng('random')\n",
        "\n",
        "    filters = block.input_filters * block.expand_ratio\n",
        "\n",
        "    x = inputs\n",
        "    bn_index = 0\n",
        "\n",
        "    if block.fused_conv:\n",
        "      # If we use fused mbconv, skip expansion and use regular conv.\n",
        "      x = conv2d(\n",
        "          x,\n",
        "          filters,\n",
        "          config,\n",
        "          kernel_size=block.kernel_size,\n",
        "          strides=block.strides,\n",
        "          activation=activation,\n",
        "          conv_name='fused_conv2d_0',\n",
        "          bn_name='batch_normalization_' + str(bn_index),\n",
        "          train=train,\n",
        "          dtype=dtype,\n",
        "      )\n",
        "      bn_index += 1\n",
        "    else:\n",
        "      if block.expand_ratio != 1:\n",
        "        # Expansion phase\n",
        "        kernel_size = (1, 1) if use_depthwise else (3, 3)\n",
        "        x = conv2d(\n",
        "            x,\n",
        "            filters,\n",
        "            config,\n",
        "            kernel_size=kernel_size,\n",
        "            activation=activation,\n",
        "            conv_name='expand_conv2d_0',\n",
        "            bn_name='batch_normalization_' + str(bn_index),\n",
        "            train=train,\n",
        "            dtype=dtype,\n",
        "        )\n",
        "        bn_index += 1\n",
        "      # Depthwise Convolution\n",
        "      if use_depthwise:\n",
        "        x = conv2d(\n",
        "            x,\n",
        "            num_filters=x.shape[-1],  # Depthwise conv\n",
        "            config=config,\n",
        "            kernel_size=block.kernel_size,\n",
        "            strides=block.strides,\n",
        "            activation=activation,\n",
        "            depthwise=True,\n",
        "            conv_name='depthwise_conv2d',\n",
        "            bn_name='batch_normalization_' + str(bn_index),\n",
        "            train=train,\n",
        "            dtype=dtype,\n",
        "        )\n",
        "        bn_index += 1\n",
        "\n",
        "    # Squeeze and Excitation phase\n",
        "    if use_se:\n",
        "      assert block.se_ratio is not None\n",
        "      assert 0 < block.se_ratio <= 1\n",
        "      x = SqueezeExcite(\n",
        "          num_filters=filters, block=block, config=config, train=train\n",
        "      )(x)\n",
        "\n",
        "    # Output phase\n",
        "    x = conv2d(\n",
        "        x,\n",
        "        block.output_filters,\n",
        "        config,\n",
        "        activation=None,\n",
        "        conv_name='project_conv2d_0',\n",
        "        bn_name='batch_normalization_' + str(bn_index),\n",
        "        train=train,\n",
        "        dtype=dtype,\n",
        "    )\n",
        "\n",
        "    if (\n",
        "        block.id_skip\n",
        "        and all(s == 1 for s in block.strides)\n",
        "        and block.input_filters == block.output_filters\n",
        "    ):\n",
        "      if drop_connect_rate and drop_connect_rate > 0:\n",
        "        survival_probability = 1 - drop_connect_rate\n",
        "        x = stochastic_depth(\n",
        "            x, rng, survival_probability, deterministic=not train\n",
        "        )\n",
        "      x = x + inputs\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Stem(nn.Module):\n",
        "  \"\"\"Initial block of Efficientnet.\n",
        "\n",
        "  Attributes:\n",
        "    config: ModelConfig, a set of model parameters.\n",
        "    train: Whether we are training or predicting.\n",
        "  \"\"\"\n",
        "\n",
        "  config: ModelConfig\n",
        "  train: bool = False\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Returns the output of the stem block.\n",
        "\n",
        "    Args:\n",
        "      inputs: The input to the block.\n",
        "\n",
        "    Returns:\n",
        "      Output of the block\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "    train = self.train\n",
        "    x = conv2d(\n",
        "        inputs,\n",
        "        round_filters(config.stem_base_filters, config),\n",
        "        config,\n",
        "        kernel_size=(3, 3),\n",
        "        strides=(2, 2),\n",
        "        activation=config.activation,\n",
        "        train=train,\n",
        "        dtype=config.dtype,\n",
        "    )\n",
        "    return x\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\"Final block of Efficientnet.\n",
        "\n",
        "  Attributes:\n",
        "    config: A set of model parameters.\n",
        "    train: Whether we are training or predicting.\n",
        "  \"\"\"\n",
        "\n",
        "  config: Any\n",
        "  train: bool = True\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Returns the output of the head block.\n",
        "\n",
        "    Args:\n",
        "      inputs: The input to the block.\n",
        "\n",
        "    Returns:\n",
        "      x: Classifier logits.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "    train = self.train\n",
        "    dtype = config.dtype\n",
        "    # Build top.\n",
        "    x = conv2d(\n",
        "        inputs,\n",
        "        round_filters(config.top_base_filters, config),\n",
        "        config,\n",
        "        activation=config.activation,\n",
        "        train=train,\n",
        "        dtype=dtype,\n",
        "    )\n",
        "    return x\n",
        "# pytype: enable=attribute-error\n",
        "\n",
        "\n",
        "class EfficientNetWithFilm(nn.Module):\n",
        "  \"\"\"EfficientNet with FiLM conditioning.\"\"\"\n",
        "\n",
        "  config: Any\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self, inputs: jnp.ndarray, context_input: jnp.ndarray, *, train: bool\n",
        "  ):\n",
        "    \"\"\"Returns the output of the EfficientNet model.\"\"\"\n",
        "    config = copy.deepcopy(self.config)\n",
        "    config.dtype = self.dtype\n",
        "    depth_coefficient = config.depth_coefficient\n",
        "    blocks = config.blocks\n",
        "    drop_connect_rate = config.drop_connect_rate\n",
        "\n",
        "    inputs = jnp.asarray(inputs, self.dtype)\n",
        "\n",
        "    # Build stem.\n",
        "    x = Stem(config=config, train=train)(inputs)\n",
        "\n",
        "    # Build blocks.\n",
        "    num_blocks_total = sum(\n",
        "        round_repeats(block.num_repeat, depth_coefficient) for block in blocks\n",
        "    )\n",
        "    block_num = 0\n",
        "\n",
        "    for _, block in enumerate(blocks):\n",
        "      assert block.num_repeat > 0\n",
        "      # Update block input and output filters based on depth multiplier.\n",
        "      block.input_filters = round_filters(block.input_filters, config)\n",
        "      block.output_filters = round_filters(block.output_filters, config)\n",
        "      block.num_repeat = round_repeats(block.num_repeat, depth_coefficient)\n",
        "\n",
        "      # The first block needs to take care of stride and filter size increase\n",
        "      drop_rate = drop_connect_rate * float(block_num) / num_blocks_total\n",
        "      config.drop_connect_rate = drop_rate\n",
        "\n",
        "      x = MBConvBlock(block=block, config=config, train=train)(x)\n",
        "\n",
        "      x = FilmConditioning(num_channels=x.shape[-1])(\n",
        "          x, context_input\n",
        "      )\n",
        "\n",
        "      block_num += 1\n",
        "      if block.num_repeat > 1:\n",
        "        block.input_filters = block.output_filters\n",
        "        block.strides = [1, 1]\n",
        "\n",
        "        for _ in range(block.num_repeat - 1):\n",
        "          drop_rate = drop_connect_rate * float(block_num) / num_blocks_total\n",
        "          config.drop_connect_rate = drop_rate\n",
        "          x = MBConvBlock(block=block, config=config, train=train)(x)\n",
        "          x = FilmConditioning(num_channels=x.shape[-1])(\n",
        "              x, context_input\n",
        "          )\n",
        "\n",
        "          block_num += 1\n",
        "\n",
        "    x = Head(self.config, train=train)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "  \"\"\"Identity layer, convenient for giving a name to an array.\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "    return x\n",
        "\n",
        "\n",
        "class MlpBlock(nn.Module):\n",
        "  \"\"\"Transformer MLP / feed-forward block.\"\"\"\n",
        "\n",
        "  mlp_dim: int\n",
        "  out_dim: Optional[int] = None\n",
        "  dropout_rate: float = 0.1\n",
        "  use_bias: bool = True\n",
        "  kernel_init: Initializer = nn.initializers.xavier_uniform()\n",
        "  bias_init: Initializer = nn.initializers.normal(stddev=1e-6)\n",
        "  activation_fn: Callable[[jnp.ndarray], jnp.ndarray] = nn.gelu\n",
        "  precision: Optional[jax.lax.Precision] = None\n",
        "  dtype: jnp.ndarray = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray, *, deterministic: bool):\n",
        "    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n",
        "    actual_out_dim = inputs.shape[-1] if self.out_dim is None else self.out_dim\n",
        "    x = nn.Dense(\n",
        "        self.mlp_dim,\n",
        "        dtype=self.dtype,\n",
        "        use_bias=self.use_bias,\n",
        "        kernel_init=self.kernel_init,\n",
        "        bias_init=self.bias_init,\n",
        "        precision=self.precision)(\n",
        "            inputs)\n",
        "    x = IdentityLayer(name='mlp1')(self.activation_fn(x))\n",
        "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "    output = nn.Dense(\n",
        "        actual_out_dim,\n",
        "        dtype=self.dtype,\n",
        "        use_bias=self.use_bias,\n",
        "        kernel_init=self.kernel_init,\n",
        "        bias_init=self.bias_init,\n",
        "        precision=self.precision)(\n",
        "            x)\n",
        "    output = IdentityLayer(name='mlp2')(output)\n",
        "    output = nn.Dropout(rate=self.dropout_rate)(\n",
        "        output, deterministic=deterministic)\n",
        "    return output\n",
        "\n",
        "\n",
        "class TokenLearnerModuleV11(nn.Module):\n",
        "  \"\"\"TokenLearner module Version 1.1, using slightly different conv. layers.\n",
        "\n",
        "  Instead of using 4 conv. layers with small channels to implement spatial\n",
        "  attention, this version uses a MLP with gelu inbetween. It also uses softmax\n",
        "  instead of sigmoid. We confirmed that this version works better in general.\n",
        "\n",
        "  Attributes:\n",
        "    num_tokens: Number of tokens.\n",
        "    bottleneck_dim: The size of hidden units in the MLP for spatial attention.\n",
        "    dropout_rate: Dropout rate.\n",
        "  \"\"\"\n",
        "  num_tokens: int\n",
        "  bottleneck_dim: int = 64\n",
        "  dropout_rate: float = 0.\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray, deterministic: bool) -> jnp.ndarray:\n",
        "    \"\"\"Applies learnable tokenization to the 2D inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Inputs of shape `[bs, h, w, c]`.\n",
        "      deterministic: Weather we are in the deterministic mode (e.g inference\n",
        "        time) or not.\n",
        "\n",
        "    Returns:\n",
        "      Output of shape `[bs, n_token, c]`.\n",
        "    \"\"\"\n",
        "    if inputs.ndim == 4:\n",
        "      n, h, w, c = inputs.shape\n",
        "      inputs = jnp.reshape(inputs, [n, h*w, c])\n",
        "\n",
        "    feature_shape = inputs.shape\n",
        "\n",
        "    selected = inputs\n",
        "\n",
        "    selected = nn.LayerNorm()(selected)\n",
        "\n",
        "    selected = MlpBlock(\n",
        "        mlp_dim=self.bottleneck_dim,\n",
        "        out_dim=self.num_tokens,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "        activation_fn=nn.gelu,\n",
        "        name='token_masking')(\n",
        "            selected, deterministic=deterministic)\n",
        "\n",
        "    selected = jnp.reshape(\n",
        "        selected,\n",
        "        [feature_shape[0], -1, self.num_tokens])  # Shape: [bs, h*w, n_token].\n",
        "    selected = jnp.transpose(selected, [0, 2, 1])  # Shape: [bs, n_token, h*w].\n",
        "    selected = jax.nn.softmax(selected, axis=-1)\n",
        "\n",
        "    feat = inputs\n",
        "    feat = jnp.reshape(\n",
        "        feat, [feature_shape[0], -1, feature_shape[-1]])  # Shape: [bs, h*w, c].\n",
        "\n",
        "    feat = jnp.einsum('...si,...id->...sd', selected, feat)\n",
        "\n",
        "    return feat\n",
        "\n",
        "\n",
        "class FFNOptions(enum.Enum):\n",
        "  \"\"\"Different choices of FFN block for ablation testing.\"\"\"\n",
        "\n",
        "  LINEAR = 'linear'  # RT-1 Legacy\n",
        "  SWIGLU = 'swiglu'  # Match LLaMa\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  \"\"\"A self-attention transformer block.\n",
        "\n",
        "  See the `_TransformerLayer` in\n",
        "  google-research/robotics_transformer/transformer.py for the original\n",
        "  tensorflow implementation.\n",
        "  \"\"\"\n",
        "  layer_size: int = 128\n",
        "  num_heads: int = 8\n",
        "  feed_forward_hidden_size: int = 512\n",
        "  feed_forward_output_size: int = 512\n",
        "  ffn_option: FFNOptions = FFNOptions.SWIGLU\n",
        "  dropout_rate: float = 0.1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, attn_mask: jnp.ndarray, *, train: bool):\n",
        "    x1 = nn.LayerNorm()(x)\n",
        "\n",
        "    x1 = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=self.num_heads,\n",
        "        qkv_features=(self.layer_size * self.num_heads),\n",
        "        dropout_rate=self.dropout_rate,\n",
        "    )(x1, x1, mask=attn_mask, deterministic=not train)\n",
        "\n",
        "    x = x + x1\n",
        "\n",
        "    y = nn.LayerNorm()(x)\n",
        "\n",
        "    if self.ffn_option == FFNOptions.SWIGLU:\n",
        "      h1 = nn.Dense(self.feed_forward_hidden_size, use_bias=False)(y)\n",
        "      h1 = nn.swish(h1)\n",
        "      gate = nn.Dense(self.feed_forward_hidden_size, use_bias=False)(y)\n",
        "      ff_y = nn.Dense(self.feed_forward_output_size, use_bias=False)(h1 * gate)\n",
        "    elif self.ffn_option == FFNOptions.LINEAR:\n",
        "      ff_y = nn.Dense(self.feed_forward_output_size, use_bias=False)(y)\n",
        "    else:\n",
        "      raise ValueError(f'Unknown FFN option: {self.ffn_option}')\n",
        "\n",
        "    ff_y = nn.Dropout(self.dropout_rate)(ff_y, deterministic=not train)\n",
        "    x = x + ff_y\n",
        "    return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  \"\"\"Transformer architecture with dense positional embedding.\n",
        "\n",
        "  See the `Transformer` in\n",
        "  google-research/robotics_transformer/transformer.py for the original\n",
        "  tensorflow implementation.\n",
        "  \"\"\"\n",
        "\n",
        "  num_layers: int = 8\n",
        "  layer_size: int = 128\n",
        "  num_heads: int = 8\n",
        "  feed_forward_hidden_size: int = 512\n",
        "  feed_forward_output_size: int = 512\n",
        "  ffn_option: FFNOptions = FFNOptions.SWIGLU\n",
        "  dropout_rate: float = 0.1\n",
        "  vocab_size: int = 256\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, attn_mask: jnp.ndarray, *, train: bool):\n",
        "    bs, seqlen, *_ = x.shape\n",
        "\n",
        "    pos = jnp.expand_dims(jnp.arange(0, seqlen, 1), 0)\n",
        "    pos = jnp.tile(pos, [bs, 1])\n",
        "    pos = jax.nn.one_hot(pos, seqlen)\n",
        "\n",
        "    x = nn.Dense(self.feed_forward_output_size)(x)\n",
        "    pos_emb = nn.Dense(self.feed_forward_output_size)(pos)\n",
        "    x += pos_emb\n",
        "\n",
        "    for _ in range(self.num_layers):\n",
        "      x = TransformerBlock(\n",
        "          layer_size=self.layer_size,\n",
        "          num_heads=self.num_heads,\n",
        "          feed_forward_hidden_size=self.feed_forward_hidden_size,\n",
        "          feed_forward_output_size=self.feed_forward_output_size,\n",
        "          dropout_rate=self.dropout_rate,\n",
        "          ffn_option=self.ffn_option,\n",
        "      )(x, attn_mask, train=train)\n",
        "\n",
        "    output_tokens = nn.Dense(self.vocab_size)(x)\n",
        "    return output_tokens\n",
        "\n",
        "\n",
        "class TokenLearnerModuleV11(nn.Module):\n",
        "  \"\"\"TokenLearner module Version 1.1, using slightly different conv. layers.\n",
        "\n",
        "  Instead of using 4 conv. layers with small channels to implement spatial\n",
        "  attention, this version uses a MLP with gelu inbetween. It also uses softmax\n",
        "  instead of sigmoid. We confirmed that this version works better in general.\n",
        "\n",
        "  From google-research/scenic/projects/token_learner/model.py.\n",
        "\n",
        "  Attributes:\n",
        "    num_tokens: Number of tokens.\n",
        "    bottleneck_dim: The size of hidden units in the MLP for spatial attention.\n",
        "    dropout_rate: Dropout rate.\n",
        "  \"\"\"\n",
        "  num_tokens: int\n",
        "  bottleneck_dim: int = 64\n",
        "  dropout_rate: float = 0.\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray, deterministic: bool) -> jnp.ndarray:\n",
        "    \"\"\"Applies learnable tokenization to the 2D inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Inputs of shape `[bs, h, w, c]`.\n",
        "      deterministic: Weather we are in the deterministic mode (e.g inference\n",
        "        time) or not.\n",
        "\n",
        "    Returns:\n",
        "      Output of shape `[bs, n_token, c]`.\n",
        "    \"\"\"\n",
        "    if inputs.ndim == 4:\n",
        "      n, h, w, c = inputs.shape\n",
        "      inputs = jnp.reshape(inputs, [n, h*w, c])\n",
        "\n",
        "    feature_shape = inputs.shape\n",
        "\n",
        "    selected = inputs\n",
        "\n",
        "    selected = nn.LayerNorm()(selected)\n",
        "\n",
        "    selected = MlpBlock(\n",
        "        mlp_dim=self.bottleneck_dim,\n",
        "        out_dim=self.num_tokens,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "        activation_fn=nn.gelu,\n",
        "        name='token_masking')(\n",
        "            selected, deterministic=deterministic)\n",
        "\n",
        "    selected = jnp.reshape(\n",
        "        selected,\n",
        "        [feature_shape[0], -1, self.num_tokens])  # Shape: [bs, h*w, n_token].\n",
        "    selected = jnp.transpose(selected, [0, 2, 1])  # Shape: [bs, n_token, h*w].\n",
        "    selected = jax.nn.softmax(selected, axis=-1)\n",
        "\n",
        "    feat = inputs\n",
        "    feat = jnp.reshape(\n",
        "        feat, [feature_shape[0], -1, feature_shape[-1]])  # Shape: [bs, h*w, c].\n",
        "\n",
        "    feat = jnp.einsum('...si,...id->...sd', selected, feat)\n",
        "\n",
        "    return feat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "id": "5XSY1Thyutjg"
      },
      "outputs": [],
      "source": [
        "# @title Main RT-1 model code\n",
        "\n",
        "class ImageTokenizer(nn.Module):\n",
        "  \"\"\"Tokenizes images with EfficientNet+FiLM.\n",
        "\n",
        "  This is based on the `RT1ImageTokenizer` implementation here:\n",
        "  google-research/robotics_transformer/tokenizers/image_tokenizer.py\n",
        "\n",
        "  The overall flow of the image tokenizer:\n",
        "  * The input image batch dimensions are squashed, and the image is normalized.\n",
        "  * The image is fed through the `EfficientNetWithFilm`.\n",
        "  * A 1x1 convolution is applied to project to `num_features`.\n",
        "  * Another final `FilmConditioning` layer is applied with the context.\n",
        "  * `TokenLearnerModuleV11` is applied to project the tokens to `num_tokens`.\n",
        "  \"\"\"\n",
        "\n",
        "  num_tokens: int = 8\n",
        "  num_features: int = 512\n",
        "\n",
        "  use_token_learner: bool = True\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self, image: jnp.ndarray, context_input: jnp.ndarray, *, train: bool\n",
        "  ):\n",
        "    \"\"\"Tokenizes the image using an EfficientNet.\n",
        "\n",
        "    Args:\n",
        "      image: jnp.Array with batch and seqlen leading dimensions. We assume the\n",
        "        input image is of size 300x300, since the EfficientNet takes in images\n",
        "        of that size.\n",
        "      context_input: jnp.Array with shape (batch * seqlen, size).\n",
        "      train: Training mode.\n",
        "\n",
        "    Returns:\n",
        "      shape (batch, seqlen, num_tokens, num_features) array.\n",
        "    \"\"\"\n",
        "    bs, seqlen, *_ = image.shape\n",
        "\n",
        "    # The efficientnet-b3 model uses 300x300 images.\n",
        "    efficientnet_config = EN_MODEL_CONFIGS['efficientnet-b3']\n",
        "    image = jnp.reshape(image, [bs * seqlen, 300, 300, 3])\n",
        "    image -= jnp.array(MEAN_RGB)\n",
        "    image /= jnp.array(STDDEV_RGB)\n",
        "\n",
        "    # Apply film in EfficientNet.\n",
        "    x = EfficientNetWithFilm(efficientnet_config)(\n",
        "        image, context_input=context_input, train=train\n",
        "    )\n",
        "\n",
        "    # 1x1 conv. This corresponds to the 1x1 conv here:\n",
        "    # google-research/robotics_transformer/film_efficientnet/pretrained_efficientnet_encoder.py\n",
        "    var_init = nn.initializers.variance_scaling(\n",
        "        scale=1.0,\n",
        "        mode='fan_in',\n",
        "        distribution='truncated_normal',\n",
        "    )\n",
        "    x = nn.Conv(\n",
        "        features=self.num_features,\n",
        "        kernel_size=(1, 1),\n",
        "        strides=(1, 1),\n",
        "        padding='SAME',\n",
        "        use_bias=False,\n",
        "        kernel_init=var_init,\n",
        "    )(x)\n",
        "\n",
        "    x = FilmConditioning(num_channels=self.num_features)(\n",
        "        x, context_input\n",
        "    )\n",
        "\n",
        "    if self.use_token_learner:\n",
        "      x = TokenLearnerModuleV11(num_tokens=self.num_tokens)(\n",
        "          x, deterministic=not train\n",
        "      )\n",
        "\n",
        "    x = jnp.reshape(x, [bs, seqlen, self.num_tokens, -1])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def tokenize_action(\n",
        "    actions: Dict[str, jnp.ndarray],\n",
        "    vocab_size: int,\n",
        "    world_vector_range: Tuple[float, float] = (-1.0, 1.0),\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Tokenizes the action for the RT-1 task.\n",
        "\n",
        "  <name>: <shape> <bounds>\n",
        "  terminate_episode: (3,) int32,\n",
        "    mode 0: terminate episode\n",
        "    mode 1: arm + gripper\n",
        "\n",
        "    mode 2: base\n",
        "  world_vector: (3,) [-1.0, 1.0] (RT-1) or [-2.0, 2.0] (RT-1-X)\n",
        "  rotation_delta: (3,) [-np.pi, np.pi]\n",
        "  gripper_closedness_action: (1,) [-1, 1]\n",
        "  base_displacement_vertical_rotation: (1,) [-np.pi, np.pi]\n",
        "  base_displacement_vector: (2,) [-1.0, 1.0]\n",
        "\n",
        "  Args:\n",
        "    actions: The raw action dictionary.\n",
        "    vocab_size: The vocab size of the tokenized actions.\n",
        "    world_vector_range: The bounds to use for the world_vector token.\n",
        "\n",
        "  Returns:\n",
        "    the tokenized action.\n",
        "  \"\"\"\n",
        "  action_tokens = []\n",
        "\n",
        "  # Handle the discrete one first.\n",
        "  terminate_episode = actions['terminate_episode']\n",
        "  terminate_episode = jnp.argmax(terminate_episode, axis=-1)\n",
        "  terminate_episode = jnp.expand_dims(terminate_episode, -1)\n",
        "  terminate_episode = terminate_episode.astype(jnp.int32)\n",
        "  action_tokens.append(terminate_episode)\n",
        "\n",
        "  for act_name, act_min, act_max in [\n",
        "      ('world_vector', world_vector_range[0], world_vector_range[1]),\n",
        "      ('rotation_delta', -np.pi / 2, np.pi / 2),\n",
        "      ('gripper_closedness_action', -1.0, 1.0),\n",
        "      ('base_displacement_vertical_rotation', -np.pi, np.pi),\n",
        "      ('base_displacement_vector', -1.0, 1.0),\n",
        "  ]:\n",
        "    act = actions[act_name]\n",
        "    act = jnp.clip(act, act_min, act_max)\n",
        "    act = (act - act_min) / (act_max - act_min)\n",
        "    act = act * (vocab_size - 1)\n",
        "    act = act.astype(jnp.int32)\n",
        "    action_tokens.append(act)\n",
        "\n",
        "  tokenized = jnp.concatenate(action_tokens, axis=-1)\n",
        "  return tokenized\n",
        "\n",
        "\n",
        "def detokenize_action(\n",
        "    tokenized_actions: jnp.ndarray,\n",
        "    vocab_size: int,\n",
        "    world_vector_range: Tuple[float, float] = (-1.0, 1.0),\n",
        ") -> Dict[str, jnp.ndarray]:\n",
        "  \"\"\"De-tokenizes the action for the RT-1 task.\n",
        "\n",
        "  See `tokenize_action` for information on the action structure.\n",
        "\n",
        "  Args:\n",
        "    tokenized_actions: The tokenized action vector.\n",
        "    vocab_size: The vocab size of the tokenized actions.\n",
        "    world_vector_range: The bounds to use for the world_vector token.\n",
        "\n",
        "  Returns:\n",
        "    the detokenized action dictionary.\n",
        "  \"\"\"\n",
        "  terminate_episode = tokenized_actions[:, 0]\n",
        "  terminate_episode = jax.nn.one_hot(terminate_episode, 3)\n",
        "\n",
        "  raw_actions = dict(\n",
        "      world_vector=tokenized_actions[:, 1:4].astype(jnp.float32),\n",
        "      rotation_delta=tokenized_actions[:, 4:7].astype(jnp.float32),\n",
        "      gripper_closedness_action=tokenized_actions[:, 7:8].astype(jnp.float32),\n",
        "      base_displacement_vertical_rotation=tokenized_actions[:, 8:9].astype(\n",
        "          jnp.float32\n",
        "      ),\n",
        "      base_displacement_vector=tokenized_actions[:, 9:11].astype(jnp.float32),\n",
        "  )\n",
        "\n",
        "  act_dict = {'terminate_episode': terminate_episode.astype(jnp.int32)}\n",
        "  for act_name, act_min, act_max in [\n",
        "      ('world_vector', world_vector_range[0], world_vector_range[1]),\n",
        "      ('rotation_delta', -np.pi / 2, np.pi / 2),\n",
        "      ('gripper_closedness_action', -1.0, 1.0),\n",
        "      ('base_displacement_vertical_rotation', -np.pi, np.pi),\n",
        "      ('base_displacement_vector', -1.0, 1.0),\n",
        "  ]:\n",
        "    act = raw_actions[act_name]\n",
        "    act = act / (vocab_size - 1)\n",
        "    act = act * (act_max - act_min)\n",
        "    act = act + act_min\n",
        "    act_dict[act_name] = act\n",
        "\n",
        "  return act_dict\n",
        "\n",
        "\n",
        "class RT1(nn.Module):\n",
        "  \"\"\"Full RT-1 and RT-1-X architecture.\"\"\"\n",
        "\n",
        "  num_layers: int = 8\n",
        "  layer_size: int = 128\n",
        "  num_heads: int = 8\n",
        "  feed_forward_hidden_size: int = 512\n",
        "  feed_forward_output_size: int = 512\n",
        "  ffn_option: FFNOptions = FFNOptions.SWIGLU\n",
        "  dropout_rate: float = 0.1\n",
        "  vocab_size: int = 256\n",
        "  num_image_tokens: int = 8\n",
        "  num_action_tokens: int = 11\n",
        "  image_num_features: int = 512\n",
        "\n",
        "  world_vector_range: Tuple[float, float] = (-1.0, 1.0)\n",
        "\n",
        "  use_token_learner: bool = True\n",
        "\n",
        "  # By default, mask out previous actions.\n",
        "  include_prev_timesteps_actions: bool = False\n",
        "\n",
        "  sow_intermediates: bool = False\n",
        "\n",
        "  def setup(self):\n",
        "    self.image_tokenizer = ImageTokenizer(\n",
        "        num_tokens=self.num_image_tokens,\n",
        "        num_features=self.image_num_features,\n",
        "        use_token_learner=self.use_token_learner,\n",
        "    )\n",
        "\n",
        "  def tokenize_image(\n",
        "      self, image: jnp.ndarray, context: jnp.ndarray, *, train: bool\n",
        "  ):\n",
        "    bs, seqlen, *_ = image.shape\n",
        "    context = jnp.reshape(context, [bs * seqlen, -1])\n",
        "    return self.image_tokenizer(image, context_input=context, train=train)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self,\n",
        "      obs: Dict[str, jnp.ndarray],\n",
        "      act: Dict[str, jnp.ndarray],\n",
        "      obs_tokens: Optional[jnp.ndarray] = None,\n",
        "      act_tokens: Optional[jnp.ndarray] = None,\n",
        "      *,\n",
        "      train: bool,\n",
        "  ):\n",
        "    bs = obs['image'].shape[0]\n",
        "    seqlen = obs['image'].shape[1]\n",
        "\n",
        "    # Depending on whether `obs_tokens` is passed, we either run the full\n",
        "    # sequence of images through the image tokenizer, or simply use the\n",
        "    # image tokens passed into this function. `obs_tokens` is usually passed\n",
        "    # during an inference call when caching tokens from previous elements of\n",
        "    # the input sequence.\n",
        "    if obs_tokens is None:\n",
        "      # Get image + language fused tokens.\n",
        "      image = obs['image']\n",
        "      lang = obs['natural_language_embedding']\n",
        "      lang = jnp.reshape(lang, [bs * seqlen, -1])\n",
        "      context_image_tokens = self.image_tokenizer(\n",
        "          image=image, context_input=lang, train=train\n",
        "      )\n",
        "    else:\n",
        "      context_image_tokens = obs_tokens\n",
        "\n",
        "    if self.sow_intermediates:\n",
        "      self.sow('intermediates', 'image_tokens', context_image_tokens)\n",
        "\n",
        "    # We either tokenize the action ourselves using `tokenize_action_fn` or\n",
        "    # use the tokens passed into this function. `act_tokens` is usually supplied\n",
        "    # during an inference call when caching tokens from previous actions.\n",
        "    if act_tokens is None:\n",
        "      action_tokens = tokenize_action(\n",
        "          act, self.vocab_size, self.world_vector_range\n",
        "      )  # pylint: disable=too-many-function-args\n",
        "    else:\n",
        "      action_tokens = act_tokens\n",
        "\n",
        "    if self.include_prev_timesteps_actions:\n",
        "      # Always zero out the final action tokens.\n",
        "      previous_action_tokens = action_tokens[:, : (seqlen - 1), :]\n",
        "      zero_action_tokens = jnp.zeros((bs, 1, self.num_action_tokens))\n",
        "      action_tokens = jnp.concatenate(\n",
        "          [previous_action_tokens, zero_action_tokens], axis=-2\n",
        "      )\n",
        "\n",
        "      # Project the actions to the token dimension.\n",
        "      action_tokens = jax.nn.one_hot(action_tokens, num_classes=self.vocab_size)\n",
        "      action_tokens = nn.Dense(self.image_num_features)(action_tokens)\n",
        "    else:\n",
        "      # If we're not including the previous actions, then we can zero out\n",
        "      # the action tokens. We do it here to ensure tokens are consistently\n",
        "      # zero regardless of the input actions passed to the function.\n",
        "      action_tokens = jnp.zeros(\n",
        "          (bs, seqlen, self.num_action_tokens, self.image_num_features)\n",
        "      )\n",
        "\n",
        "    # Assemble the input tokens into a single sequence.\n",
        "    full_tokens = jnp.concatenate(\n",
        "        [context_image_tokens, action_tokens], axis=-2\n",
        "    )\n",
        "\n",
        "    num_action_tokens = action_tokens.shape[-2]\n",
        "    full_tokens = jnp.reshape(\n",
        "        full_tokens,\n",
        "        [bs, seqlen * (self.num_image_tokens + num_action_tokens), -1],\n",
        "    )\n",
        "\n",
        "    attn_mask = self._construct_attn_mask(\n",
        "        seqlen * (self.num_image_tokens + self.num_action_tokens)\n",
        "    )\n",
        "    output_tokens = Transformer(\n",
        "        num_layers=self.num_layers,\n",
        "        layer_size=self.layer_size,\n",
        "        num_heads=self.num_heads,\n",
        "        feed_forward_hidden_size=self.feed_forward_hidden_size,\n",
        "        feed_forward_output_size=self.feed_forward_output_size,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "        vocab_size=self.vocab_size,\n",
        "        ffn_option=self.ffn_option,\n",
        "    )(full_tokens, attn_mask=attn_mask, train=train)\n",
        "\n",
        "    return output_tokens\n",
        "\n",
        "  def _get_action_index_for_token(self, k: int, num_tokens: int):\n",
        "    \"\"\"Returns action associated with the token at given position `k`.\n",
        "\n",
        "    If k is not an action token then it returns -1.\n",
        "    If k is part of the first action in the sequence then returns 0 etc.\n",
        "\n",
        "    Based on `_get_action_index_for_token` here:\n",
        "    google-research/robotics_transformer/transformer_network.py\n",
        "\n",
        "    Args:\n",
        "      k: an int that represents the position in the sequence.\n",
        "      num_tokens: The total number of tokens in the sequence.\n",
        "    Returns:\n",
        "      The index of the action that this position belongs to, or if this\n",
        "      position is part of an image token then returns -1.\n",
        "    \"\"\"\n",
        "    if k < 0 or k >= num_tokens:\n",
        "      return -1\n",
        "\n",
        "    single_time_step_num_tokens = self.num_image_tokens + self.num_action_tokens\n",
        "    n = k\n",
        "    if n % single_time_step_num_tokens < self.num_image_tokens:\n",
        "      return -1\n",
        "\n",
        "    return int(n / single_time_step_num_tokens)\n",
        "\n",
        "  def _construct_attn_mask(self, num_tokens: ...):\n",
        "    \"\"\"Generate mask for action prediction loss.\n",
        "\n",
        "    This masks out all action tokens.\n",
        "\n",
        "    Based on `_generate_masks` here:\n",
        "    google-research/robotics_transformer/transformer_network.py\n",
        "\n",
        "    Args:\n",
        "      num_tokens: The number of tokens with which to construct the input mask.\n",
        "\n",
        "    Returns:\n",
        "      A (num_tokens, num_tokens) attention mask.\n",
        "    \"\"\"\n",
        "    default_attn_mask = np.tril(np.ones((num_tokens, num_tokens), np.int32))\n",
        "    action_mask = np.zeros(shape=(num_tokens, num_tokens), dtype=np.int32)\n",
        "\n",
        "    for i in range(num_tokens):\n",
        "      for j in range(num_tokens):\n",
        "        action_i = self._get_action_index_for_token(i, num_tokens)\n",
        "        action_j = self._get_action_index_for_token(j, num_tokens)\n",
        "        mask = 0\n",
        "        if action_i != -1 and action_j != -1:\n",
        "          # Ignore actions of previous steps.\n",
        "          if action_j < action_i:\n",
        "            mask = 1\n",
        "          # If we're not auto-regression, ignore action dimensions of current\n",
        "          # step.\n",
        "          if action_j == action_i and j <= i:\n",
        "            mask = 1\n",
        "        # i not is an action, but j is an action token.\n",
        "        # Hence, also mask j when predicting i, to prevent accidental\n",
        "        # dependency between output and masked dimensions because the output\n",
        "        # can still depend on the masked dimensions when predictions of the\n",
        "        # transformer layers after the first layer depends on the masked\n",
        "        # dimensions.\n",
        "        elif action_j != -1:\n",
        "          if not self.include_prev_timesteps_actions and j < i:\n",
        "            mask = 1\n",
        "        action_mask[i, j] = mask\n",
        "    return default_attn_mask - action_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "executionInfo": {
          "elapsed": 95728,
          "status": "ok",
          "timestamp": 1702072619026,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "AsJBedCbv_cl",
        "outputId": "ad37c985-10ea-4f60-c6cb-81b996690119"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 32\u001b[0m\n\u001b[1;32m     19\u001b[0m obs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatural_language_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m512\u001b[39m)),\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     23\u001b[0m act \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m: jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotation_delta\u001b[39m\u001b[38;5;124m\"\u001b[39m: jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminate_episode\u001b[39m\u001b[38;5;124m\"\u001b[39m: jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m     30\u001b[0m }\n\u001b[0;32m---> 32\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mrt1x_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m model_output \u001b[38;5;241m=\u001b[39m rt1x_model\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     42\u001b[0m     variables,\n\u001b[1;32m     43\u001b[0m     obs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     rngs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m: jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)},\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Inspect the model weights and output.\u001b[39;00m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:2438\u001b[0m, in \u001b[0;36mModule.init\u001b[0;34m(self, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes a module method with variables and returns modified variables.\u001b[39;00m\n\u001b[1;32m   2308\u001b[0m \n\u001b[1;32m   2309\u001b[0m \u001b[38;5;124;03m``init`` takes as first argument either a single ``PRNGKey``, or a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;124;03m  The initialized variable dict.\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m Module\u001b[38;5;241m.\u001b[39m_module_checks(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 2438\u001b[0m _, v_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_with_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[43m  \u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmutable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmutable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2443\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcapture_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_intermediates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2444\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v_out\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:2290\u001b[0m, in \u001b[0;36mModule.init_with_output\u001b[0;34m(self, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2288\u001b[0m   method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m\n\u001b[1;32m   2289\u001b[0m method \u001b[38;5;241m=\u001b[39m _get_unbound_fn(method)\n\u001b[0;32m-> 2290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_with_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmutable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmutable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcapture_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_intermediates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/core/scope.py:1137\u001b[0m, in \u001b[0;36minit.<locals>.wrapper\u001b[0;34m(rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m   rngs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: rngs}\n\u001b[1;32m   1136\u001b[0m init_flags \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(flags \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitializing\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m-> 1137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_flags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m  \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrngs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrngs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/core/scope.py:1101\u001b[0m, in \u001b[0;36mapply.<locals>.wrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mApplyScopeInvalidVariablesStructureError(variables)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m bind(\n\u001b[1;32m   1099\u001b[0m   variables, rngs\u001b[38;5;241m=\u001b[39mrngs, mutable\u001b[38;5;241m=\u001b[39mmutable, flags\u001b[38;5;241m=\u001b[39mflags\n\u001b[1;32m   1100\u001b[0m )\u001b[38;5;241m.\u001b[39mtemporary() \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[0;32m-> 1101\u001b[0m   y \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mutable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m y, root\u001b[38;5;241m.\u001b[39mmutable_variables()\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:3077\u001b[0m, in \u001b[0;36minit_with_output.<locals>.scope_fn\u001b[0;34m(scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3075\u001b[0m _context\u001b[38;5;241m.\u001b[39mcapture_stack\u001b[38;5;241m.\u001b[39mappend(capture_intermediates)\n\u001b[1;32m   3076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3077\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_deep_clone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3079\u001b[0m   _context\u001b[38;5;241m.\u001b[39mcapture_stack\u001b[38;5;241m.\u001b[39mpop()\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[0;32mIn[24], line 243\u001b[0m, in \u001b[0;36mRT1.__call__\u001b[0;34m(self, obs, act, obs_tokens, act_tokens, train)\u001b[0m\n\u001b[1;32m    241\u001b[0m   lang \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnatural_language_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    242\u001b[0m   lang \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mreshape(lang, [bs \u001b[38;5;241m*\u001b[39m seqlen, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 243\u001b[0m   context_image_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m      \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m   context_image_tokens \u001b[38;5;241m=\u001b[39m obs_tokens\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[0;32mIn[24], line 47\u001b[0m, in \u001b[0;36mImageTokenizer.__call__\u001b[0;34m(self, image, context_input, train)\u001b[0m\n\u001b[1;32m     44\u001b[0m image \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(STDDEV_RGB)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Apply film in EfficientNet.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mEfficientNetWithFilm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefficientnet_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# 1x1 conv. This corresponds to the 1x1 conv here:\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# google-research/robotics_transformer/film_efficientnet/pretrained_efficientnet_encoder.py\u001b[39;00m\n\u001b[1;32m     53\u001b[0m var_init \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39mvariance_scaling(\n\u001b[1;32m     54\u001b[0m     scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     55\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfan_in\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     56\u001b[0m     distribution\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncated_normal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     57\u001b[0m )\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[0;32mIn[23], line 651\u001b[0m, in \u001b[0;36mEfficientNetWithFilm.__call__\u001b[0;34m(self, inputs, context_input, train)\u001b[0m\n\u001b[1;32m    649\u001b[0m drop_rate \u001b[38;5;241m=\u001b[39m drop_connect_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mfloat\u001b[39m(block_num) \u001b[38;5;241m/\u001b[39m num_blocks_total\n\u001b[1;32m    650\u001b[0m config\u001b[38;5;241m.\u001b[39mdrop_connect_rate \u001b[38;5;241m=\u001b[39m drop_rate\n\u001b[0;32m--> 651\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mMBConvBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m x \u001b[38;5;241m=\u001b[39m FilmConditioning(num_channels\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])(\n\u001b[1;32m    653\u001b[0m     x, context_input\n\u001b[1;32m    654\u001b[0m )\n\u001b[1;32m    656\u001b[0m block_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[0;32mIn[23], line 494\u001b[0m, in \u001b[0;36mMBConvBlock.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m block\u001b[38;5;241m.\u001b[39mse_ratio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    493\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m block\u001b[38;5;241m.\u001b[39mse_ratio \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 494\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mSqueezeExcite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Output phase\u001b[39;00m\n\u001b[1;32m    499\u001b[0m x \u001b[38;5;241m=\u001b[39m conv2d(\n\u001b[1;32m    500\u001b[0m     x,\n\u001b[1;32m    501\u001b[0m     block\u001b[38;5;241m.\u001b[39moutput_filters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    508\u001b[0m )\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[0;32mIn[23], line 388\u001b[0m, in \u001b[0;36mSqueezeExcite.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    375\u001b[0m se \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mavg_pool(inputs, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    376\u001b[0m se \u001b[38;5;241m=\u001b[39m conv2d(\n\u001b[1;32m    377\u001b[0m     se,\n\u001b[1;32m    378\u001b[0m     num_reduced_filters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    386\u001b[0m )\n\u001b[0;32m--> 388\u001b[0m se \u001b[38;5;241m=\u001b[39m \u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_batch_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexpand_conv2d_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs \u001b[38;5;241m*\u001b[39m se\n",
            "Cell \u001b[0;32mIn[23], line 289\u001b[0m, in \u001b[0;36mconv2d\u001b[0;34m(inputs, num_filters, config, kernel_size, strides, use_batch_norm, use_bias, activation, depthwise, train, conv_name, bn_name, dtype)\u001b[0m\n\u001b[1;32m    286\u001b[0m conv_name \u001b[38;5;241m=\u001b[39m conv_name \u001b[38;5;28;01mif\u001b[39;00m conv_name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv2d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    287\u001b[0m bn_name \u001b[38;5;241m=\u001b[39m bn_name \u001b[38;5;28;01mif\u001b[39;00m bn_name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_normalization\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mconv_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_kernel_init_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_batch_norm:\n\u001b[1;32m    301\u001b[0m   x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm(\n\u001b[1;32m    302\u001b[0m       use_running_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m train,\n\u001b[1;32m    303\u001b[0m       momentum\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbn_momentum,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m       dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    307\u001b[0m   )(x)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/linear.py:637\u001b[0m, in \u001b[0;36m_Conv.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m kernel_shape:\n\u001b[1;32m    632\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMask needs to have the same shape as weights. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShapes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    635\u001b[0m   )\n\u001b[0;32m--> 637\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m   kernel \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/linen/module.py:1863\u001b[0m, in \u001b[0;36mModule.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNameInUseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam\u001b[39m\u001b[38;5;124m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1863\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mchildren[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/flax/core/scope.py:990\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeCollectionNotFound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamNotFoundError(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[0;32m--> 990\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mput_variable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, value)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unbox:\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/nn/initializers.py:341\u001b[0m, in \u001b[0;36mvariance_scaling.<locals>.init\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _complex_truncated_normal(key, \u001b[38;5;241m2\u001b[39m, named_shape, dtype) \u001b[38;5;241m*\u001b[39m stddev\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m distribution \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 341\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamed_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(variance)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m distribution \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    343\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39missubdtype(dtype, jnp\u001b[38;5;241m.\u001b[39mfloating):\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/random.py:705\u001b[0m, in \u001b[0;36mnormal\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    703\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype)\n\u001b[1;32m    704\u001b[0m shape \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mas_named_shape(shape)\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:327\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 327\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m      \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    330\u001b[0m   pgle_profiler \u001b[38;5;241m=\u001b[39m _read_pgle_profiler(jaxpr)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:185\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39minit_states, \u001b[38;5;241m*\u001b[39margs_flat]\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/core.py:2834\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2830\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2831\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2832\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2833\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/core.py:420\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    419\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 420\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/core.py:921\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1635\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1632\u001b[0m donated_argnums \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d]\n\u001b[1;32m   1633\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1634\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_argnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_registry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_explicit_sharding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1614\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1614\u001b[0m   out_flat, compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m      \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m      \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m      \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1620\u001b[0m   pgle_profiler \u001b[38;5;241m=\u001b[39m _read_pgle_profiler(jaxpr)\n\u001b[1;32m   1621\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1622\u001b[0m       compiled, tree_structure(out_flat), args, out_flat, [], jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[1;32m   1623\u001b[0m       jaxpr\u001b[38;5;241m.\u001b[39mconsts, \u001b[38;5;28;01mNone\u001b[39;00m, pgle_profiler)\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1544\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     compile_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfdo_profile\u001b[39m\u001b[38;5;124m'\u001b[39m: fdo_profile}\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;66;03m# TODO(patrios): Do not pass mutable profile session through cached lowering\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;66;03m# chain. Instead we need to move profilers dictionary to pxla module and use\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;66;03m# module as key. Right now we can't do that since there is no way to evict _pjit_lower_cached cache for in PGLE mode.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpgle_profiler\u001b[49m\n\u001b[0;32m-> 1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m _most_recent_pjit_call_executable\u001b[38;5;241m.\u001b[39mweak_key_dict[jaxpr] \u001b[38;5;241m=\u001b[39m compiled\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:2496\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, compiler_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MeshExecutable:\n\u001b[1;32m   2495\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2496\u001b[0m     executable \u001b[38;5;241m=\u001b[39m \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2500\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:2995\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2992\u001b[0m       mesh \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mmesh  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2993\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 2995\u001b[0m xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[1;32m   3002\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:2810\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values, pgle_profiler)\u001b[0m\n\u001b[1;32m   2802\u001b[0m compile_options \u001b[38;5;241m=\u001b[39m create_compile_options(\n\u001b[1;32m   2803\u001b[0m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[1;32m   2804\u001b[0m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[1;32m   2805\u001b[0m     dev, pmap_nreps, compiler_options)\n\u001b[1;32m   2807\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2808\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2809\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2810\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2812\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/compiler.py:378\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compile_and_write_autotune_config(\n\u001b[1;32m    368\u001b[0m       backend,\n\u001b[1;32m    369\u001b[0m       computation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m       min_device_process_id\n\u001b[1;32m    376\u001b[0m   )\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/compiler.py:608\u001b[0m, in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compile_and_write_cache\u001b[39m(\n\u001b[1;32m    600\u001b[0m     backend: xc\u001b[38;5;241m.\u001b[39mClient,\n\u001b[1;32m    601\u001b[0m     computation: ir\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    606\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xc\u001b[38;5;241m.\u001b[39mLoadedExecutable:\n\u001b[1;32m    607\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 608\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m   compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    612\u001b[0m   _cache_write(\n\u001b[1;32m    613\u001b[0m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[1;32m    614\u001b[0m   )\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
            "File \u001b[0;32m~/Thesis/open_x_embodiment/.venv/lib/python3.10/site-packages/jax/_src/compiler.py:238\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    234\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "SEQUENCE_LENGTH = 15\n",
        "NUM_ACTION_TOKENS = 11\n",
        "LAYER_SIZE = 256\n",
        "VOCAB_SIZE = 512\n",
        "NUM_IMAGE_TOKENS = 81\n",
        "\n",
        "rt1x_model = RT1(\n",
        "    num_image_tokens=NUM_IMAGE_TOKENS,\n",
        "    num_action_tokens=NUM_ACTION_TOKENS,\n",
        "    layer_size=LAYER_SIZE,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    # Use token learner to reduce tokens per image to 81.\n",
        "    use_token_learner=True,\n",
        "    # RT-1-X uses (-2.0, 2.0) instead of (-1.0, 1.0).\n",
        "    world_vector_range=(-2.0, 2.0)\n",
        ")\n",
        "\n",
        "# Initialize random weights for the model and run a forward pass.\n",
        "obs = {\n",
        "    \"image\": jnp.ones((1, 15, 300, 300, 3)),\n",
        "    \"natural_language_embedding\": jnp.ones((1, 15, 512)),\n",
        "}\n",
        "act = {\n",
        "    \"world_vector\": jnp.ones((1, 15, 3)),\n",
        "    \"rotation_delta\": jnp.ones((1, 15, 3)),\n",
        "    \"gripper_closedness_action\": jnp.ones((1, 15, 1)),\n",
        "    \"base_displacement_vertical_rotation\": jnp.ones((1, 15, 1)),\n",
        "    \"base_displacement_vector\": jnp.ones((1, 15, 2)),\n",
        "    \"terminate_episode\": jnp.ones((1, 15, 3), dtype=jnp.int32),\n",
        "}\n",
        "\n",
        "variables = rt1x_model.init(\n",
        "    {\n",
        "        \"params\": jax.random.PRNGKey(0),\n",
        "        \"random\": jax.random.PRNGKey(0),\n",
        "    },\n",
        "    obs,\n",
        "    act,\n",
        "    train=False,\n",
        ")\n",
        "model_output = rt1x_model.apply(\n",
        "    variables,\n",
        "    obs,\n",
        "    act,\n",
        "    train=False,\n",
        "    rngs={\"random\": jax.random.PRNGKey(0)},\n",
        ")\n",
        "\n",
        "# Inspect the model weights and output.\n",
        "\n",
        "param_count = sum(x.size for x in jax.tree_util.tree_leaves(variables[\"params\"]))\n",
        "print(f\"Number of parameters: {param_count}\")\n",
        "\n",
        "print(f\"Output shape: {model_output.shape}.\")\n",
        "\n",
        "# Extract the actions from the model.\n",
        "time_step_tokens = (\n",
        "    NUM_IMAGE_TOKENS + NUM_ACTION_TOKENS\n",
        ")\n",
        "output_logits = jnp.reshape(\n",
        "    model_output, (1, SEQUENCE_LENGTH, time_step_tokens, -1)\n",
        ")\n",
        "action_logits = output_logits[:, -1, ...]\n",
        "action_logits = action_logits[:, NUM_IMAGE_TOKENS - 1 : -1]\n",
        "\n",
        "action_logp = jax.nn.softmax(action_logits)\n",
        "action_token = jnp.argmax(action_logp, axis=-1)\n",
        "\n",
        "action_detokenized = detokenize_action(action_token, VOCAB_SIZE, world_vector_range=(-2.0, 2.0))\n",
        "print(f\"Detokenized actions: {action_detokenized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkRGdUx5lhg8"
      },
      "source": [
        "# Training Setup and Loop\n",
        "\n",
        "* First, add some additional preprocessors in the data pipeline. This crops and resizes images and filters out terminal steps from the episode.\n",
        "* We set up the parallelism. We use a data parallel mesh and shard the batch across all of the available devices.\n",
        "* Create the `create_train_state` and `train` functions. `create_train_state` initializes the train state, while `train` is the main step function called on each train iteration. `rt1_loss` implements the cross-entropy loss for the RT-1 model.\n",
        "* We `jit` the functions above, and initialize the train state and place the input arrays on the available devices.\n",
        "* Run the train loop which iterates through the batches and calls the train step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nofx5oWB0DqY"
      },
      "outputs": [],
      "source": [
        "# @title Additional data preprocessing\n",
        "\n",
        "def _is_not_terminal(trajectory):\n",
        "  # -1 selects the final step in a trajectory\n",
        "  if trajectory[rlds.IS_TERMINAL][-1]:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def convert_dtype_and_crop_images(\n",
        "    images,\n",
        "    resize_size,\n",
        "    training: bool = True,\n",
        "    convert_dtype: bool = True,\n",
        "    seed: Optional[tf.Tensor] = None,\n",
        "):\n",
        "  \"\"\"Convert uint8 images to float32 and square crop.\n",
        "\n",
        "  Args:\n",
        "    images: [B, H, W, 3] uint8 tensor of images.\n",
        "    resize_size: (H, W) of resize.\n",
        "    training: If we are in training (random crop) or not-training (fixed crop).\n",
        "    convert_dtype: whether or not to convert the image to float32 in the range\n",
        "      of (0, 1).\n",
        "    seed: Optional seed of shape (2,) for giving to tf.random.stateless_uniform\n",
        "\n",
        "  Returns:\n",
        "    [B, crop_size, crop_size, 3] images of dtype float32.\n",
        "  \"\"\"\n",
        "\n",
        "  if seed is None:\n",
        "    seed = tf.random.uniform(shape=(2,), maxval=2**30, dtype=tf.int32)\n",
        "\n",
        "  seed2 = tf.random.experimental.stateless_split(seed, num=1)[0]\n",
        "\n",
        "  if convert_dtype:\n",
        "    images = tf.image.convert_image_dtype(images, tf.float32)\n",
        "  image_height = images.get_shape().as_list()[-3]\n",
        "  image_width = images.get_shape().as_list()[-2]\n",
        "\n",
        "  if training:\n",
        "    if image_height == 512:\n",
        "      ud_pad = 40\n",
        "      lr_pad = 100\n",
        "    elif image_height == 256:\n",
        "      ud_pad = 20\n",
        "      lr_pad = 50\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          'convert_dtype_and_crop_images only supports image height 512 or 256.'\n",
        "      )\n",
        "    max_y = 2 * ud_pad\n",
        "    max_x = 2 * lr_pad\n",
        "    images = tf.image.pad_to_bounding_box(\n",
        "        images,\n",
        "        offset_height=ud_pad,\n",
        "        offset_width=lr_pad,\n",
        "        target_height=image_height + 2 * ud_pad,\n",
        "        target_width=image_width + 2 * lr_pad,\n",
        "    )\n",
        "    offset_y = tf.random.stateless_uniform(\n",
        "        (), maxval=max_y + 1, dtype=tf.int32, seed=seed\n",
        "    )\n",
        "    offset_x = tf.random.stateless_uniform(\n",
        "        (), maxval=max_x + 1, dtype=tf.int32, seed=seed2\n",
        "    )\n",
        "    images = tf.image.crop_to_bounding_box(\n",
        "        images, offset_y, offset_x, image_height, image_width\n",
        "    )\n",
        "\n",
        "  # Add resize in pipeline for jax.\n",
        "  images = tf.image.resize(images, size=resize_size)\n",
        "  return images\n",
        "\n",
        "\n",
        "def prepare_for_model_input(\n",
        "    ds, target_height, target_width, training\n",
        "):\n",
        "  \"\"\"Removes terminal trajectory, string from features and crops image.\"\"\"\n",
        "  ds = ds.filter(_is_not_terminal)\n",
        "\n",
        "  # Remove non-jax types.\n",
        "  def _remove_str(steps):\n",
        "    if 'natural_language_instruction' in steps['observation']:\n",
        "      del steps['observation']['natural_language_instruction']\n",
        "    return steps\n",
        "\n",
        "  ds = ds.map(_remove_str, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  # Cropping augmentation.\n",
        "  def _add_crop_augmentation(step):\n",
        "    # Crop and pad augmentation. Added for jax.\n",
        "    image = step['observation']['image']\n",
        "    step['observation']['image'] = (\n",
        "        convert_dtype_and_crop_images(\n",
        "            image, (target_height, target_width), training=training\n",
        "        )\n",
        "    )\n",
        "    return step\n",
        "\n",
        "  ds = ds.map(_add_crop_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "executionInfo": {
          "elapsed": 9679,
          "status": "ok",
          "timestamp": 1702072629279,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "D_fZErEk0l9Z",
        "outputId": "c70419c8-2d5f-4554-aba9-2d568ee443d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1747095/1129709687.py:104: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  sample_batch = jax.tree_map(lambda x: x, next(train_iter))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "Local batch size: 5\n",
            "Global batch size: 5\n",
            "Devices: [cuda(id=0)]\n",
            "Sample batch keys: dict_keys(['action', 'is_first', 'is_last', 'is_terminal', 'observation'])\n"
          ]
        }
      ],
      "source": [
        "# @title Set up sharding and data parallel mesh\n",
        "\n",
        "# Actual global batch size is 1024. Use a smaller batch size for this colab\n",
        "# example.\n",
        "PER_DEVICE_BATCH_SIZE = 5\n",
        "\n",
        "def reshard(tree, shardings):\n",
        "  \"\"\"Take an arbitrarily sharded pytree and shard it according to `shardings`.\n",
        "\n",
        "  From `big_vision.utils.reshard`. See that doc for full details.\n",
        "\n",
        "  Args:\n",
        "    tree: a pytree of arrays.\n",
        "    shardings: a (prefix) pytree of jax array shardings.\n",
        "\n",
        "  Returns:\n",
        "    A pytree of global jax arrays that follows provided shardings.\n",
        "  \"\"\"\n",
        "\n",
        "  def _make_global_arr(x, shard, shape):\n",
        "    # Avoid unnecessary copies and transfers:\n",
        "    if hasattr(x, \"sharding\") and x.sharding.is_equivalent_to(\n",
        "        shard, len(shape)\n",
        "    ):  # pylint: disable=line-too-long\n",
        "      return x\n",
        "    if not getattr(x, \"is_fully_addressable\", True):\n",
        "      raise RuntimeError(\n",
        "          \"Trying to reshard a non-fully-addressable array. \"\n",
        "          \"Please see the doc-comment for detailed explanation.\"\n",
        "      )\n",
        "    x = jax.device_get(x)  # Might be on local devices.\n",
        "    xs = [\n",
        "        jax.device_put(x[s], device=d)\n",
        "        for d, s in shard.addressable_devices_indices_map(shape).items()\n",
        "    ]\n",
        "    return jax.make_array_from_single_device_arrays(shape, shard, xs)\n",
        "\n",
        "  shapes = jax.tree_map(np.shape, tree)\n",
        "  shardings = tree_broadcast(shardings, tree)\n",
        "  return jax.tree_map(_make_global_arr, tree, shardings, shapes)\n",
        "\n",
        "def tree_broadcast(prefix, target):\n",
        "  \"\"\"Broadcasts a prefix tree to a full tree.\n",
        "\n",
        "  See big_vision.utils.tree_broadcast.\n",
        "\n",
        "  Args:\n",
        "    prefix: prefix pytree.\n",
        "    target: boradcast target for a prefix tree.\n",
        "\n",
        "  Returns:\n",
        "    prefix tree broadcasted to a target tree.\n",
        "  \"\"\"\n",
        "\n",
        "  def _broadcast(leaf, subtree):\n",
        "    return jax.tree_map(lambda _: leaf, subtree)\n",
        "\n",
        "  return jax.tree_map(_broadcast, prefix, target)\n",
        "\n",
        "\n",
        "NamedSharding = jax.sharding.NamedSharding\n",
        "P = jax.sharding.PartitionSpec\n",
        "\n",
        "# TODO: Replace this with an UMI dataset.\n",
        "train_dataset = tf.data.Dataset.sample_from_datasets(datasets, weights=weights)\n",
        "\n",
        "# print first 5 elements of the dataset\n",
        "# for i, element in enumerate(train_dataset):\n",
        "#   if i >= 3:\n",
        "#     break\n",
        "  # print(element['action']['world_vector'])\n",
        "\n",
        "train_dataset = prepare_for_model_input(\n",
        "    train_dataset, target_height=300, target_width=300, training=True\n",
        ")\n",
        "\n",
        "# Creating mesh and shardings.\n",
        "num_devices = len(jax.devices())\n",
        "mesh = jax.sharding.Mesh(\n",
        "    mesh_utils.create_device_mesh((num_devices,)), (\"data\",)\n",
        ")\n",
        "\n",
        "# Data parallel mesh.\n",
        "sharding = jax.sharding.NamedSharding(mesh, P(\"data\"))\n",
        "replicate_sharding = NamedSharding(mesh, P())\n",
        "\n",
        "global_batch_size = jax.device_count() * PER_DEVICE_BATCH_SIZE\n",
        "local_batch_size = jax.local_device_count() * PER_DEVICE_BATCH_SIZE\n",
        "\n",
        "# size = 0\n",
        "# for i in train_dataset:\n",
        "#   size += 1\n",
        "# print(f\"Dataset size: {size}\")\n",
        "\n",
        "\n",
        "# train_iter = train_dataset.repeat().shuffle(300).batch(local_batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE).as_numpy_iterator()\n",
        "\n",
        "train_dataset = train_dataset.shuffle(100).repeat().batch(local_batch_size, drop_remainder=True)\n",
        "\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_iter = train_dataset.as_numpy_iterator()\n",
        "\n",
        "sample_batch = jax.tree_map(lambda x: x, next(train_iter))\n",
        "\n",
        "print(len(sample_batch['is_first']))\n",
        "\n",
        "# size = 0\n",
        "# for i in train_dataset:\n",
        "#   size += 1\n",
        "# print(f\"Dataset size: {size}\")\n",
        "\n",
        "# shuffle and get new iterator for next epoch\n",
        "def get_new_iterator():\n",
        "  print(\"Shuffling dataset and getting new iterator.\")\n",
        "  shuffled_train_dataset = train_dataset.shuffle(1000)\n",
        "  shuffled_train_iter = shuffled_train_dataset.as_numpy_iterator()\n",
        "  return shuffled_train_iter\n",
        "\n",
        "print(f\"Local batch size: {local_batch_size}\")\n",
        "print(f\"Global batch size: {global_batch_size}\")\n",
        "print(f\"Devices: {jax.devices()}\")\n",
        "print(f\"Sample batch keys: {sample_batch.keys()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBJwdoo22Oed"
      },
      "outputs": [],
      "source": [
        "# @title Create the train init fn, train step fn, and loss function.\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class TrainState:\n",
        "  step: int\n",
        "  params: Any\n",
        "  opt_state: optax.OptState\n",
        "  batch_stats: Any\n",
        "\n",
        "base_checkpoint_path = \"/home/jonathan/Thesis/ROS2_RT-1-X/ros2_ws/src/ros2_rt_1_x/ros2_rt_1_x/checkpoints/rt_1_x_jax\"\n",
        "# base_checkpoint_path = \"/home/jonathan/Thesis/open_x_embodiment/training_results/train_1721474164_lr_42_eps_1e-07/custom_rt1x_checkpoint_step_7000_epoch185_loss5.850278466823511e-05\"\n",
        "checkpoint_state_dict = checkpoints.restore_checkpoint(base_checkpoint_path, None)\n",
        "\n",
        "# Create train state based on previously saved checkpoint.\n",
        "def create_train_state_with_loaded_params(model, batch, rng, optimizer):\n",
        "\n",
        "    train_state = TrainState(\n",
        "        step=0,\n",
        "        params=checkpoint_state_dict[\"params\"],\n",
        "        opt_state=optimizer.init(checkpoint_state_dict[\"params\"]),\n",
        "        batch_stats=checkpoint_state_dict[\"batch_stats\"],\n",
        "    )\n",
        "    return train_state\n",
        "\n",
        "\n",
        "\n",
        "def create_train_state(model, batch, rng, optimizer):\n",
        "    \"\"\"Creates the train state and initial metrics for agent.\"\"\"\n",
        "    obs_input = batch[\"observation\"]\n",
        "    act_input = batch[\"action\"]\n",
        "\n",
        "    rng, rng2, rng3 = jax.random.split(rng, 3)\n",
        "    variables = model.init(\n",
        "        {\"params\": rng, \"random\": rng3},\n",
        "        obs=obs_input,\n",
        "        act=act_input,\n",
        "        train=False,\n",
        "    )\n",
        "\n",
        "    params = flax.core.unfreeze(variables[\"params\"])\n",
        "    batch_stats = flax.core.unfreeze(variables[\"batch_stats\"])\n",
        "\n",
        "    train_state = TrainState(\n",
        "        step=0,\n",
        "        params=flax.core.unfreeze(params),\n",
        "        opt_state=optimizer.init(params),\n",
        "        batch_stats=batch_stats,\n",
        "    )\n",
        "    return train_state\n",
        "\n",
        "\n",
        "def train(batch, state, model, optimizer, rng):\n",
        "  \"\"\"Performs a single training step.\"\"\"\n",
        "  rng, loss_rng = jax.random.split(rng)\n",
        "\n",
        "  def loss_fn(params):\n",
        "    variables = {\"params\": params, \"batch_stats\": state.batch_stats}\n",
        "    per_example_loss, new_variables = rt1_loss(\n",
        "        model, batch=batch, variables=variables, rng=loss_rng\n",
        "    )\n",
        "    loss = jnp.mean(per_example_loss)\n",
        "    return loss, new_variables[\"batch_stats\"]\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, new_batch_stats), grad = grad_fn(state.params)\n",
        "\n",
        "  loss = jnp.mean(loss)\n",
        "\n",
        "  updates, new_opt_state = optimizer.update(\n",
        "      grad, state.opt_state, state.params\n",
        "  )\n",
        "\n",
        "  new_params = optax.apply_updates(state.params, updates)\n",
        "  new_state = state.replace(\n",
        "      step=state.step + 1,\n",
        "      params=flax.core.unfreeze(new_params),\n",
        "      opt_state=flax.core.unfreeze(new_opt_state),\n",
        "      batch_stats=flax.core.unfreeze(new_batch_stats),\n",
        "  )\n",
        "\n",
        "  metrics_update = {\n",
        "      \"loss\": loss,\n",
        "  }\n",
        "  return new_state, metrics_update\n",
        "\n",
        "\n",
        "def rt1_loss(\n",
        "      model,\n",
        "      batch,\n",
        "      variables,\n",
        "      rng,\n",
        "  ):\n",
        "  \"\"\"Implements the RT-1 loss.\"\"\"\n",
        "  observation = batch[\"observation\"]\n",
        "  action = batch[\"action\"]\n",
        "\n",
        "  print(action)\n",
        "\n",
        "  bs = observation[\"image\"].shape[0]\n",
        "  seqlen = observation[\"image\"].shape[1]\n",
        "\n",
        "  # First, we encode the observations using the model.encode method.\n",
        "  # This will give us an observation encoding (for the entire sequence).\n",
        "  rng, params_rng = jax.random.split(rng)\n",
        "  rng, dropout_rng = jax.random.split(rng)\n",
        "  rng, sd_rng = jax.random.split(rng)\n",
        "  rng, random_rng = jax.random.split(rng)\n",
        "  logits, new_variables = model.apply(\n",
        "      variables,\n",
        "      obs=observation,\n",
        "      act=action,\n",
        "      train=True,\n",
        "      mutable=[\"batch_stats\"],\n",
        "      rngs={\n",
        "          \"params\": params_rng,\n",
        "          \"dropout\": dropout_rng,\n",
        "          \"random\": random_rng,\n",
        "      },\n",
        "  )\n",
        "\n",
        "  vocab_size = model.vocab_size\n",
        "\n",
        "  # `action` is dict of (B, T, ...), we combine actions into B*T batch to\n",
        "  # tokenize.\n",
        "  action = jax.tree_map(lambda x: jnp.reshape(x, (bs * seqlen, -1)), action)\n",
        "  labels = tokenize_action(action, vocab_size=vocab_size)\n",
        "  labels = jax.tree_map(lambda x: jnp.reshape(x, (bs, seqlen, -1)), labels)\n",
        "  labels = labels[:, :, :, None]  # labels should be (B, seqlen, 11, 1)\n",
        "\n",
        "  # Get num_action_tokens tokens for the action prediction. By default,\n",
        "  # RT-1 computes the loss for all `seqlen * num_action_tokens`, not just\n",
        "  # the final timestep's action.\n",
        "  # In the default RT-1 setup (8 img, 11 act tokens), we have:\n",
        "  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
        "  # |-----image tokens------|-------------action tokens--------------|\n",
        "  #                      |----------------logits------------------|\n",
        "  # For each time step, we want [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17] for\n",
        "  # the logits, for the \"next token\" prediction.\n",
        "  num_image_tokens = model.num_image_tokens\n",
        "  num_action_tokens = model.num_action_tokens\n",
        "  time_step_tokens = num_image_tokens + num_action_tokens\n",
        "  logits = jnp.reshape(logits, (bs, seqlen, time_step_tokens, vocab_size))\n",
        "  logits = logits[:, :, num_image_tokens - 1 : -1]\n",
        "\n",
        "  logp = jax.nn.log_softmax(logits)\n",
        "  loglik = jnp.take_along_axis(logp, labels, axis=-1)\n",
        "  loglik = jnp.mean(loglik, axis=(1, 2, 3))\n",
        "\n",
        "  return -loglik, new_variables\n",
        "\n",
        "# print(checkpoint_state_dict[\"params\"][\"Transformer_0\"].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N9PgzbdS2mEN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(('Transformer_0', 'Dense_0', 'bias'), 'trainable'), (('Transformer_0', 'Dense_0', 'kernel'), 'trainable'), (('Transformer_0', 'Dense_1', 'bias'), 'trainable'), (('Transformer_0', 'Dense_1', 'kernel'), 'trainable'), (('Transformer_0', 'Dense_2', 'bias'), 'trainable'), (('Transformer_0', 'Dense_2', 'kernel'), 'trainable'), (('Transformer_0', 'TransformerBlock_0', 'Dense_0', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'Dense_1', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'Dense_2', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'LayerNorm_0', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'LayerNorm_0', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'LayerNorm_1', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'LayerNorm_1', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'MultiHeadDotProductAttention_0', 'key', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'MultiHeadDotProductAttention_0', 'key', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'MultiHeadDotProductAttention_0', 'out', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'MultiHeadDotProductAttention_0', 'out', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'MultiHeadDotProductAttention_0', 'query', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'MultiHeadDotProductAttention_0', 'query', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'MultiHeadDotProductAttention_0', 'value', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_0', 'MultiHeadDotProductAttention_0', 'value', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'Dense_0', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'Dense_1', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'Dense_2', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'LayerNorm_0', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'LayerNorm_0', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'LayerNorm_1', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'LayerNorm_1', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'MultiHeadDotProductAttention_0', 'key', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'MultiHeadDotProductAttention_0', 'key', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'MultiHeadDotProductAttention_0', 'out', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'MultiHeadDotProductAttention_0', 'out', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'MultiHeadDotProductAttention_0', 'query', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'MultiHeadDotProductAttention_0', 'query', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'MultiHeadDotProductAttention_0', 'value', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_1', 'MultiHeadDotProductAttention_0', 'value', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'Dense_0', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'Dense_1', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'Dense_2', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'LayerNorm_0', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'LayerNorm_0', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'LayerNorm_1', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'LayerNorm_1', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'MultiHeadDotProductAttention_0', 'key', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'MultiHeadDotProductAttention_0', 'key', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'MultiHeadDotProductAttention_0', 'out', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'MultiHeadDotProductAttention_0', 'out', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'MultiHeadDotProductAttention_0', 'query', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'MultiHeadDotProductAttention_0', 'query', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'MultiHeadDotProductAttention_0', 'value', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_2', 'MultiHeadDotProductAttention_0', 'value', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'Dense_0', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'Dense_1', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'Dense_2', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'LayerNorm_0', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'LayerNorm_0', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'LayerNorm_1', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'LayerNorm_1', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'MultiHeadDotProductAttention_0', 'key', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'MultiHeadDotProductAttention_0', 'key', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'MultiHeadDotProductAttention_0', 'out', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'MultiHeadDotProductAttention_0', 'out', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'MultiHeadDotProductAttention_0', 'query', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'MultiHeadDotProductAttention_0', 'query', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'MultiHeadDotProductAttention_0', 'value', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_3', 'MultiHeadDotProductAttention_0', 'value', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'Dense_0', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'Dense_1', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'Dense_2', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'LayerNorm_0', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'LayerNorm_0', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'LayerNorm_1', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'LayerNorm_1', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'MultiHeadDotProductAttention_0', 'key', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'MultiHeadDotProductAttention_0', 'key', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'MultiHeadDotProductAttention_0', 'out', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'MultiHeadDotProductAttention_0', 'out', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'MultiHeadDotProductAttention_0', 'query', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'MultiHeadDotProductAttention_0', 'query', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'MultiHeadDotProductAttention_0', 'value', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_4', 'MultiHeadDotProductAttention_0', 'value', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'Dense_0', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'Dense_1', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'Dense_2', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'LayerNorm_0', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'LayerNorm_0', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'LayerNorm_1', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'LayerNorm_1', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'MultiHeadDotProductAttention_0', 'key', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'MultiHeadDotProductAttention_0', 'key', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'MultiHeadDotProductAttention_0', 'out', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'MultiHeadDotProductAttention_0', 'out', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'MultiHeadDotProductAttention_0', 'query', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'MultiHeadDotProductAttention_0', 'query', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'MultiHeadDotProductAttention_0', 'value', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_5', 'MultiHeadDotProductAttention_0', 'value', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'Dense_0', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'Dense_1', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'Dense_2', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'LayerNorm_0', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'LayerNorm_0', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'LayerNorm_1', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'LayerNorm_1', 'scale'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'MultiHeadDotProductAttention_0', 'key', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'MultiHeadDotProductAttention_0', 'key', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'MultiHeadDotProductAttention_0', 'out', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'MultiHeadDotProductAttention_0', 'out', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'MultiHeadDotProductAttention_0', 'query', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'MultiHeadDotProductAttention_0', 'query', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'MultiHeadDotProductAttention_0', 'value', 'bias'), 'frozen'), (('Transformer_0', 'TransformerBlock_6', 'MultiHeadDotProductAttention_0', 'value', 'kernel'), 'frozen'), (('Transformer_0', 'TransformerBlock_7', 'Dense_0', 'kernel'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'Dense_1', 'kernel'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'Dense_2', 'kernel'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'LayerNorm_0', 'bias'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'LayerNorm_0', 'scale'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'LayerNorm_1', 'bias'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'LayerNorm_1', 'scale'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'MultiHeadDotProductAttention_0', 'key', 'bias'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'MultiHeadDotProductAttention_0', 'key', 'kernel'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'MultiHeadDotProductAttention_0', 'out', 'bias'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'MultiHeadDotProductAttention_0', 'out', 'kernel'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'MultiHeadDotProductAttention_0', 'query', 'bias'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'MultiHeadDotProductAttention_0', 'query', 'kernel'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'MultiHeadDotProductAttention_0', 'value', 'bias'), 'trainable'), (('Transformer_0', 'TransformerBlock_7', 'MultiHeadDotProductAttention_0', 'value', 'kernel'), 'trainable'), (('image_tokenizer', 'Conv_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_0', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_0', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_0', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_0', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_1', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_1', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_1', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_1', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_10', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_10', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_10', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_10', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_11', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_11', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_11', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_11', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_12', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_12', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_12', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_12', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_13', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_13', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_13', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_13', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_14', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_14', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_14', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_14', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_15', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_15', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_15', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_15', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_16', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_16', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_16', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_16', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_17', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_17', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_17', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_17', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_18', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_18', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_18', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_18', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_19', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_19', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_19', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_19', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_2', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_2', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_2', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_2', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_20', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_20', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_20', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_20', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_21', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_21', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_21', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_21', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_22', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_22', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_22', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_22', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_23', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_23', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_23', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_23', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_24', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_24', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_24', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_24', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_25', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_25', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_25', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_25', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_3', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_3', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_3', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_3', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_4', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_4', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_4', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_4', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_5', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_5', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_5', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_5', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_6', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_6', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_6', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_6', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_7', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_7', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_7', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_7', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_8', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_8', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_8', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_8', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_9', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_9', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_9', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'FilmConditioning_9', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'Head_0', 'batch_normalization', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'Head_0', 'batch_normalization', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'Head_0', 'conv2d', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_0', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_1', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_10', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_11', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_12', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_13', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_14', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_15', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_16', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_17', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_18', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_19', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_2', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_20', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_21', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_22', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_23', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_24', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_25', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_3', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_4', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_5', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_6', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_7', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_8', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'SqueezeExcite_0', 'expand_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'SqueezeExcite_0', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'SqueezeExcite_0', 'reduce_conv2d_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'SqueezeExcite_0', 'reduce_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'batch_normalization_0', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'batch_normalization_0', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'batch_normalization_1', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'batch_normalization_1', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'batch_normalization_2', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'batch_normalization_2', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'depthwise_conv2d', 'depthwise_kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'expand_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'MBConvBlock_9', 'project_conv2d_0', 'kernel'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'Stem_0', 'batch_normalization', 'bias'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'Stem_0', 'batch_normalization', 'scale'), 'trainable'), (('image_tokenizer', 'EfficientNetWithFilm_0', 'Stem_0', 'conv2d', 'kernel'), 'trainable'), (('image_tokenizer', 'FilmConditioning_0', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'FilmConditioning_0', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'FilmConditioning_0', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'FilmConditioning_0', 'Dense_1', 'kernel'), 'trainable'), (('image_tokenizer', 'TokenLearnerModuleV11_0', 'LayerNorm_0', 'bias'), 'trainable'), (('image_tokenizer', 'TokenLearnerModuleV11_0', 'LayerNorm_0', 'scale'), 'trainable'), (('image_tokenizer', 'TokenLearnerModuleV11_0', 'token_masking', 'Dense_0', 'bias'), 'trainable'), (('image_tokenizer', 'TokenLearnerModuleV11_0', 'token_masking', 'Dense_0', 'kernel'), 'trainable'), (('image_tokenizer', 'TokenLearnerModuleV11_0', 'token_masking', 'Dense_1', 'bias'), 'trainable'), (('image_tokenizer', 'TokenLearnerModuleV11_0', 'token_masking', 'Dense_1', 'kernel'), 'trainable')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1747095/153420822.py:46: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  global_data_shape = jax.tree_map(\n",
            "/tmp/ipykernel_1747095/153420822.py:65: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  sample_batch = jax.tree_map(_form_gda, sample_batch, global_data_shape)\n"
          ]
        }
      ],
      "source": [
        "# @title Set up the functions for training\n",
        "\n",
        "UMI_LEARNING_RATE = 1e-6\n",
        "UMI_EPSILON = 1e-7\n",
        "\n",
        "# LEARNING RATE FROM OCTO\n",
        "octo_learning_rate = optax.join_schedules(\n",
        "    [optax.linear_schedule(0, 3e-5, 100), optax.constant_schedule(3e-5)], [100]\n",
        ")\n",
        "\n",
        "optimizer = optax.adam(learning_rate=UMI_LEARNING_RATE, eps=UMI_EPSILON)\n",
        "\n",
        "# FREEZE SOME LAYERS\n",
        "partition_optimizers = {'trainable': optimizer, 'frozen': optax.set_to_zero()}\n",
        "\n",
        "param_partitions = traverse_util.path_aware_map(\n",
        "  lambda path, v: 'frozen' if ('TransformerBlock_0' in path or 'TransformerBlock_1' in path or 'TransformerBlock_2' in path or 'TransformerBlock_3' in path or 'TransformerBlock_4' in path or 'TransformerBlock_5' in path or 'TransformerBlock_6' in path) else 'trainable', checkpoint_state_dict[\"params\"])\n",
        "tx = optax.multi_transform(partition_optimizers, param_partitions)\n",
        "\n",
        "tx = optimizer\n",
        "\n",
        "# visualize a subset of the param_partitions structure\n",
        "flat = list(traverse_util.flatten_dict(param_partitions).items())\n",
        "# print(traverse_util.unflatten_dict(dict(flat[:3] + flat[-3:])))\n",
        "print(flat)\n",
        "\n",
        "# Create the train state.\n",
        "# input: batch, rng, ds_info\n",
        "# output: state\n",
        "agent_create_train_state = functools.partial(\n",
        "    create_train_state, model=rt1x_model, optimizer=optimizer\n",
        ")\n",
        "create_train_state_jit = jax.jit(\n",
        "    agent_create_train_state,\n",
        "    out_shardings=replicate_sharding,\n",
        ")\n",
        "\n",
        "agent_create_train_state_with_loaded_params = functools.partial(\n",
        "    create_train_state_with_loaded_params, model=rt1x_model, optimizer=tx\n",
        ")\n",
        "create_train_state_with_loaded_params_jit = jax.jit(\n",
        "    agent_create_train_state_with_loaded_params,\n",
        "    out_shardings=replicate_sharding,\n",
        ")\n",
        "\n",
        "global_data_shape = jax.tree_map(\n",
        "    lambda x: (global_batch_size,) + x.shape[1:], sample_batch\n",
        ")\n",
        "\n",
        "local_devices = mesh.local_devices\n",
        "local_device_count = jax.local_device_count()\n",
        "\n",
        "def _put_to_devices(x):\n",
        "  per_device_arrays = np.split(x, local_device_count, axis=0)\n",
        "  return jax.device_put(per_device_arrays, local_devices)\n",
        "\n",
        "def _form_gda(local_data, global_shape):\n",
        "  arrays = _put_to_devices(local_data)\n",
        "  return jax.make_array_from_single_device_arrays(\n",
        "      global_shape, sharding, arrays\n",
        "  )\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "sample_batch = jax.tree_map(_form_gda, sample_batch, global_data_shape)\n",
        "rng, agent_rng = jax.random.split(rng)\n",
        "\n",
        "# CHANGE HERE FOR CHECKPOINT\n",
        "\n",
        "# state = create_train_state_jit(\n",
        "#     batch=sample_batch, rng=agent_rng\n",
        "# )\n",
        "state = create_train_state_with_loaded_params_jit(\n",
        "    batch=sample_batch, rng=agent_rng\n",
        ")\n",
        "\n",
        "# Create the train step.\n",
        "agent_train = functools.partial(train, model=rt1x_model, optimizer=tx)\n",
        "jitted_train_step = jax.jit(\n",
        "    agent_train,\n",
        "    out_shardings=(replicate_sharding, replicate_sharding),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ3mdV4D5C9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1747095/1129709687.py:38: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  shapes = jax.tree_map(np.shape, tree)\n",
            "/tmp/ipykernel_1747095/1129709687.py:58: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  return jax.tree_map(_broadcast, prefix, target)\n",
            "/tmp/ipykernel_1747095/1129709687.py:56: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  return jax.tree_map(lambda _: leaf, subtree)\n",
            "/tmp/ipykernel_1747095/1129709687.py:40: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  return jax.tree_map(_make_global_arr, tree, shardings, shapes)\n",
            "/tmp/ipykernel_1747095/4216287265.py:42: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  batch = jax.tree_map(lambda x: x, next(train_iter))\n",
            "/tmp/ipykernel_1747095/4216287265.py:43: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  batch = jax.tree_map(_form_gda, batch, global_data_shape)\n",
            "WARNING:absl:SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before May 1st, 2024. If your Pytree has empty ([], {}, None) values then use PyTreeCheckpointHandler(..., write_tree_metadata=True, ...) or use StandardCheckpointHandler to avoid TypeHandler Registry error. Please note that PyTreeCheckpointHandler.write_tree_metadata default value is already set to True.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint after epoch 1.\n",
            "{'base_displacement_vector': Traced<ShapedArray(float32[5,15,2])>with<DynamicJaxprTrace(level=1/0)>, 'base_displacement_vertical_rotation': Traced<ShapedArray(float32[5,15,1])>with<DynamicJaxprTrace(level=1/0)>, 'gripper_closedness_action': Traced<ShapedArray(float32[5,15,1])>with<DynamicJaxprTrace(level=1/0)>, 'rotation_delta': Traced<ShapedArray(float32[5,15,3])>with<DynamicJaxprTrace(level=1/0)>, 'terminate_episode': Traced<ShapedArray(int32[5,15,3])>with<DynamicJaxprTrace(level=1/0)>, 'world_vector': Traced<ShapedArray(float32[5,15,3])>with<DynamicJaxprTrace(level=1/0)>}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1747095/1781465395.py:125: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  action = jax.tree_map(lambda x: jnp.reshape(x, (bs * seqlen, -1)), action)\n",
            "/tmp/ipykernel_1747095/1781465395.py:127: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  labels = jax.tree_map(lambda x: jnp.reshape(x, (bs, seqlen, -1)), labels)\n",
            "E0720 19:00:14.549527 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:14.653255 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:16.508116 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:16.660957 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:18.796033 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:18.881410 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:20.467976 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:20.553960 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:20.639752 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:20.726617 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:20.813331 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:20.899339 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:23.479699 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:23.659427 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:28.028826 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:28.114107 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:28.447360 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:28.532349 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:30.023780 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:30.109492 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:30.344456 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0720 19:00:30.437554 1747095 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics: step=0 (2), epoch=1 {'loss': array(6.403913, dtype=float32)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1747095/4216287265.py:42: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  batch = jax.tree_map(lambda x: x, next(train_iter))\n",
            "/tmp/ipykernel_1747095/4216287265.py:43: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  batch = jax.tree_map(_form_gda, batch, global_data_shape)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics: step=10 (12), epoch=1 {'loss': array(6.667052, dtype=float32)}\n",
            "Metrics: step=20 (22), epoch=1 {'loss': array(6.7212067, dtype=float32)}\n",
            "Metrics: step=30 (32), epoch=1 {'loss': array(5.45981, dtype=float32)}\n",
            "Metrics: step=40 (42), epoch=1 {'loss': array(4.586323, dtype=float32)}\n",
            "Metrics: step=50 (52), epoch=1 {'loss': array(4.623955, dtype=float32)}\n",
            "Metrics: step=60 (62), epoch=1 {'loss': array(4.206703, dtype=float32)}\n",
            "Metrics: step=70 (72), epoch=1 {'loss': array(4.610414, dtype=float32)}\n",
            "Metrics: step=80 (82), epoch=1 {'loss': array(4.793424, dtype=float32)}\n",
            "Metrics: step=90 (92), epoch=1 {'loss': array(3.9418232, dtype=float32)}\n",
            "Metrics: step=100 (102), epoch=1 {'loss': array(3.2206256, dtype=float32)}\n",
            "Metrics: step=110 (112), epoch=1 {'loss': array(3.974955, dtype=float32)}\n",
            "Metrics: step=120 (122), epoch=1 {'loss': array(3.0205586, dtype=float32)}\n",
            "Metrics: step=130 (132), epoch=1 {'loss': array(2.4036424, dtype=float32)}\n",
            "Metrics: step=140 (142), epoch=1 {'loss': array(2.7739758, dtype=float32)}\n",
            "Metrics: step=150 (152), epoch=1 {'loss': array(3.4371278, dtype=float32)}\n",
            "Metrics: step=160 (162), epoch=1 {'loss': array(2.3179233, dtype=float32)}\n",
            "Metrics: step=170 (172), epoch=1 {'loss': array(2.0926206, dtype=float32)}\n",
            "Metrics: step=180 (182), epoch=1 {'loss': array(2.023343, dtype=float32)}\n",
            "Metrics: step=190 (192), epoch=1 {'loss': array(2.571409, dtype=float32)}\n",
            "Metrics: step=200 (202), epoch=1 {'loss': array(1.8507159, dtype=float32)}\n",
            "Metrics: step=210 (212), epoch=1 {'loss': array(2.454519, dtype=float32)}\n",
            "Metrics: step=220 (222), epoch=1 {'loss': array(1.4681326, dtype=float32)}\n",
            "Metrics: step=230 (232), epoch=1 {'loss': array(2.1685395, dtype=float32)}\n",
            "Metrics: step=240 (242), epoch=1 {'loss': array(1.6627667, dtype=float32)}\n",
            "Metrics: step=250 (252), epoch=1 {'loss': array(2.1793938, dtype=float32)}\n",
            "Metrics: step=260 (262), epoch=1 {'loss': array(1.4108669, dtype=float32)}\n",
            "Metrics: step=270 (272), epoch=1 {'loss': array(1.5755638, dtype=float32)}\n",
            "Metrics: step=280 (282), epoch=1 {'loss': array(1.8714079, dtype=float32)}\n",
            "Metrics: step=290 (292), epoch=1 {'loss': array(2.4627903, dtype=float32)}\n",
            "Metrics: step=300 (302), epoch=1 {'loss': array(1.7326568, dtype=float32)}\n",
            "Metrics: step=310 (312), epoch=1 {'loss': array(1.5157878, dtype=float32)}\n",
            "Metrics: step=320 (322), epoch=1 {'loss': array(1.1602532, dtype=float32)}\n",
            "Metrics: step=330 (332), epoch=1 {'loss': array(1.1850499, dtype=float32)}\n",
            "Metrics: step=340 (342), epoch=1 {'loss': array(1.0476398, dtype=float32)}\n",
            "Metrics: step=350 (352), epoch=1 {'loss': array(1.2104992, dtype=float32)}\n",
            "Metrics: step=360 (362), epoch=1 {'loss': array(1.375031, dtype=float32)}\n",
            "Metrics: step=370 (372), epoch=1 {'loss': array(1.3178144, dtype=float32)}\n",
            "Metrics: step=380 (382), epoch=1 {'loss': array(1.1957774, dtype=float32)}\n",
            "Metrics: step=390 (392), epoch=1 {'loss': array(1.1781254, dtype=float32)}\n",
            "Metrics: step=400 (402), epoch=1 {'loss': array(1.06385, dtype=float32)}\n",
            "Metrics: step=410 (412), epoch=1 {'loss': array(1.627545, dtype=float32)}\n",
            "Metrics: step=420 (422), epoch=1 {'loss': array(1.204346, dtype=float32)}\n",
            "Metrics: step=430 (432), epoch=1 {'loss': array(1.132239, dtype=float32)}\n",
            "Metrics: step=440 (442), epoch=1 {'loss': array(1.499097, dtype=float32)}\n",
            "Metrics: step=450 (452), epoch=1 {'loss': array(0.9418351, dtype=float32)}\n",
            "Metrics: step=460 (462), epoch=1 {'loss': array(1.1374277, dtype=float32)}\n",
            "Metrics: step=470 (472), epoch=1 {'loss': array(1.1470743, dtype=float32)}\n",
            "Metrics: step=480 (482), epoch=1 {'loss': array(1.1505862, dtype=float32)}\n",
            "Metrics: step=490 (492), epoch=1 {'loss': array(0.8200157, dtype=float32)}\n",
            "Saved checkpoint after epoch 1.\n",
            "Metrics: step=500 (502), epoch=1 {'loss': array(0.88370055, dtype=float32)}\n",
            "Metrics: step=510 (512), epoch=1 {'loss': array(1.059658, dtype=float32)}\n",
            "Metrics: step=520 (522), epoch=1 {'loss': array(1.1615683, dtype=float32)}\n",
            "Metrics: step=530 (532), epoch=1 {'loss': array(0.93355983, dtype=float32)}\n",
            "Metrics: step=540 (542), epoch=1 {'loss': array(1.2868056, dtype=float32)}\n",
            "Metrics: step=550 (552), epoch=1 {'loss': array(0.79769343, dtype=float32)}\n",
            "Metrics: step=560 (562), epoch=1 {'loss': array(1.2847108, dtype=float32)}\n",
            "Metrics: step=570 (572), epoch=1 {'loss': array(1.2781748, dtype=float32)}\n",
            "Metrics: step=580 (582), epoch=1 {'loss': array(1.1017213, dtype=float32)}\n",
            "Metrics: step=590 (592), epoch=1 {'loss': array(1.1853452, dtype=float32)}\n",
            "Metrics: step=600 (602), epoch=1 {'loss': array(0.6518165, dtype=float32)}\n",
            "Metrics: step=610 (612), epoch=1 {'loss': array(0.5927011, dtype=float32)}\n",
            "Metrics: step=620 (622), epoch=1 {'loss': array(1.1017238, dtype=float32)}\n",
            "Metrics: step=630 (632), epoch=1 {'loss': array(0.76355773, dtype=float32)}\n",
            "Metrics: step=640 (642), epoch=1 {'loss': array(0.8475093, dtype=float32)}\n",
            "Metrics: step=650 (652), epoch=1 {'loss': array(0.8179969, dtype=float32)}\n",
            "Metrics: step=660 (662), epoch=1 {'loss': array(1.1249803, dtype=float32)}\n",
            "Metrics: step=670 (672), epoch=1 {'loss': array(1.1055651, dtype=float32)}\n",
            "Metrics: step=680 (682), epoch=1 {'loss': array(1.6811012, dtype=float32)}\n",
            "Metrics: step=690 (692), epoch=1 {'loss': array(1.3080162, dtype=float32)}\n",
            "Metrics: step=700 (702), epoch=1 {'loss': array(0.6976789, dtype=float32)}\n",
            "Metrics: step=710 (712), epoch=1 {'loss': array(0.68080145, dtype=float32)}\n",
            "Metrics: step=720 (722), epoch=1 {'loss': array(0.6477774, dtype=float32)}\n",
            "Metrics: step=730 (732), epoch=1 {'loss': array(0.5937721, dtype=float32)}\n",
            "Metrics: step=740 (742), epoch=1 {'loss': array(0.7150108, dtype=float32)}\n",
            "Metrics: step=750 (752), epoch=1 {'loss': array(1.8547443, dtype=float32)}\n",
            "Metrics: step=760 (762), epoch=1 {'loss': array(0.96325505, dtype=float32)}\n",
            "Metrics: step=770 (772), epoch=1 {'loss': array(0.731787, dtype=float32)}\n",
            "Metrics: step=780 (782), epoch=1 {'loss': array(0.6735241, dtype=float32)}\n",
            "Metrics: step=790 (792), epoch=1 {'loss': array(0.79401195, dtype=float32)}\n",
            "Metrics: step=800 (802), epoch=1 {'loss': array(0.77654773, dtype=float32)}\n",
            "Metrics: step=810 (812), epoch=1 {'loss': array(0.76612777, dtype=float32)}\n",
            "Metrics: step=820 (822), epoch=1 {'loss': array(1.0286322, dtype=float32)}\n",
            "Metrics: step=830 (832), epoch=1 {'loss': array(0.79214996, dtype=float32)}\n",
            "Metrics: step=840 (842), epoch=1 {'loss': array(0.9632646, dtype=float32)}\n",
            "Metrics: step=850 (852), epoch=1 {'loss': array(0.7155863, dtype=float32)}\n",
            "Metrics: step=860 (862), epoch=1 {'loss': array(0.7793627, dtype=float32)}\n",
            "Metrics: step=870 (872), epoch=1 {'loss': array(0.92665523, dtype=float32)}\n",
            "Metrics: step=880 (882), epoch=1 {'loss': array(0.58096963, dtype=float32)}\n",
            "Metrics: step=890 (892), epoch=1 {'loss': array(0.7133343, dtype=float32)}\n",
            "Metrics: step=900 (902), epoch=1 {'loss': array(0.6927819, dtype=float32)}\n",
            "Metrics: step=910 (912), epoch=1 {'loss': array(0.74820024, dtype=float32)}\n",
            "Metrics: step=920 (922), epoch=1 {'loss': array(0.61610454, dtype=float32)}\n",
            "Metrics: step=930 (932), epoch=1 {'loss': array(0.95988715, dtype=float32)}\n",
            "Metrics: step=940 (942), epoch=1 {'loss': array(0.59648, dtype=float32)}\n",
            "Metrics: step=950 (952), epoch=1 {'loss': array(0.6650602, dtype=float32)}\n",
            "Metrics: step=960 (962), epoch=1 {'loss': array(0.71939915, dtype=float32)}\n",
            "Metrics: step=970 (972), epoch=1 {'loss': array(1.1173081, dtype=float32)}\n",
            "Metrics: step=980 (982), epoch=1 {'loss': array(0.48768488, dtype=float32)}\n",
            "Metrics: step=990 (992), epoch=1 {'loss': array(1.0777568, dtype=float32)}\n",
            "Saved checkpoint after epoch 1.\n",
            "Metrics: step=1000 (1002), epoch=1 {'loss': array(0.6703281, dtype=float32)}\n",
            "Metrics: step=1010 (1012), epoch=1 {'loss': array(0.5268814, dtype=float32)}\n",
            "Metrics: step=1020 (1022), epoch=1 {'loss': array(0.885073, dtype=float32)}\n",
            "Metrics: step=1030 (1032), epoch=1 {'loss': array(0.5497403, dtype=float32)}\n",
            "Metrics: step=1040 (1042), epoch=1 {'loss': array(0.6369862, dtype=float32)}\n",
            "Metrics: step=1050 (1052), epoch=1 {'loss': array(0.65005815, dtype=float32)}\n",
            "Metrics: step=1060 (1062), epoch=1 {'loss': array(1.0613943, dtype=float32)}\n",
            "Metrics: step=1070 (1072), epoch=1 {'loss': array(0.4982632, dtype=float32)}\n",
            "Metrics: step=1080 (1082), epoch=1 {'loss': array(0.9867509, dtype=float32)}\n",
            "Metrics: step=1090 (1092), epoch=1 {'loss': array(0.670726, dtype=float32)}\n",
            "Metrics: step=1100 (1102), epoch=1 {'loss': array(1.0026441, dtype=float32)}\n",
            "Metrics: step=1110 (1112), epoch=1 {'loss': array(0.6035554, dtype=float32)}\n",
            "Metrics: step=1120 (1122), epoch=1 {'loss': array(0.35400692, dtype=float32)}\n",
            "Metrics: step=1130 (1132), epoch=1 {'loss': array(0.5994277, dtype=float32)}\n",
            "Metrics: step=1140 (1142), epoch=1 {'loss': array(0.33219025, dtype=float32)}\n",
            "Metrics: step=1150 (1152), epoch=1 {'loss': array(0.53404915, dtype=float32)}\n",
            "Metrics: step=1160 (1162), epoch=1 {'loss': array(0.63590384, dtype=float32)}\n",
            "Metrics: step=1170 (1172), epoch=1 {'loss': array(0.57379574, dtype=float32)}\n",
            "Metrics: step=1180 (1182), epoch=1 {'loss': array(0.5055968, dtype=float32)}\n",
            "Metrics: step=1190 (1192), epoch=1 {'loss': array(0.5915954, dtype=float32)}\n",
            "Metrics: step=1200 (1202), epoch=1 {'loss': array(0.47931758, dtype=float32)}\n",
            "Metrics: step=1210 (1212), epoch=1 {'loss': array(0.662839, dtype=float32)}\n",
            "Metrics: step=1220 (1222), epoch=1 {'loss': array(0.56493247, dtype=float32)}\n",
            "Metrics: step=1230 (1232), epoch=1 {'loss': array(0.41822538, dtype=float32)}\n",
            "Metrics: step=1240 (1242), epoch=1 {'loss': array(0.7200548, dtype=float32)}\n",
            "Metrics: step=1250 (1252), epoch=1 {'loss': array(0.34681675, dtype=float32)}\n",
            "Metrics: step=1260 (1262), epoch=1 {'loss': array(0.52569497, dtype=float32)}\n",
            "Metrics: step=1270 (1272), epoch=1 {'loss': array(0.5984497, dtype=float32)}\n",
            "Metrics: step=1280 (1282), epoch=1 {'loss': array(0.8856541, dtype=float32)}\n",
            "Metrics: step=1290 (1292), epoch=1 {'loss': array(0.6934595, dtype=float32)}\n",
            "Metrics: step=1300 (1302), epoch=1 {'loss': array(0.48695683, dtype=float32)}\n",
            "Metrics: step=1310 (1312), epoch=1 {'loss': array(0.8295708, dtype=float32)}\n",
            "Metrics: step=1320 (1322), epoch=1 {'loss': array(0.6603283, dtype=float32)}\n",
            "Metrics: step=1330 (1332), epoch=1 {'loss': array(1.1465118, dtype=float32)}\n",
            "Metrics: step=1340 (1342), epoch=1 {'loss': array(0.5690462, dtype=float32)}\n",
            "Metrics: step=1350 (1352), epoch=1 {'loss': array(0.49158064, dtype=float32)}\n",
            "Metrics: step=1360 (1362), epoch=1 {'loss': array(1.1150888, dtype=float32)}\n",
            "Metrics: step=1370 (1372), epoch=1 {'loss': array(0.43233195, dtype=float32)}\n",
            "Metrics: step=1380 (1382), epoch=1 {'loss': array(0.54076356, dtype=float32)}\n",
            "Metrics: step=1390 (1392), epoch=1 {'loss': array(0.52942973, dtype=float32)}\n",
            "Metrics: step=1400 (1402), epoch=1 {'loss': array(0.909682, dtype=float32)}\n",
            "Metrics: step=1410 (1412), epoch=1 {'loss': array(0.44001767, dtype=float32)}\n",
            "Metrics: step=1420 (1422), epoch=1 {'loss': array(0.895739, dtype=float32)}\n",
            "Metrics: step=1430 (1432), epoch=1 {'loss': array(0.41170478, dtype=float32)}\n",
            "Metrics: step=1440 (1442), epoch=1 {'loss': array(0.65318936, dtype=float32)}\n",
            "Metrics: step=1450 (1452), epoch=1 {'loss': array(0.52441114, dtype=float32)}\n",
            "Metrics: step=1460 (1462), epoch=1 {'loss': array(0.42044273, dtype=float32)}\n",
            "Metrics: step=1470 (1472), epoch=1 {'loss': array(0.6257437, dtype=float32)}\n",
            "Metrics: step=1480 (1482), epoch=1 {'loss': array(0.40044075, dtype=float32)}\n",
            "Metrics: step=1490 (1492), epoch=1 {'loss': array(0.3865001, dtype=float32)}\n",
            "Saved checkpoint after epoch 1.\n",
            "Metrics: step=1500 (1502), epoch=1 {'loss': array(0.74671346, dtype=float32)}\n",
            "Metrics: step=1510 (1512), epoch=1 {'loss': array(0.35894036, dtype=float32)}\n",
            "Metrics: step=1520 (1522), epoch=1 {'loss': array(0.4074133, dtype=float32)}\n",
            "Metrics: step=1530 (1532), epoch=1 {'loss': array(0.5834393, dtype=float32)}\n",
            "Metrics: step=1540 (1542), epoch=1 {'loss': array(0.5202035, dtype=float32)}\n",
            "Metrics: step=1550 (1552), epoch=1 {'loss': array(0.663718, dtype=float32)}\n",
            "Metrics: step=1560 (1562), epoch=1 {'loss': array(0.43107352, dtype=float32)}\n",
            "Metrics: step=1570 (1572), epoch=1 {'loss': array(0.34887084, dtype=float32)}\n",
            "Metrics: step=1580 (1582), epoch=1 {'loss': array(0.41339046, dtype=float32)}\n",
            "Metrics: step=1590 (1592), epoch=1 {'loss': array(0.38352892, dtype=float32)}\n",
            "Metrics: step=1600 (1602), epoch=1 {'loss': array(0.6926166, dtype=float32)}\n",
            "Metrics: step=1610 (1612), epoch=1 {'loss': array(0.30702236, dtype=float32)}\n",
            "Metrics: step=1620 (1622), epoch=1 {'loss': array(0.80740815, dtype=float32)}\n",
            "Metrics: step=1630 (1632), epoch=1 {'loss': array(0.57222635, dtype=float32)}\n",
            "Metrics: step=1640 (1642), epoch=1 {'loss': array(0.31274566, dtype=float32)}\n",
            "Metrics: step=1650 (1652), epoch=1 {'loss': array(0.3414326, dtype=float32)}\n",
            "Metrics: step=1660 (1662), epoch=1 {'loss': array(0.56860465, dtype=float32)}\n",
            "Metrics: step=1670 (1672), epoch=1 {'loss': array(0.2500332, dtype=float32)}\n",
            "Metrics: step=1680 (1682), epoch=1 {'loss': array(0.2934589, dtype=float32)}\n",
            "Metrics: step=1690 (1692), epoch=1 {'loss': array(0.29018384, dtype=float32)}\n",
            "Metrics: step=1700 (1702), epoch=1 {'loss': array(0.34821674, dtype=float32)}\n",
            "Metrics: step=1710 (1712), epoch=1 {'loss': array(0.5986976, dtype=float32)}\n",
            "Metrics: step=1720 (1722), epoch=1 {'loss': array(0.32427764, dtype=float32)}\n",
            "Metrics: step=1730 (1732), epoch=1 {'loss': array(0.22323237, dtype=float32)}\n",
            "Metrics: step=1740 (1742), epoch=1 {'loss': array(0.40792748, dtype=float32)}\n",
            "Metrics: step=1750 (1752), epoch=1 {'loss': array(0.40462324, dtype=float32)}\n",
            "Metrics: step=1760 (1762), epoch=1 {'loss': array(0.51986545, dtype=float32)}\n",
            "Metrics: step=1770 (1772), epoch=1 {'loss': array(0.2512544, dtype=float32)}\n",
            "Metrics: step=1780 (1782), epoch=1 {'loss': array(0.3969104, dtype=float32)}\n",
            "Metrics: step=1790 (1792), epoch=1 {'loss': array(0.31604922, dtype=float32)}\n",
            "Metrics: step=1800 (1802), epoch=1 {'loss': array(0.32227984, dtype=float32)}\n",
            "Metrics: step=1810 (1812), epoch=1 {'loss': array(0.40889546, dtype=float32)}\n",
            "Metrics: step=1820 (1822), epoch=1 {'loss': array(0.6458826, dtype=float32)}\n",
            "Metrics: step=1830 (1832), epoch=1 {'loss': array(0.28716585, dtype=float32)}\n",
            "Metrics: step=1840 (1842), epoch=1 {'loss': array(0.24919024, dtype=float32)}\n",
            "Metrics: step=1850 (1852), epoch=1 {'loss': array(0.57961565, dtype=float32)}\n",
            "Metrics: step=1860 (1862), epoch=1 {'loss': array(0.36200762, dtype=float32)}\n",
            "Metrics: step=1870 (1872), epoch=1 {'loss': array(0.35317984, dtype=float32)}\n",
            "Metrics: step=1880 (1882), epoch=1 {'loss': array(0.33531377, dtype=float32)}\n",
            "Metrics: step=1890 (1892), epoch=1 {'loss': array(0.50684893, dtype=float32)}\n",
            "Metrics: step=1900 (1902), epoch=1 {'loss': array(0.48591805, dtype=float32)}\n",
            "Metrics: step=1910 (1912), epoch=1 {'loss': array(0.2630914, dtype=float32)}\n",
            "Metrics: step=1920 (1922), epoch=1 {'loss': array(0.32627282, dtype=float32)}\n",
            "Metrics: step=1930 (1932), epoch=1 {'loss': array(0.27715164, dtype=float32)}\n",
            "Metrics: step=1940 (1942), epoch=1 {'loss': array(0.3909119, dtype=float32)}\n",
            "Metrics: step=1950 (1952), epoch=1 {'loss': array(0.30036142, dtype=float32)}\n",
            "Metrics: step=1960 (1962), epoch=1 {'loss': array(0.24860048, dtype=float32)}\n",
            "Metrics: step=1970 (1972), epoch=1 {'loss': array(0.25701877, dtype=float32)}\n",
            "Metrics: step=1980 (1982), epoch=1 {'loss': array(0.22736578, dtype=float32)}\n",
            "Metrics: step=1990 (1992), epoch=1 {'loss': array(0.19563636, dtype=float32)}\n",
            "Saved checkpoint after epoch 1.\n",
            "Metrics: step=2000 (2002), epoch=1 {'loss': array(0.40887818, dtype=float32)}\n",
            "Metrics: step=2010 (2012), epoch=1 {'loss': array(0.28947076, dtype=float32)}\n",
            "Metrics: step=2020 (2022), epoch=1 {'loss': array(0.31082165, dtype=float32)}\n",
            "Metrics: step=2030 (2032), epoch=1 {'loss': array(0.4735427, dtype=float32)}\n",
            "Metrics: step=2040 (2042), epoch=1 {'loss': array(0.53015447, dtype=float32)}\n",
            "Metrics: step=2050 (2052), epoch=1 {'loss': array(0.21872146, dtype=float32)}\n",
            "Metrics: step=2060 (2062), epoch=1 {'loss': array(0.23254521, dtype=float32)}\n",
            "Metrics: step=2070 (2072), epoch=1 {'loss': array(0.3061258, dtype=float32)}\n",
            "Metrics: step=2080 (2082), epoch=1 {'loss': array(0.2243959, dtype=float32)}\n",
            "Metrics: step=2090 (2092), epoch=1 {'loss': array(0.3729203, dtype=float32)}\n",
            "Metrics: step=2100 (2102), epoch=1 {'loss': array(0.26205775, dtype=float32)}\n",
            "Metrics: step=2110 (2112), epoch=1 {'loss': array(0.29955873, dtype=float32)}\n",
            "Metrics: step=2120 (2122), epoch=1 {'loss': array(0.34940168, dtype=float32)}\n",
            "Metrics: step=2130 (2132), epoch=1 {'loss': array(0.21001878, dtype=float32)}\n",
            "Metrics: step=2140 (2142), epoch=1 {'loss': array(0.22834392, dtype=float32)}\n",
            "Metrics: step=2150 (2152), epoch=1 {'loss': array(0.31939435, dtype=float32)}\n",
            "Metrics: step=2160 (2162), epoch=1 {'loss': array(0.28234076, dtype=float32)}\n",
            "Metrics: step=2170 (2172), epoch=1 {'loss': array(0.38365865, dtype=float32)}\n",
            "Metrics: step=2180 (2182), epoch=1 {'loss': array(0.20907824, dtype=float32)}\n",
            "Metrics: step=2190 (2192), epoch=1 {'loss': array(0.29531604, dtype=float32)}\n",
            "Metrics: step=2200 (2202), epoch=1 {'loss': array(0.3343695, dtype=float32)}\n",
            "Metrics: step=2210 (2212), epoch=1 {'loss': array(0.36999932, dtype=float32)}\n",
            "Metrics: step=2220 (2222), epoch=1 {'loss': array(0.19086511, dtype=float32)}\n",
            "Metrics: step=2230 (2232), epoch=1 {'loss': array(0.23835509, dtype=float32)}\n",
            "Metrics: step=2240 (2242), epoch=1 {'loss': array(0.35264584, dtype=float32)}\n",
            "Metrics: step=2250 (2252), epoch=1 {'loss': array(0.17975585, dtype=float32)}\n",
            "Metrics: step=2260 (2262), epoch=1 {'loss': array(0.26740476, dtype=float32)}\n",
            "Metrics: step=2270 (2272), epoch=1 {'loss': array(0.20597418, dtype=float32)}\n",
            "Metrics: step=2280 (2282), epoch=1 {'loss': array(0.2705172, dtype=float32)}\n",
            "Metrics: step=2290 (2292), epoch=1 {'loss': array(0.15991893, dtype=float32)}\n",
            "Metrics: step=2300 (2302), epoch=1 {'loss': array(0.21407877, dtype=float32)}\n",
            "Metrics: step=2310 (2312), epoch=1 {'loss': array(0.23694909, dtype=float32)}\n",
            "Metrics: step=2320 (2322), epoch=1 {'loss': array(0.37367183, dtype=float32)}\n",
            "Metrics: step=2330 (2332), epoch=1 {'loss': array(0.17665341, dtype=float32)}\n",
            "Metrics: step=2340 (2342), epoch=1 {'loss': array(0.3351894, dtype=float32)}\n",
            "Metrics: step=2350 (2352), epoch=1 {'loss': array(0.16824825, dtype=float32)}\n",
            "Metrics: step=2360 (2362), epoch=1 {'loss': array(0.36392924, dtype=float32)}\n",
            "Metrics: step=2370 (2372), epoch=1 {'loss': array(0.17136632, dtype=float32)}\n",
            "Metrics: step=2380 (2382), epoch=1 {'loss': array(0.20884834, dtype=float32)}\n",
            "Metrics: step=2390 (2392), epoch=1 {'loss': array(0.4699292, dtype=float32)}\n",
            "Metrics: step=2400 (2402), epoch=1 {'loss': array(0.24021164, dtype=float32)}\n",
            "Metrics: step=2410 (2412), epoch=1 {'loss': array(0.24841054, dtype=float32)}\n",
            "Metrics: step=2420 (2422), epoch=1 {'loss': array(0.18924247, dtype=float32)}\n",
            "Metrics: step=2430 (2432), epoch=1 {'loss': array(0.19030498, dtype=float32)}\n",
            "Metrics: step=2440 (2442), epoch=1 {'loss': array(0.10818706, dtype=float32)}\n",
            "Metrics: step=2450 (2452), epoch=1 {'loss': array(0.20755737, dtype=float32)}\n",
            "Metrics: step=2460 (2462), epoch=1 {'loss': array(0.1331386, dtype=float32)}\n",
            "Metrics: step=2470 (2472), epoch=1 {'loss': array(0.18177335, dtype=float32)}\n",
            "Metrics: step=2480 (2482), epoch=1 {'loss': array(0.20850225, dtype=float32)}\n",
            "Metrics: step=2490 (2492), epoch=1 {'loss': array(0.18912588, dtype=float32)}\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# @title Run the train loop\n",
        "\n",
        "num_train_steps = 1_000_000  # 1k for example, actual should be > 1M\n",
        "log_loss_every_steps = 10\n",
        "epoch_count = 1\n",
        "epoch_step = 1\n",
        "\n",
        "timestr = str(int(time.time()))\n",
        "dirname = f\"/home/jonathan/Thesis/open_x_embodiment/training_results/train_{timestr}_lr_{UMI_LEARNING_RATE}_eps_{UMI_EPSILON}\"\n",
        "\n",
        "# make directory for loss log\n",
        "os.makedirs(dirname, exist_ok=True)\n",
        "\n",
        "loss_log_csv = f\"{dirname}/loss_log.csv\"\n",
        "\n",
        "# Write the header to the csv file\n",
        "with open(loss_log_csv, mode='w') as loss_log_file:\n",
        "  loss_log_file.write(\"step,epoch,loss\\n\")\n",
        "\n",
        "\n",
        "# The state should be resharded since we may have loaded pretrained weights\n",
        "# that need to be converted to jax.Arrays.\n",
        "state_repl = reshard(state, shardings=replicate_sharding)\n",
        "# The RNG must be replicated.\n",
        "rng_repl = reshard(rng, shardings=replicate_sharding)\n",
        "\n",
        "# train_iter = get_new_iterator()\n",
        "\n",
        "save_checkpoint_path = dirname\n",
        "\n",
        "currloss = 0.0\n",
        "\n",
        "for step in range(num_train_steps):\n",
        "  is_last_step = step == num_train_steps\n",
        "\n",
        "  rng_repl = jax.random.fold_in(rng_repl, step)\n",
        "\n",
        "\n",
        "  # check if there is next in train_iter\n",
        "  try:\n",
        "    # batch = next(train_iter)\n",
        "    batch = jax.tree_map(lambda x: x, next(train_iter))\n",
        "    batch = jax.tree_map(_form_gda, batch, global_data_shape)\n",
        "    epoch_step += 1\n",
        "  except StopIteration:\n",
        "    # Save the current state. If there are more than 10 checkpoints already saved, delete the oldest\n",
        "\n",
        "    # if epoch_count > 10:\n",
        "    #   shutil.rmtree(f'{save_checkpoint_path}/custom_rt1x_checkpoint_epoch{epoch_count - 10}', ignore_errors=True)\n",
        "\n",
        "    if step >= 50_000:\n",
        "      break\n",
        "\n",
        "    train_iter = get_new_iterator()\n",
        "    epoch_count += 1\n",
        "    epoch_step = 1\n",
        "    batch = next(train_iter)\n",
        "    batch = jax.tree_map(_form_gda, batch, global_data_shape)\n",
        "\n",
        "  if step % 500 == 0:\n",
        "\n",
        "    checkpoint_path = f'{save_checkpoint_path}/custom_rt1x_checkpoint_step_{step}_epoch{epoch_count}_loss{currloss}'\n",
        "    checkpoints.save_checkpoint(ckpt_dir=checkpoint_path, target=state_repl, step=step, overwrite=True)\n",
        "    print(f\"Saved checkpoint after epoch {epoch_count}.\")\n",
        "\n",
        "  state_repl, metrics_update = jitted_train_step(\n",
        "      state=state_repl, batch=batch, rng=rng_repl\n",
        "  )\n",
        "\n",
        "  currloss = jax.device_get(metrics_update)[\"loss\"]\n",
        "\n",
        "  if step % 500 == 0:\n",
        "    # with open(loss_log_csv, mode='a') as loss_log_file:\n",
        "    #     loss_log_file.write(f\"{step},{epoch_count},{currloss}\\n\")\n",
        "    with open(loss_log_csv, mode='a', newline='') as loss_log_file:\n",
        "        writer = csv.writer(loss_log_file)\n",
        "        writer.writerow([step, epoch_count, currloss])\n",
        "\n",
        "  if step % log_loss_every_steps == 0 or is_last_step:\n",
        "    metrics_update = jax.device_get(metrics_update)\n",
        "    print(f\"Metrics: step={step} ({epoch_step}), epoch={epoch_count} {metrics_update}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "provenance": [
        {
          "file_id": "1pTVYsQBR0dBJrfDJ1A3p143ZRrSNasxn",
          "timestamp": 1702073007553
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
