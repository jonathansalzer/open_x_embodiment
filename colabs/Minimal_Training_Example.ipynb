{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX6hBigQc-AA"
      },
      "outputs": [],
      "source": [
        "# !pip install rlds dm-reverb[tensorflow]\n",
        "# !pip install flax jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIwwvkYwpqld"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "from typing import Any, Callable, Dict, Optional, Sequence, Union, NamedTuple, Tuple\n",
        "\n",
        "import copy\n",
        "import enum\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "from jax.experimental import mesh_utils\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import rlds\n",
        "import reverb\n",
        "from rlds import transformations\n",
        "import tensorflow_datasets as tfds\n",
        "import tree\n",
        "\n",
        "import abc\n",
        "import dataclasses\n",
        "import math\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from rlds import rlds_types\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import tensorflow_datasets as tfds\n",
        "import functools\n",
        "from typing import Callable, Sequence\n",
        "import matplotlib.pyplot as plt\n",
        "from flax.training import checkpoints\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj_StgNflaJh"
      },
      "source": [
        "# Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFD03KnSxvV8"
      },
      "outputs": [],
      "source": [
        "# @title Transformation definitions\n",
        "\n",
        "# For an example usage of the code in this code cell, please take a look at the\n",
        "# dataset colab at the link below:\n",
        "# https://colab.research.google.com/github/google-deepmind/open_x_embodiment/blob/main/colabs/Open_X_Embodiment_Datasets.ipynb\n",
        "\n",
        "def _features_to_tensor_spec(\n",
        "    feature: tfds.features.FeatureConnector\n",
        ") -> tf.TensorSpec:\n",
        "  \"\"\"Converts a tfds Feature into a TensorSpec.\"\"\"\n",
        "\n",
        "  def _get_feature_spec(nested_feature: tfds.features.FeatureConnector):\n",
        "    if isinstance(nested_feature, tf.DType):\n",
        "      return tf.TensorSpec(shape=(), dtype=nested_feature)\n",
        "    else:\n",
        "      return nested_feature.get_tensor_spec()\n",
        "\n",
        "  # FeaturesDict can sometimes be a plain dictionary, so we use tf.nest to\n",
        "  # make sure we deal with the nested structure.\n",
        "  return tf.nest.map_structure(_get_feature_spec, feature)\n",
        "\n",
        "\n",
        "def _encoded_feature(feature: Optional[tfds.features.FeatureConnector],\n",
        "                     image_encoding: Optional[str],\n",
        "                     tensor_encoding: Optional[tfds.features.Encoding]):\n",
        "  \"\"\"Adds encoding to Images and/or Tensors.\"\"\"\n",
        "  def _apply_encoding(feature: tfds.features.FeatureConnector,\n",
        "                      image_encoding: Optional[str],\n",
        "                      tensor_encoding: Optional[tfds.features.Encoding]):\n",
        "    if image_encoding and isinstance(feature, tfds.features.Image):\n",
        "      return tfds.features.Image(\n",
        "          shape=feature.shape,\n",
        "          dtype=feature.dtype,\n",
        "          use_colormap=feature.use_colormap,\n",
        "          encoding_format=image_encoding)\n",
        "    if tensor_encoding and isinstance(\n",
        "        feature, tfds.features.Tensor) and feature.dtype != tf.string:\n",
        "      return tfds.features.Tensor(\n",
        "          shape=feature.shape, dtype=feature.dtype, encoding=tensor_encoding)\n",
        "    return feature\n",
        "\n",
        "  if not feature:\n",
        "    return None\n",
        "  return tf.nest.map_structure(\n",
        "      lambda x: _apply_encoding(x, image_encoding, tensor_encoding), feature)\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class RLDSSpec(metaclass=abc.ABCMeta):\n",
        "  \"\"\"Specification of an RLDS Dataset.\n",
        "\n",
        "  It is used to hold a spec that can be converted into a TFDS DatasetInfo or\n",
        "  a `tf.data.Dataset` spec.\n",
        "  \"\"\"\n",
        "  observation_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  action_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  reward_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  discount_info: Optional[tfds.features.FeatureConnector] = None\n",
        "  step_metadata_info: Optional[tfds.features.FeaturesDict] = None\n",
        "  episode_metadata_info: Optional[tfds.features.FeaturesDict] = None\n",
        "\n",
        "  def step_tensor_spec(self) -> Dict[str, tf.TensorSpec]:\n",
        "    \"\"\"Obtains the TensorSpec of an RLDS step.\"\"\"\n",
        "    step = {}\n",
        "    if self.observation_info:\n",
        "      step[rlds_types.OBSERVATION] = _features_to_tensor_spec(\n",
        "          self.observation_info)\n",
        "    if self.action_info:\n",
        "      step[rlds_types.ACTION] = _features_to_tensor_spec(\n",
        "          self.action_info)\n",
        "    if self.discount_info:\n",
        "      step[rlds_types.DISCOUNT] = _features_to_tensor_spec(\n",
        "          self.discount_info)\n",
        "    if self.reward_info:\n",
        "      step[rlds_types.REWARD] = _features_to_tensor_spec(\n",
        "          self.reward_info)\n",
        "    if self.step_metadata_info:\n",
        "      for k, v in self.step_metadata_info.items():\n",
        "        step[k] = _features_to_tensor_spec(v)\n",
        "\n",
        "    step[rlds_types.IS_FIRST] = tf.TensorSpec(shape=(), dtype=bool)\n",
        "    step[rlds_types.IS_LAST] = tf.TensorSpec(shape=(), dtype=bool)\n",
        "    step[rlds_types.IS_TERMINAL] = tf.TensorSpec(shape=(), dtype=bool)\n",
        "    return step\n",
        "\n",
        "  def episode_tensor_spec(self) -> Dict[str, tf.TensorSpec]:\n",
        "    \"\"\"Obtains the TensorSpec of an RLDS step.\"\"\"\n",
        "    episode = {}\n",
        "    episode[rlds_types.STEPS] = tf.data.DatasetSpec(\n",
        "        element_spec=self.step_tensor_spec())\n",
        "    if self.episode_metadata_info:\n",
        "      for k, v in self.episode_metadata_info.items():\n",
        "        episode[k] = _features_to_tensor_spec(v)\n",
        "    return episode\n",
        "\n",
        "  def to_dataset_config(\n",
        "      self,\n",
        "      name: str,\n",
        "      image_encoding: Optional[str] = None,\n",
        "      tensor_encoding: Optional[tfds.features.Encoding] = None,\n",
        "      citation: Optional[str] = None,\n",
        "      homepage: Optional[str] = None,\n",
        "      description: Optional[str] = None,\n",
        "      overall_description: Optional[str] = None,\n",
        "  ) -> tfds.rlds.rlds_base.DatasetConfig:\n",
        "    \"\"\"Obtains the DatasetConfig for TFDS from the Spec.\"\"\"\n",
        "    return tfds.rlds.rlds_base.DatasetConfig(\n",
        "        name=name,\n",
        "        description=description,\n",
        "        overall_description=overall_description,\n",
        "        homepage=homepage,\n",
        "        citation=citation,\n",
        "        observation_info=_encoded_feature(self.observation_info, image_encoding,\n",
        "                                          tensor_encoding),\n",
        "        action_info=_encoded_feature(self.action_info, image_encoding,\n",
        "                                     tensor_encoding),\n",
        "        reward_info=_encoded_feature(self.reward_info, image_encoding,\n",
        "                                     tensor_encoding),\n",
        "        discount_info=_encoded_feature(self.discount_info, image_encoding,\n",
        "                                       tensor_encoding),\n",
        "        step_metadata_info=_encoded_feature(self.step_metadata_info,\n",
        "                                            image_encoding, tensor_encoding),\n",
        "        episode_metadata_info=_encoded_feature(self.episode_metadata_info,\n",
        "                                               image_encoding, tensor_encoding))\n",
        "\n",
        "  def to_features_dict(self):\n",
        "    \"\"\"Returns a TFDS FeaturesDict representing the dataset config.\"\"\"\n",
        "    step_config = {\n",
        "        rlds_types.IS_FIRST: tf.bool,\n",
        "        rlds_types.IS_LAST: tf.bool,\n",
        "        rlds_types.IS_TERMINAL: tf.bool,\n",
        "    }\n",
        "\n",
        "    if self.observation_info:\n",
        "      step_config[rlds_types.OBSERVATION] = self.observation_info\n",
        "    if self.action_info:\n",
        "      step_config[rlds_types.ACTION] = self.action_info\n",
        "    if self.discount_info:\n",
        "      step_config[rlds_types.DISCOUNT] = self.discount_info\n",
        "    if self.reward_info:\n",
        "      step_config[rlds_types.REWARD] = self.reward_info\n",
        "\n",
        "    if self.step_metadata_info:\n",
        "      for k, v in self.step_metadata_info.items():\n",
        "        step_config[k] = v\n",
        "\n",
        "    if self.episode_metadata_info:\n",
        "      return tfds.features.FeaturesDict({\n",
        "          rlds_types.STEPS: tfds.features.Dataset(step_config),\n",
        "          **self.episode_metadata_info,\n",
        "      })\n",
        "    else:\n",
        "      return tfds.features.FeaturesDict({\n",
        "          rlds_types.STEPS: tfds.features.Dataset(step_config),\n",
        "      })\n",
        "\n",
        "RLDS_SPEC = RLDSSpec\n",
        "TENSOR_SPEC = Union[tf.TensorSpec, dict[str, tf.TensorSpec]]\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class TrajectoryTransform(metaclass=abc.ABCMeta):\n",
        "  \"\"\"Specification the TrajectoryTransform applied to a dataset of episodes.\n",
        "\n",
        "  A TrajectoryTransform is a set of rules transforming a dataset\n",
        "  of RLDS episodes to a dataset of trajectories.\n",
        "  This involves three distinct stages:\n",
        "  - An optional `episode_to_steps_map_fn(episode)` is called at the episode\n",
        "    level, and can be used to select or modify steps.\n",
        "    - Augmentation: an `episode_key` could be propagated to `steps` for\n",
        "      debugging.\n",
        "    - Selection: Particular steps can be selected.\n",
        "    - Stripping: Features can be removed from steps. Prefer using `step_map_fn`.\n",
        "  - An optional `step_map_fn` is called at the flattened steps dataset for each\n",
        "    step, and can be used to featurize a step, e.g. add/remove features, or\n",
        "    augument images\n",
        "  - A `pattern` leverages DM patterns to set a rule of slicing an episode to a\n",
        "    dataset of overlapping trajectories.\n",
        "\n",
        "  Importantly, each TrajectoryTransform must define a `expected_tensor_spec`\n",
        "  which specifies a nested TensorSpec of the resulting dataset. This is what\n",
        "  this TrajectoryTransform will produce, and can be used as an interface with\n",
        "  a neural network.\n",
        "  \"\"\"\n",
        "  episode_dataset_spec: RLDS_SPEC\n",
        "  episode_to_steps_fn_dataset_spec: RLDS_SPEC\n",
        "  steps_dataset_spec: Any\n",
        "  pattern: reverb.structured_writer.Pattern\n",
        "  episode_to_steps_map_fn: Any\n",
        "  expected_tensor_spec: TENSOR_SPEC\n",
        "  step_map_fn: Optional[Any] = None\n",
        "\n",
        "  def get_for_cached_trajectory_transform(self):\n",
        "    \"\"\"Creates a copy of this traj transform to use with caching.\n",
        "\n",
        "    The returned TrajectoryTransfrom copy will be initialized with the default\n",
        "    version of the `episode_to_steps_map_fn`, because the effect of that\n",
        "    function has already been materialized in the cached copy of the dataset.\n",
        "    Returns:\n",
        "      trajectory_transform: A copy of the TrajectoryTransform with overridden\n",
        "        `episode_to_steps_map_fn`.\n",
        "    \"\"\"\n",
        "    traj_copy = dataclasses.replace(self)\n",
        "    traj_copy.episode_dataset_spec = traj_copy.episode_to_steps_fn_dataset_spec\n",
        "    traj_copy.episode_to_steps_map_fn = lambda e: e[rlds_types.STEPS]\n",
        "    return traj_copy\n",
        "\n",
        "  def transform_episodic_rlds_dataset(self, episodes_dataset: tf.data.Dataset):\n",
        "    \"\"\"Applies this TrajectoryTransform to the dataset of episodes.\"\"\"\n",
        "\n",
        "    # Convert the dataset of episodes to the dataset of steps.\n",
        "    steps_dataset = episodes_dataset.map(\n",
        "        self.episode_to_steps_map_fn, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).flat_map(lambda x: x)\n",
        "\n",
        "    return self._create_pattern_dataset(steps_dataset)\n",
        "\n",
        "  def transform_steps_rlds_dataset(\n",
        "      self, steps_dataset: tf.data.Dataset\n",
        "  ) -> tf.data.Dataset:\n",
        "    \"\"\"Applies this TrajectoryTransform to the dataset of episode steps.\"\"\"\n",
        "\n",
        "    return self._create_pattern_dataset(steps_dataset)\n",
        "\n",
        "  def create_test_dataset(\n",
        "      self,\n",
        "  ) -> tf.data.Dataset:\n",
        "    \"\"\"Creates a test dataset of trajectories.\n",
        "\n",
        "    It is guaranteed that the structure of this dataset will be the same as\n",
        "    when flowing real data. Hence this is a useful construct for tests or\n",
        "    initialization of JAX models.\n",
        "    Returns:\n",
        "      dataset: A test dataset made of zeros structurally identical to the\n",
        "        target dataset of trajectories.\n",
        "    \"\"\"\n",
        "    zeros = transformations.zeros_from_spec(self.expected_tensor_spec)\n",
        "\n",
        "    return tf.data.Dataset.from_tensors(zeros)\n",
        "\n",
        "  def _create_pattern_dataset(\n",
        "      self, steps_dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
        "    \"\"\"Create PatternDataset from the `steps_dataset`.\"\"\"\n",
        "    config = create_structured_writer_config('temp', self.pattern)\n",
        "\n",
        "    # Further transform each step if the `step_map_fn` is provided.\n",
        "    if self.step_map_fn:\n",
        "      steps_dataset = steps_dataset.map(self.step_map_fn)\n",
        "    pattern_dataset = reverb.PatternDataset(\n",
        "        input_dataset=steps_dataset,\n",
        "        configs=[config],\n",
        "        respect_episode_boundaries=True,\n",
        "        is_end_of_episode=lambda x: x[rlds_types.IS_LAST])\n",
        "    return pattern_dataset\n",
        "\n",
        "\n",
        "class TrajectoryTransformBuilder(object):\n",
        "  \"\"\"Facilitates creation of the `TrajectoryTransform`.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               dataset_spec: RLDS_SPEC,\n",
        "               episode_to_steps_map_fn=lambda e: e[rlds_types.STEPS],\n",
        "               step_map_fn=None,\n",
        "               pattern_fn=None,\n",
        "               expected_tensor_spec=None):\n",
        "    self._rds_dataset_spec = dataset_spec\n",
        "    self._steps_spec = None\n",
        "    self._episode_to_steps_map_fn = episode_to_steps_map_fn\n",
        "    self._step_map_fn = step_map_fn\n",
        "    self._pattern_fn = pattern_fn\n",
        "    self._expected_tensor_spec = expected_tensor_spec\n",
        "\n",
        "  def build(self,\n",
        "            validate_expected_tensor_spec: bool = True) -> TrajectoryTransform:\n",
        "    \"\"\"Creates `TrajectoryTransform` from a `TrajectoryTransformBuilder`.\"\"\"\n",
        "\n",
        "    if validate_expected_tensor_spec and self._expected_tensor_spec is None:\n",
        "      raise ValueError('`expected_tensor_spec` must be set.')\n",
        "\n",
        "    episode_ds = zero_episode_dataset_from_spec(self._rds_dataset_spec)\n",
        "\n",
        "    steps_ds = episode_ds.flat_map(self._episode_to_steps_map_fn)\n",
        "\n",
        "    episode_to_steps_fn_dataset_spec = self._rds_dataset_spec\n",
        "\n",
        "    if self._step_map_fn is not None:\n",
        "      steps_ds = steps_ds.map(self._step_map_fn)\n",
        "\n",
        "    zeros_spec = transformations.zeros_from_spec(steps_ds.element_spec)  # pytype: disable=wrong-arg-types\n",
        "\n",
        "    ref_step = reverb.structured_writer.create_reference_step(zeros_spec)\n",
        "\n",
        "    pattern = self._pattern_fn(ref_step)\n",
        "\n",
        "    steps_ds_spec = steps_ds.element_spec\n",
        "\n",
        "    target_tensor_structure = create_reverb_table_signature(\n",
        "        'temp_table', steps_ds_spec, pattern)\n",
        "\n",
        "    if (validate_expected_tensor_spec and\n",
        "        self._expected_tensor_spec != target_tensor_structure):\n",
        "      raise RuntimeError(\n",
        "          'The tensor spec of the TrajectoryTransform doesn\\'t '\n",
        "          'match the expected spec.\\n'\n",
        "          'Expected:\\n%s\\nActual:\\n%s\\n' %\n",
        "          (str(self._expected_tensor_spec).replace('TensorSpec',\n",
        "                                                   'tf.TensorSpec'),\n",
        "           str(target_tensor_structure).replace('TensorSpec', 'tf.TensorSpec')))\n",
        "\n",
        "    return TrajectoryTransform(\n",
        "        episode_dataset_spec=self._rds_dataset_spec,\n",
        "        episode_to_steps_fn_dataset_spec=episode_to_steps_fn_dataset_spec,\n",
        "        steps_dataset_spec=steps_ds_spec,\n",
        "        pattern=pattern,\n",
        "        episode_to_steps_map_fn=self._episode_to_steps_map_fn,\n",
        "        step_map_fn=self._step_map_fn,\n",
        "        expected_tensor_spec=target_tensor_structure)\n",
        "\n",
        "def zero_episode_dataset_from_spec(rlds_spec: RLDS_SPEC):\n",
        "  \"\"\"Creates a zero valued dataset of episodes for the given RLDS Spec.\"\"\"\n",
        "\n",
        "  def add_steps(episode, step_spec):\n",
        "    episode[rlds_types.STEPS] = transformations.zero_dataset_like(\n",
        "        tf.data.DatasetSpec(step_spec))\n",
        "    if 'fake' in episode:\n",
        "      del episode['fake']\n",
        "    return episode\n",
        "\n",
        "  episode_without_steps_spec = {\n",
        "      k: v\n",
        "      for k, v in rlds_spec.episode_tensor_spec().items()\n",
        "      if k != rlds_types.STEPS\n",
        "  }\n",
        "\n",
        "  if episode_without_steps_spec:\n",
        "    episodes_dataset = transformations.zero_dataset_like(\n",
        "        tf.data.DatasetSpec(episode_without_steps_spec))\n",
        "  else:\n",
        "    episodes_dataset = tf.data.Dataset.from_tensors({'fake': ''})\n",
        "\n",
        "  episodes_dataset_with_steps = episodes_dataset.map(\n",
        "      lambda episode: add_steps(episode, rlds_spec.step_tensor_spec()))\n",
        "  return episodes_dataset_with_steps\n",
        "\n",
        "\n",
        "def create_reverb_table_signature(table_name: str, steps_dataset_spec,\n",
        "                                  pattern: reverb.structured_writer.Pattern) -> reverb.reverb_types.SpecNest:\n",
        "  config = create_structured_writer_config(table_name, pattern)\n",
        "  reverb_table_spec = reverb.structured_writer.infer_signature(\n",
        "      [config], steps_dataset_spec)\n",
        "  return reverb_table_spec\n",
        "\n",
        "\n",
        "def create_structured_writer_config(table_name: str,\n",
        "                                    pattern: reverb.structured_writer.Pattern) -> Any:\n",
        "  config = reverb.structured_writer.create_config(\n",
        "      pattern=pattern, table=table_name, conditions=[])\n",
        "  return config\n",
        "\n",
        "def n_step_pattern_builder(n: int) -> Any:\n",
        "  \"\"\"Creates trajectory of length `n` from all fields of a `ref_step`.\"\"\"\n",
        "\n",
        "  def transform_fn(ref_step):\n",
        "    traj = {}\n",
        "    for key in ref_step:\n",
        "      if isinstance(ref_step[key], dict):\n",
        "        transformed_entry = tree.map_structure(lambda ref_node: ref_node[-n:],\n",
        "                                               ref_step[key])\n",
        "        traj[key] = transformed_entry\n",
        "      else:\n",
        "        traj[key] = ref_step[key][-n:]\n",
        "\n",
        "    return traj\n",
        "\n",
        "  return transform_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3yS0kizwpaLS"
      },
      "outputs": [],
      "source": [
        "# @title Shared map functions\n",
        "\n",
        "StepFnMapType = Callable[[rlds.Step, rlds.Step], None]\n",
        "\n",
        "\n",
        "def resize_to_resolution(\n",
        "    image: Union[tf.Tensor, np.ndarray],\n",
        "    target_width: int = 320,\n",
        "    target_height: int = 256,\n",
        "    to_numpy: bool = True,\n",
        ") -> Union[tf.Tensor, np.ndarray]:\n",
        "  \"\"\"Resizes image and casts to uint8.\"\"\"\n",
        "  image = tf.image.resize_with_pad(\n",
        "      image,\n",
        "      target_width=target_width,\n",
        "      target_height=target_height,\n",
        "  )\n",
        "  image = tf.cast(image, tf.uint8)\n",
        "  if to_numpy:\n",
        "    image = image.numpy()\n",
        "  return image\n",
        "\n",
        "\n",
        "def map_observation(\n",
        "    to_step: rlds.Step,\n",
        "    from_step: rlds.Step,\n",
        "    from_image_feature_names: tuple[str, ...] = ('image',),\n",
        "    to_image_feature_names: tuple[str, ...] = ('image',),\n",
        "    resize: bool = True,\n",
        ") -> None:\n",
        "  \"\"\"Map observation to model observation spec.\"\"\"\n",
        "\n",
        "  to_step[rlds.OBSERVATION]['natural_language_embedding'] = from_step[\n",
        "      rlds.OBSERVATION\n",
        "  ]['natural_language_embedding']\n",
        "\n",
        "  for from_feature_name, to_feature_name in zip(\n",
        "      from_image_feature_names, to_image_feature_names\n",
        "  ):\n",
        "    if resize:\n",
        "      to_step['observation'][to_feature_name] = resize_to_resolution(\n",
        "          from_step['observation'][from_feature_name],\n",
        "          to_numpy=False,\n",
        "          target_width=320,\n",
        "          target_height=256,\n",
        "      )\n",
        "\n",
        "\n",
        "def terminate_bool_to_act(terminate_episode: tf.Tensor) -> tf.Tensor:\n",
        "  return tf.cond(\n",
        "      terminate_episode == tf.constant(1.0),\n",
        "      lambda: tf.constant([1, 0, 0], dtype=tf.int32),\n",
        "      lambda: tf.constant([0, 1, 0], dtype=tf.int32),\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A0IBN-1Oo-ur"
      },
      "outputs": [],
      "source": [
        "# @title RT-1 action map function\n",
        "\n",
        "\n",
        "def rt_1_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  to_step[rlds.ACTION] = from_step[rlds.ACTION]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "idQIobyEuuAx"
      },
      "outputs": [],
      "source": [
        "# @title Bridge action map function\n",
        "\n",
        "def rescale_action_with_bound(\n",
        "    actions: tf.Tensor,\n",
        "    low: float,\n",
        "    high: float,\n",
        "    safety_margin: float = 0,\n",
        "    post_scaling_max: float = 1.0,\n",
        "    post_scaling_min: float = -1.0,\n",
        ") -> tf.Tensor:\n",
        "  \"\"\"Formula taken from https://stats.stackexchange.com/questions/281162/scale-a-number-between-a-range.\"\"\"\n",
        "  resc_actions = (actions - low) / (high - low) * (\n",
        "      post_scaling_max - post_scaling_min\n",
        "  ) + post_scaling_min\n",
        "  return tf.clip_by_value(\n",
        "      resc_actions,\n",
        "      post_scaling_min + safety_margin,\n",
        "      post_scaling_max - safety_margin,\n",
        "  )\n",
        "\n",
        "\n",
        "def _rescale_action(action):\n",
        "  \"\"\"Rescales action.\"\"\"\n",
        "\n",
        "  # Values taken from\n",
        "  # https://github.com/Asap7772/rt1_eval/blob/2fad77e9bf4def2ef82604d445270f83475e9726/kitchen_eval/rt1_wrapper.py#L39\n",
        "  action['world_vector'] = rescale_action_with_bound(\n",
        "      action['world_vector'],\n",
        "      low=-0.05,\n",
        "      high=0.05,\n",
        "      safety_margin=0.01,\n",
        "      post_scaling_max=1.75,\n",
        "      post_scaling_min=-1.75,\n",
        "  )\n",
        "  action['rotation_delta'] = rescale_action_with_bound(\n",
        "      action['rotation_delta'],\n",
        "      low=-0.25,\n",
        "      high=0.25,\n",
        "      safety_margin=0.01,\n",
        "      post_scaling_max=1.4,\n",
        "      post_scaling_min=-1.4,\n",
        "  )\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "def bridge_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps Bridge dataset action to action expected by the model.\"\"\"\n",
        "\n",
        "  to_step['action']['world_vector'] = from_step['action']['world_vector']\n",
        "  to_step['action']['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step['action']['terminate_episode']\n",
        "  )\n",
        "\n",
        "  to_step['action']['rotation_delta'] = from_step['action']['rotation_delta']\n",
        "\n",
        "  open_gripper = from_step['action']['open_gripper']\n",
        "\n",
        "  possible_values = tf.constant([True, False], dtype=tf.bool)\n",
        "  eq = tf.equal(possible_values, open_gripper)\n",
        "\n",
        "  assert_op = tf.Assert(tf.reduce_any(eq), [open_gripper])\n",
        "\n",
        "  with tf.control_dependencies([assert_op]):\n",
        "    to_step['action']['gripper_closedness_action'] = tf.cond(\n",
        "        # for open_gripper in bridge dataset,\n",
        "        # 0 is fully closed and 1 is fully open\n",
        "        open_gripper,\n",
        "        # for Fractal data,\n",
        "        # gripper_closedness_action = -1 means opening the gripper and\n",
        "        # gripper_closedness_action = 1 means closing the gripper.\n",
        "        lambda: tf.constant([-1.0], dtype=tf.float32),\n",
        "        lambda: tf.constant([1.0], dtype=tf.float32),\n",
        "    )\n",
        "\n",
        "  to_step['action'] = _rescale_action(to_step['action'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J6BAjveUpNsR"
      },
      "outputs": [],
      "source": [
        "# @title Task Agnostic Robot Play map function\n",
        "\n",
        "def taco_play_rescale_actions_by_bounds(actions, lows, highs, safety_margin=0.01):\n",
        "  # Actions is SymbolicTensor, shape (N,)\n",
        "  resc_actions = (actions - lows) / (highs - lows) * 2 - 1\n",
        "  return tf.clip_by_value(resc_actions, -1 + safety_margin, 1 - safety_margin)\n",
        "\n",
        "\n",
        "\n",
        "def taco_play_rescale_action(action):\n",
        "  \"\"\"Rescales actions based on measured per dimension ranges.\"\"\"\n",
        "  # Rotation Delta\n",
        "  rd_lows = tf.constant([-3.2, -0.8, -1.8])\n",
        "  rd_highs = tf.constant([3.2, 0.2, 2.5])\n",
        "  action['rotation_delta'] = taco_play_rescale_actions_by_bounds(\n",
        "      action['rotation_delta'], lows=rd_lows, highs=rd_highs\n",
        "  )\n",
        "\n",
        "  # World Vector\n",
        "  wv_lows = tf.constant([0.0, -0.5, 0.0])\n",
        "  wv_highs = tf.constant([0.8, 0.7, 0.6])\n",
        "  action['world_vector'] = taco_play_rescale_actions_by_bounds(\n",
        "      action['world_vector'], lows=wv_lows, highs=wv_highs\n",
        "  )\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "def taco_play_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps Taco Play Panda action to action expected by the model.\"\"\"\n",
        "\n",
        "  # 'actions' is absolute, and not relative action. There is relative action in\n",
        "  # the materialized dataset that can be used for training (not yet supported).\n",
        "  actions = from_step[rlds.ACTION]['actions']\n",
        "  to_step[rlds.ACTION]['world_vector'] = actions[:3]\n",
        "  to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step[rlds.ACTION]['terminate_episode']\n",
        "  )\n",
        "\n",
        "  to_step[rlds.ACTION]['rotation_delta'] = actions[3:6]\n",
        "  to_step[rlds.ACTION]['gripper_closedness_action'] = tf.expand_dims(\n",
        "      actions[6], axis=-1\n",
        "  )\n",
        "\n",
        "  to_step[rlds.ACTION] = _rescale_action(to_step[rlds.ACTION])\n",
        "\n",
        "\n",
        "taco_play_map_observation = functools.partial(\n",
        "    map_observation,\n",
        "    from_image_feature_names=('rgb_static',),\n",
        "    to_image_feature_names=('image',))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Lv5YtakupoN4"
      },
      "outputs": [],
      "source": [
        "# @title Jaco Play map function\n",
        "\n",
        "def _normalize(value, mean, std):\n",
        "  return (value - mean) / std\n",
        "\n",
        "\n",
        "def jaco_play_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  to_step['action']['world_vector'] = _normalize(\n",
        "      from_step['action']['world_vector'],\n",
        "      mean=tf.constant(\n",
        "          [0.00096585, -0.00580069, -0.00395066], dtype=tf.float32\n",
        "      ),\n",
        "      std=tf.constant([0.12234575, 0.09676983, 0.11155209], dtype=tf.float32),\n",
        "  )\n",
        "  to_step['action']['gripper_closedness_action'] = from_step['action'][\n",
        "      'gripper_closedness_action'\n",
        "  ]\n",
        "  to_step['action']['terminate_episode'] = from_step['action'][\n",
        "      'terminate_episode'\n",
        "  ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_6F3XPY-qFbf"
      },
      "outputs": [],
      "source": [
        "# @title Cable Routing map function\n",
        "\n",
        "def berkeley_cable_routing_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "\n",
        "  to_step['action']['world_vector'] = from_step['action']['world_vector']\n",
        "  to_step['action']['rotation_delta'] = from_step['action']['rotation_delta']\n",
        "\n",
        "  to_step['action']['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step['action']['terminate_episode']\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "caiR409BqLz-"
      },
      "outputs": [],
      "source": [
        "# @title RoboTurk map function\n",
        "\n",
        "def roboturk_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  to_step[rlds.ACTION]['world_vector'] = from_step[rlds.ACTION]['world_vector']\n",
        "  to_step[rlds.ACTION]['gripper_closedness_action'] = from_step[rlds.ACTION][\n",
        "      'gripper_closedness_action'\n",
        "  ]\n",
        "  to_step[rlds.ACTION]['rotation_delta'] = from_step[rlds.ACTION]['rotation_delta']\n",
        "  to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step[rlds.ACTION]['terminate_episode']\n",
        "  )\n",
        "\n",
        "\n",
        "roboturk_map_observation = functools.partial(\n",
        "    map_observation,\n",
        "    from_image_feature_names=('front_rgb',),\n",
        "    to_image_feature_names=('image',)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1b4OUQ7Wqf6-"
      },
      "outputs": [],
      "source": [
        "# @title NYU VINN map function\n",
        "\n",
        "def nyu_door_opening_surprising_effectiveness_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps dataset action to action expected by the model.\"\"\"\n",
        "\n",
        "  # The world vector as existed in the dataset on disk ranges from -0.07 to 0.07\n",
        "  # We scale by 20.0 so that the action spans the limit of the world_vector\n",
        "  # action, from -2.0 to 2.0.\n",
        "  to_step['action']['world_vector'] = from_step['action']['world_vector'] * 20.0\n",
        "\n",
        "  # Similarly, the rotation_delta in the dataset on disk ranges from -0.07 to\n",
        "  # 0.07.\n",
        "  # We scale by 15.0 so that the rotation_delta almost spans the limit of\n",
        "  # rotation_delta, from -pi/2 to pi/2.\n",
        "  to_step['action']['rotation_delta'] = (\n",
        "      from_step['action']['rotation_delta'] * 15.0\n",
        "  )\n",
        "\n",
        "  to_step['action']['gripper_closedness_action'] = (\n",
        "      from_step['action']['gripper_closedness_action']\n",
        "  )\n",
        "\n",
        "  to_step['action']['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step['action']['terminate_episode']\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sseg7Qyiqjw2"
      },
      "outputs": [],
      "source": [
        "# @title Austin VIOLA map function\n",
        "\n",
        "def viola_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps dataset action to action expected by the model.\"\"\"\n",
        "\n",
        "  # The world vector as existed in the dataset on disk ranges from -1.0 to 1.0\n",
        "  # We scale by 1.75 so that the action better spans the limit of the\n",
        "  # world_vector action, from -2.0 to 2.0.\n",
        "  to_step[rlds.ACTION]['world_vector'] = from_step[rlds.ACTION]['world_vector'] * 1.75\n",
        "  to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step[rlds.ACTION]['terminate_episode']\n",
        "  )\n",
        "\n",
        "  # Similarly, the rotation_delta in the dataset on disk ranges from -0.4 to 0.4\n",
        "  # We scale by 3.0 so that the rotation_delta almost spans the limit of\n",
        "  # rotation_delta, from -pi/2 to pi/2.\n",
        "  to_step[rlds.ACTION]['rotation_delta'] = (\n",
        "      from_step[rlds.ACTION]['rotation_delta'] * 3.0\n",
        "  )\n",
        "\n",
        "  gripper_closedness_action = from_step[rlds.ACTION]['gripper_closedness_action']\n",
        "\n",
        "  # There can be 0.0 values because of zero padding\n",
        "  possible_values = tf.constant([-1.0, 1.0, 0.0], dtype=tf.float32)\n",
        "  eq = tf.equal(possible_values, gripper_closedness_action)\n",
        "\n",
        "  # Assert that gripper_closedness_action is one of possible_values\n",
        "  assert_op = tf.Assert(tf.reduce_any(eq), [gripper_closedness_action])\n",
        "\n",
        "  with tf.control_dependencies([assert_op]):\n",
        "    gripper_closedness_action = tf.expand_dims(\n",
        "        gripper_closedness_action, axis=-1\n",
        "    )\n",
        "    to_step[rlds.ACTION]['gripper_closedness_action'] = gripper_closedness_action\n",
        "\n",
        "\n",
        "viola_map_observation = functools.partial(\n",
        "    map_observation,\n",
        "    from_image_feature_names = ('agentview_rgb',),\n",
        "    to_image_feature_names = ('image',),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8hv4j5e0qnRm"
      },
      "outputs": [],
      "source": [
        "# @title Berkeley Autolab UR5 map function\n",
        "\n",
        "def berkeley_autolab_ur5_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps Berkeley Autolab UR5 action to action expected by the model.\"\"\"\n",
        "\n",
        "  # The world vector as existed in the dataset on disk ranges from -0.02 to 0.02\n",
        "  # We scale by 100.0 so that the action spans the limit of the world_vector\n",
        "  # action, from -2.0 to 2.0.\n",
        "  to_step[rlds.ACTION]['world_vector'] = (\n",
        "      from_step[rlds.ACTION]['world_vector'] * 100.0\n",
        "  )\n",
        "  to_step[rlds.ACTION]['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step[rlds.ACTION]['terminate_episode']\n",
        "  )\n",
        "\n",
        "  # Similarly, the rotation_delta in the dataset on disk ranges from -0.07 to\n",
        "  # 0.07\n",
        "  # We scale by 15.0 so that the rotation_delta almost spans the limit of\n",
        "  # rotation_delta, from -pi/2 to pi/2.\n",
        "  to_step[rlds.ACTION]['rotation_delta'] = (\n",
        "      from_step[rlds.ACTION]['rotation_delta'] * 15.0\n",
        "  )\n",
        "  to_step[rlds.ACTION]['gripper_closedness_action'] = tf.expand_dims(\n",
        "      from_step[rlds.ACTION]['gripper_closedness_action'], axis=0\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9bljPl3Eqp7y"
      },
      "outputs": [],
      "source": [
        "# @title TOTO\n",
        "\n",
        "def toto_map_action(to_step: rlds.Step, from_step: rlds.Step):\n",
        "  \"\"\"Maps TOTO action to action expected by the model.\"\"\"\n",
        "\n",
        "  # The world vector as existed in the dataset on disk ranges from -0.7 to 0.7\n",
        "  # We scale by 2.0 so that the action better spans the limit of the\n",
        "  # world_vector action, from -2.0 to 2.0.\n",
        "  to_step['action']['world_vector'] = from_step['action']['world_vector'] * 2.0\n",
        "  to_step['action']['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step['action']['terminate_episode']\n",
        "  )\n",
        "\n",
        "  to_step['action']['rotation_delta'] = from_step['action']['rotation_delta']\n",
        "  to_step['action']['gripper_closedness_action'] = tf.expand_dims(\n",
        "      from_step['action']['open_gripper'], axis=0\n",
        "  )\n",
        "  to_step['action']['gripper_closedness_action'] = tf.cast(\n",
        "      to_step['action']['gripper_closedness_action'], tf.float32\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ni56wdvAuv17"
      },
      "outputs": [],
      "source": [
        "# @title Create trajectory datasets\n",
        "\n",
        "def pad_initial_zero_steps(\n",
        "    steps: tf.data.Dataset, num_zero_step: int\n",
        ") -> tf.data.Dataset:\n",
        "  zero_steps = steps.take(1)\n",
        "  zero_steps = zero_steps.map(lambda x: tf.nest.map_structure(tf.zeros_like, x),\n",
        "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  zero_steps = zero_steps.repeat(num_zero_step)\n",
        "  return rlds.transformations.concatenate(zero_steps, steps)\n",
        "\n",
        "\n",
        "def pad_initial_zero_episode(episode: tf.data.Dataset, num_zero_step: int) -> tf.data.Dataset:\n",
        "  episode[rlds.STEPS] = pad_initial_zero_steps(episode[rlds.STEPS], num_zero_step)\n",
        "  return episode\n",
        "\n",
        "\n",
        "def get_trajectory_dataset(builder_dir: str, step_map_fn, trajectory_length: int, split='train[:10]'):\n",
        "  dataset_builder = tfds.builder_from_directory(builder_dir=builder_dir)\n",
        "\n",
        "  dataset_builder_episodic_dataset = dataset_builder.as_dataset(split=split)\n",
        "\n",
        "  # We need pad_initial_zero_episode because reverb.PatternDataset will skip\n",
        "  # constructing trajectories where the first trajectory_length - 1 steps are\n",
        "  # the final step in a trajectory. As such, without padding, the policies will\n",
        "  # not be trained to predict the actions in the first trajectory_length - 1\n",
        "  # steps.\n",
        "  # We are padding with num_zero_step=trajectory_length-1 steps.\n",
        "  dataset_builder_episodic_dataset = dataset_builder_episodic_dataset.map(\n",
        "      functools.partial(pad_initial_zero_episode, num_zero_step=trajectory_length-1), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  rlds_spec = RLDSSpec(\n",
        "      observation_info=dataset_builder.info.features[rlds.STEPS][rlds.OBSERVATION],\n",
        "      action_info=dataset_builder.info.features[rlds.STEPS][rlds.ACTION],\n",
        "  )\n",
        "\n",
        "  trajectory_transform = TrajectoryTransformBuilder(rlds_spec,\n",
        "                                                    step_map_fn=step_map_fn,\n",
        "                                                    pattern_fn=n_step_pattern_builder(trajectory_length)).build(validate_expected_tensor_spec=False)\n",
        "\n",
        "  trajectory_dataset = trajectory_transform.transform_episodic_rlds_dataset(dataset_builder_episodic_dataset)\n",
        "\n",
        "  return trajectory_dataset\n",
        "\n",
        "\n",
        "def step_map_fn(step, map_observation: StepFnMapType, map_action: StepFnMapType):\n",
        "  transformed_step = {}\n",
        "  transformed_step[rlds.IS_FIRST] = step[rlds.IS_FIRST]\n",
        "  transformed_step[rlds.IS_LAST] = step[rlds.IS_LAST]\n",
        "  transformed_step[rlds.IS_TERMINAL] = step[rlds.IS_TERMINAL]\n",
        "\n",
        "  transformed_step[rlds.OBSERVATION] = {}\n",
        "  transformed_step[rlds.ACTION] = {\n",
        "    'gripper_closedness_action': tf.zeros(1, dtype=tf.float32),\n",
        "    'rotation_delta': tf.zeros(3, dtype=tf.float32),\n",
        "    'terminate_episode': tf.zeros(3, dtype=tf.int32),\n",
        "    'world_vector': tf.zeros(3, dtype=tf.float32),\n",
        "    'base_displacement_vertical_rotation': tf.zeros(1, dtype=tf.float32),\n",
        "    'base_displacement_vector': tf.zeros(2, dtype=tf.float32)\n",
        "  }\n",
        "\n",
        "  map_observation(transformed_step, step)\n",
        "  map_action(transformed_step, step)\n",
        "\n",
        "  return transformed_step\n",
        "\n",
        "\n",
        "DATASET_NAME_TO_TRAJECTORY_DATASET_KWARGS = {\n",
        "    # RT-1\n",
        "    'rt_1': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/fractal20220817_data/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=map_observation,\n",
        "                                        map_action=rt_1_map_action)\n",
        "    },\n",
        "    # TODO: (add Qt-Opt)\n",
        "    # Bridge\n",
        "    'bridge': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/bridge/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=map_observation,\n",
        "                                        map_action=bridge_map_action)\n",
        "    },\n",
        "    #  Task Agnostic Robot Play\n",
        "    'taco_play': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/taco_play/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=taco_play_map_observation,\n",
        "                                        map_action=taco_play_map_action)\n",
        "    },\n",
        "    # Jaco Play\n",
        "    'jaco_play': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/jaco_play/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=map_observation,\n",
        "                                        map_action=jaco_play_map_action)\n",
        "    },\n",
        "    # Cable Routing\n",
        "    'berkeley_cable_routing': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/berkeley_cable_routing/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=map_observation,\n",
        "                                        map_action=berkeley_cable_routing_map_action)\n",
        "    },\n",
        "    # Roboturk\n",
        "    'roboturk': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/roboturk/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=roboturk_map_observation,\n",
        "                                        map_action=roboturk_map_action)\n",
        "    },\n",
        "    # NYU VINN\n",
        "    'nyu_door_opening_surprising_effectiveness': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/nyu_door_opening_surprising_effectiveness/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=map_observation,\n",
        "                                        map_action=nyu_door_opening_surprising_effectiveness_map_action)\n",
        "    },\n",
        "    # Austin VIOLA\n",
        "    'viola': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/viola/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=viola_map_observation,\n",
        "                                        map_action=viola_map_action)\n",
        "    },\n",
        "    # Berkeley Autolab UR5\n",
        "    'berkeley_autolab_ur5': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/berkeley_autolab_ur5/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=map_observation,\n",
        "                                        map_action=berkeley_autolab_ur5_map_action)\n",
        "    },\n",
        "    # TODO: (add Language Table)\n",
        "    'toto': {\n",
        "        'builder_dir': 'gs://gresearch/robotics/toto/0.1.0',\n",
        "        'trajectory_length': 15,\n",
        "        'step_map_fn':functools.partial(step_map_fn,\n",
        "                                        map_observation=map_observation,\n",
        "                                        map_action=toto_map_action)\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "DATASET_NAME_TO_TRAJECTORY_DATASET = {k: get_trajectory_dataset(**v) for k, v in DATASET_NAME_TO_TRAJECTORY_DATASET_KWARGS.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nNB3PBAdJr_-"
      },
      "outputs": [],
      "source": [
        "# @title Dataset weights\n",
        "\n",
        "DATASET_NAME_TO_WEIGHTS = {\n",
        "    'rt_1': 150,\n",
        "    # 'rlds.kuka': 20,\n",
        "    'bridge': 50,\n",
        "    'taco_play': 5,\n",
        "    'jaco_play': 20,\n",
        "    'berkeley_cable_routing': 20,\n",
        "    'roboturk': 10,\n",
        "    'nyu_door_opening_surprising_effectiveness': 5,\n",
        "    'viola': 3,\n",
        "    'berkeley_autolab_ur5': 5,\n",
        "    # 'language_table.language_table': 30,\n",
        "    'toto': 5,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TrmGytLNyxeQ"
      },
      "outputs": [],
      "source": [
        "# @title Batch, and sample one training sample\n",
        "\n",
        "BATCH_SIZE = 6\n",
        "\n",
        "# Larger shuffle buffer leads to better performance, but consumes more RAM\n",
        "datasets = []\n",
        "weights = []\n",
        "\n",
        "for name, dataset in DATASET_NAME_TO_TRAJECTORY_DATASET.items():\n",
        "\n",
        "  # print number of episodes in each dataset\n",
        "  # print(f\"Number of episodes in {name} dataset: {len(list(dataset))}\")\n",
        "\n",
        "  datasets.append(dataset.shuffle(10))\n",
        "  weights.append(float(DATASET_NAME_TO_WEIGHTS[name]))\n",
        "\n",
        "dataset = tf.data.Dataset.sample_from_datasets(datasets, weights=weights)\n",
        "\n",
        "# Larger shuffle buffer leads to better performance, but consumes more RAM\n",
        "dataset = dataset.shuffle(1)\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# print(dataset.reduce(0, lambda x,_: x+1).numpy())\n",
        "\n",
        "trajectory_dataset_iter = iter(dataset)\n",
        "\n",
        "# get length of iter\n",
        "# print(sum(1 for _ in trajectory_dataset_iter))\n",
        "\n",
        "sample = next(trajectory_dataset_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 273
        },
        "executionInfo": {
          "elapsed": 138,
          "status": "ok",
          "timestamp": 1702072517841,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "lfNnoFj0y8I7",
        "outputId": "c00a3f07-8e18-45a5-a0b4-199b3f0a5661"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(sample[rlds.OBSERVATION]['image'].numpy()[0][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 26,
          "status": "ok",
          "timestamp": 1702072517976,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "wfoUdbF8JGLl",
        "outputId": "611b719b-2fa7-43e0-fdea-dbe98f637b3d"
      },
      "outputs": [],
      "source": [
        "sample[rlds.OBSERVATION]['image'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "height": 572
        },
        "executionInfo": {
          "elapsed": 2551,
          "status": "ok",
          "timestamp": 1702072520605,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "JfVV1LbzJNE4",
        "outputId": "1be2e44f-20f4-42b1-82b7-48cd52ef89e4"
      },
      "outputs": [],
      "source": [
        "# @title Visualize one batch of training data\n",
        "\n",
        "batch_size = sample[rlds.OBSERVATION]['image'].shape[0]\n",
        "trajectory_length = sample[rlds.OBSERVATION]['image'].shape[1]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=batch_size,\n",
        "                        ncols=trajectory_length,\n",
        "                        figsize=(30, 10))\n",
        "\n",
        "for batch_index in range(batch_size):\n",
        "  for trajectory_index in range(trajectory_length):\n",
        "    print(rlds.OBSERVATION)\n",
        "    axs[batch_index, trajectory_index].imshow(\n",
        "        sample[rlds.OBSERVATION]['image'][batch_index, trajectory_index])\n",
        "    axs[batch_index, trajectory_index].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE-UQNNZlc51"
      },
      "source": [
        "# RT-1 Model Code\n",
        "\n",
        "In this section we:\n",
        "\n",
        "* Add some model dependency code, which contains layers used by the RT-1 model including the FiLM layers and EfficientNet, as well as the main RT-1 flax module.\n",
        "* Initialize random variables for the model and run a forward pass as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P1GKKVdTMZy8"
      },
      "outputs": [],
      "source": [
        "# @title Model dependencies code\n",
        "\n",
        "\n",
        "MEAN_RGB = [0.485, 0.456, 0.406]\n",
        "STDDEV_RGB = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "Initializer = Callable[[jnp.ndarray, Sequence[int], jnp.dtype], jnp.ndarray]\n",
        "\n",
        "\n",
        "conv_kernel_init_fn = nn.initializers.variance_scaling(2.0, 'fan_out', 'normal')\n",
        "\n",
        "dense_kernel_init_fn = nn.initializers.variance_scaling(\n",
        "    1 / 3.0, 'fan_out', 'uniform'\n",
        ")\n",
        "\n",
        "\n",
        "class FilmConditioning(nn.Module):\n",
        "  \"\"\"FiLM conditioning layer.\"\"\"\n",
        "\n",
        "  num_channels: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, conv_filters, context):\n",
        "    \"\"\"Applies FiLM conditioning to the input.\n",
        "\n",
        "    Args:\n",
        "      conv_filters: array of shape (B, H, W, C), usually an output conv feature\n",
        "        map.\n",
        "      context: array of shape (B, context_size).\n",
        "\n",
        "    Returns:\n",
        "      array of shape (B, H, W, C) with the FiLM conditioning applied.\n",
        "    \"\"\"\n",
        "    zero_init = nn.initializers.zeros_init()\n",
        "    project_cond_add = nn.Dense(\n",
        "        self.num_channels, kernel_init=zero_init, bias_init=zero_init\n",
        "    )(context)\n",
        "    project_cond_mul = nn.Dense(\n",
        "        self.num_channels, kernel_init=zero_init, bias_init=zero_init\n",
        "    )(context)\n",
        "\n",
        "    project_cond_add = project_cond_add[:, None, None, :]\n",
        "    project_cond_mul = project_cond_mul[:, None, None, :]\n",
        "\n",
        "    result = (1 + project_cond_mul) * conv_filters + project_cond_add\n",
        "    return result\n",
        "\n",
        "class DepthwiseConv(nn.Module):\n",
        "  \"\"\"Depthwise convolution that matches tensorflow's conventions.\n",
        "\n",
        "  In Tensorflow, the shapes of depthwise kernels don't match the shapes of a\n",
        "  regular convolutional kernel of appropriate feature_group_count.\n",
        "  It is safer to use this class instead of the regular Conv (easier port of\n",
        "  tensorflow checkpoints, fan_out initialization of the previous layer will\n",
        "  match the tensorflow behavior, etc...).\n",
        "\n",
        "  Attributes:\n",
        "    features: Number of convolution filters.\n",
        "    kernel_size: Shape of the convolutional kernel.\n",
        "    strides: A sequence of `n` integers, representing the inter-window strides.\n",
        "    padding: Either the string `'SAME'`, the string `'VALID'`, or a sequence of\n",
        "      `n` `(low, high)` integer pairs that give the padding to apply before and\n",
        "      after each spatial dimension.\n",
        "    input_dilation: `None`, or a sequence of `n` integers, giving the dilation\n",
        "      factor to apply in each spatial dimension of `inputs`. Convolution with\n",
        "      input dilation `d` is equivalent to transposed convolution with stride\n",
        "      `d`.\n",
        "    kernel_dilation: `None`, or a sequence of `n` integers, giving the dilation\n",
        "      factor to apply in each spatial dimension of the convolution kernel.\n",
        "      Convolution with kernel dilation is also known as 'atrous convolution'.\n",
        "    feature_group_count: Unused attribute present in nn.Conv. Declare it to\n",
        "      match the nn.Conv API.\n",
        "    use_bias: Whether to add a bias to the output (default: True).\n",
        "    dtype: The dtype of the computation (default: float32).\n",
        "    precision: Numerical precision of the computation see `jax.lax.Precision`\n",
        "      for details.\n",
        "    kernel_init: Initializer for the convolutional kernel.\n",
        "    bias_init: Initializer for the bias.\n",
        "  \"\"\"\n",
        "\n",
        "  features: int\n",
        "  kernel_size: Tuple[int, int]\n",
        "  strides: Optional[Tuple[int, int]] = None\n",
        "  padding: Union[str, Sequence[int]] = 'SAME'\n",
        "  input_dilation: Optional[Sequence[int]] = None\n",
        "  kernel_dilation: Optional[Sequence[int]] = None\n",
        "  feature_group_count: int = 1\n",
        "  use_bias: bool = True\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "  precision: Any = None\n",
        "  kernel_init: Any = nn.initializers.lecun_normal()\n",
        "  bias_init: Any = nn.initializers.zeros\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Applies a convolution to the inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Input data with dimensions (batch, spatial_dims..., features).\n",
        "\n",
        "    Returns:\n",
        "      The convolved data.\n",
        "    \"\"\"\n",
        "    inputs = jnp.asarray(inputs, self.dtype)\n",
        "    in_features = inputs.shape[-1]\n",
        "    strides = self.strides\n",
        "\n",
        "    if strides is None:\n",
        "      strides = (1,) * (inputs.ndim - 2)\n",
        "\n",
        "    kernel_shape = self.kernel_size + (self.features, 1)\n",
        "    # Naming convention follows tensorflow.\n",
        "    kernel = self.param('depthwise_kernel', self.kernel_init, kernel_shape)\n",
        "    kernel = jnp.asarray(kernel, self.dtype)\n",
        "\n",
        "    # Need to transpose to convert tensorflow-shaped kernel to lax-shaped kernel\n",
        "    kernel = jnp.transpose(kernel, [0, 1, 3, 2])\n",
        "\n",
        "    dimension_numbers = nn.linear._conv_dimension_numbers(inputs.shape)  # pylint:disable=protected-access\n",
        "\n",
        "    y = jax.lax.conv_general_dilated(\n",
        "        inputs,\n",
        "        kernel,\n",
        "        strides,\n",
        "        self.padding,\n",
        "        lhs_dilation=self.input_dilation,\n",
        "        rhs_dilation=self.kernel_dilation,\n",
        "        dimension_numbers=dimension_numbers,\n",
        "        feature_group_count=in_features,\n",
        "        precision=self.precision,\n",
        "    )\n",
        "\n",
        "    if self.use_bias:\n",
        "      bias = self.param('bias', self.bias_init, (self.features,))\n",
        "      bias = jnp.asarray(bias, self.dtype)\n",
        "      y = y + bias\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "# pytype: disable=attribute-error\n",
        "# pylint:disable=unused-argument\n",
        "class BlockConfig(object):\n",
        "  \"\"\"Class that contains configuration parameters for a single block.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_filters: int = 0,\n",
        "      output_filters: int = 0,\n",
        "      kernel_size: int = 3,\n",
        "      num_repeat: int = 1,\n",
        "      expand_ratio: int = 1,\n",
        "      strides: Tuple[int, int] = (1, 1),\n",
        "      se_ratio: Optional[float] = None,\n",
        "      id_skip: bool = True,\n",
        "      fused_conv: bool = False,\n",
        "      conv_type: str = 'depthwise',\n",
        "  ):\n",
        "    for arg in locals().items():\n",
        "      setattr(self, *arg)\n",
        "\n",
        "\n",
        "class ModelConfig(object):\n",
        "  \"\"\"Class that contains configuration parameters for the model.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      width_coefficient: float = 1.0,\n",
        "      depth_coefficient: float = 1.0,\n",
        "      resolution: int = 224,\n",
        "      dropout_rate: float = 0.2,\n",
        "      blocks: Tuple[BlockConfig, ...] = (\n",
        "          # (input_filters, output_filters, kernel_size, num_repeat,\n",
        "          #  expand_ratio, strides, se_ratio)\n",
        "          # pylint: disable=bad-whitespace\n",
        "          BlockConfig(32, 16, 3, 1, 1, (1, 1), 0.25),\n",
        "          BlockConfig(16, 24, 3, 2, 6, (2, 2), 0.25),\n",
        "          BlockConfig(24, 40, 5, 2, 6, (2, 2), 0.25),\n",
        "          BlockConfig(40, 80, 3, 3, 6, (2, 2), 0.25),\n",
        "          BlockConfig(80, 112, 5, 3, 6, (1, 1), 0.25),\n",
        "          BlockConfig(112, 192, 5, 4, 6, (2, 2), 0.25),\n",
        "          BlockConfig(192, 320, 3, 1, 6, (1, 1), 0.25),\n",
        "          # pylint: enable=bad-whitespace\n",
        "      ),\n",
        "      stem_base_filters: int = 32,\n",
        "      top_base_filters: int = 1280,\n",
        "      activation: str = 'swish',\n",
        "      batch_norm: str = 'default',\n",
        "      bn_momentum: float = 0.99,\n",
        "      bn_epsilon: float = 1e-3,\n",
        "      # While the original implementation used a weight decay of 1e-5,\n",
        "      # tf.nn.l2_loss divides it by 2, so we halve this to compensate in Keras\n",
        "      weight_decay: float = 5e-6,\n",
        "      drop_connect_rate: float = 0.2,\n",
        "      depth_divisor: int = 8,\n",
        "      min_depth: Optional[int] = None,\n",
        "      use_se: bool = True,\n",
        "      input_channels: int = 3,\n",
        "      num_classes: int = 1000,\n",
        "      model_name: str = 'efficientnet',\n",
        "      rescale_input: bool = True,\n",
        "      data_format: str = 'channels_last',\n",
        "      final_projection_size: int = 0,\n",
        "      classifier_head: bool = True,\n",
        "      dtype: jnp.dtype = jnp.float32,\n",
        "  ):\n",
        "    \"\"\"Default Config for Efficientnet-B0.\"\"\"\n",
        "    for arg in locals().items():\n",
        "      setattr(self, *arg)\n",
        "\n",
        "\n",
        "# pylint:enable=unused-argument\n",
        "\n",
        "\n",
        "EN_MODEL_CONFIGS = {\n",
        "    # (width, depth, resolution, dropout)\n",
        "    'efficientnet-b3': ModelConfig(1.2, 1.4, 300, 0.3),\n",
        "}\n",
        "\n",
        "\n",
        "def round_filters(filters: int, config: ModelConfig) -> int:\n",
        "  \"\"\"Returns rounded number of filters based on width coefficient.\"\"\"\n",
        "  width_coefficient = config.width_coefficient\n",
        "  min_depth = config.min_depth\n",
        "  divisor = config.depth_divisor\n",
        "\n",
        "  if not width_coefficient:\n",
        "    return filters\n",
        "\n",
        "  filters *= width_coefficient\n",
        "  min_depth = min_depth or divisor\n",
        "  new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
        "  # Make sure that round down does not go down by more than 10%.\n",
        "  if new_filters < 0.9 * filters:\n",
        "    new_filters += divisor\n",
        "  return int(new_filters)\n",
        "\n",
        "\n",
        "def round_repeats(repeats: int, depth_coefficient: float) -> int:\n",
        "  \"\"\"Returns rounded number of repeats based on depth coefficient.\"\"\"\n",
        "  return int(math.ceil(depth_coefficient * repeats))\n",
        "\n",
        "\n",
        "def conv2d(\n",
        "    inputs: jnp.ndarray,\n",
        "    num_filters: int,\n",
        "    config: ModelConfig,\n",
        "    kernel_size: Union[int, Tuple[int, int]] = (1, 1),\n",
        "    strides: Tuple[int, int] = (1, 1),\n",
        "    use_batch_norm: bool = True,\n",
        "    use_bias: bool = False,\n",
        "    activation: Any = None,\n",
        "    depthwise: bool = False,\n",
        "    train: bool = True,\n",
        "    conv_name: Optional[str] = None,\n",
        "    bn_name: Optional[str] = None,\n",
        "    dtype: jnp.dtype = jnp.float32,\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Convolutional layer with possibly batch norm and activation.\n",
        "\n",
        "  Args:\n",
        "    inputs: Input data with dimensions (batch, spatial_dims..., features).\n",
        "    num_filters: Number of convolution filters.\n",
        "    config: Configuration for the model.\n",
        "    kernel_size: Size of the kernel, as a tuple of int.\n",
        "    strides: Strides for the convolution, as a tuple of int.\n",
        "    use_batch_norm: Whether batch norm should be applied to the output.\n",
        "    use_bias: Whether we should add bias to the output of the first convolution.\n",
        "    activation: Name of the activation function to use.\n",
        "    depthwise: If true, will use depthwise convolutions.\n",
        "    train: Whether the model should behave in training or inference mode.\n",
        "    conv_name: Name to give to the convolution layer.\n",
        "    bn_name: Name to give to the batch norm layer.\n",
        "    dtype: dtype for the computation.\n",
        "\n",
        "  Returns:\n",
        "    The output of the convolutional layer.\n",
        "  \"\"\"\n",
        "  conv_fn = DepthwiseConv if depthwise else nn.Conv\n",
        "  kernel_size = (\n",
        "      (kernel_size, kernel_size)\n",
        "      if isinstance(kernel_size, int)\n",
        "      else tuple(kernel_size)\n",
        "  )\n",
        "  conv_name = conv_name if conv_name else 'conv2d'\n",
        "  bn_name = bn_name if bn_name else 'batch_normalization'\n",
        "\n",
        "  x = conv_fn(\n",
        "      num_filters,\n",
        "      kernel_size,\n",
        "      tuple(strides),\n",
        "      padding='SAME',\n",
        "      use_bias=use_bias,\n",
        "      kernel_init=conv_kernel_init_fn,\n",
        "      name=conv_name,\n",
        "      dtype=dtype,\n",
        "  )(inputs)\n",
        "\n",
        "  if use_batch_norm:\n",
        "    x = nn.BatchNorm(\n",
        "        use_running_average=not train,\n",
        "        momentum=config.bn_momentum,\n",
        "        epsilon=config.bn_epsilon,\n",
        "        name=bn_name,\n",
        "        dtype=dtype,\n",
        "    )(x)\n",
        "\n",
        "  if activation is not None:\n",
        "    x = getattr(nn.activation, activation.lower())(x)\n",
        "  return x\n",
        "\n",
        "\n",
        "def stochastic_depth(\n",
        "    inputs: jnp.ndarray,\n",
        "    rng: jnp.ndarray,\n",
        "    survival_probability: float,\n",
        "    deterministic: bool = False,\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Applies stochastic depth.\n",
        "\n",
        "  Args:\n",
        "    inputs: The inputs that should be randomly masked.\n",
        "    rng: A `jax.random.PRNGKey`.\n",
        "    survival_probability: 1 - the probability of masking out a value.\n",
        "    deterministic: If false the inputs are scaled by `1 / (1 - rate)` and\n",
        "      masked, whereas if true, no mask is applied and the inputs are returned as\n",
        "      is.\n",
        "\n",
        "  Returns:\n",
        "    The masked inputs.\n",
        "  \"\"\"\n",
        "  if survival_probability == 1.0 or deterministic:\n",
        "    return inputs\n",
        "\n",
        "  mask_shape = [inputs.shape[0]] + [1 for _ in inputs.shape[1:]]\n",
        "  mask = jax.random.bernoulli(rng, p=survival_probability, shape=mask_shape)\n",
        "  mask = jnp.tile(mask, [1] + list(inputs.shape[1:]))\n",
        "  return jax.lax.select(\n",
        "      mask, inputs / survival_probability, jnp.zeros_like(inputs)\n",
        "  )\n",
        "\n",
        "\n",
        "class SqueezeExcite(nn.Module):\n",
        "  \"\"\"SqueezeExite block (See: https://arxiv.org/abs/1709.01507.)\n",
        "\n",
        "  Attributes:\n",
        "    num_filters: Number of convolution filters.\n",
        "    block: Configuration for this block.\n",
        "    config: Configuration for the model.\n",
        "    train: Whether the model is in training or inference mode.\n",
        "  \"\"\"\n",
        "\n",
        "  num_filters: int\n",
        "  block: BlockConfig\n",
        "  config: ModelConfig\n",
        "  train: bool\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Applies a convolution to the inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Input data with dimensions (batch, spatial_dims..., features).\n",
        "\n",
        "    Returns:\n",
        "      The output of the squeeze excite block.\n",
        "    \"\"\"\n",
        "    block = self.block\n",
        "    config = self.config\n",
        "    train = self.train\n",
        "    dtype = config.dtype\n",
        "    num_reduced_filters = max(1, int(block.input_filters * block.se_ratio))\n",
        "\n",
        "    se = nn.avg_pool(inputs, inputs.shape[1:3])\n",
        "    se = conv2d(\n",
        "        se,\n",
        "        num_reduced_filters,\n",
        "        config,\n",
        "        use_bias=True,\n",
        "        use_batch_norm=False,\n",
        "        activation=config.activation,\n",
        "        conv_name='reduce_conv2d_0',\n",
        "        train=train,\n",
        "        dtype=dtype,\n",
        "    )\n",
        "\n",
        "    se = conv2d(\n",
        "        se,\n",
        "        self.num_filters,\n",
        "        config,\n",
        "        use_bias=True,\n",
        "        use_batch_norm=False,\n",
        "        activation='sigmoid',\n",
        "        conv_name='expand_conv2d_0',\n",
        "        train=train,\n",
        "        dtype=dtype,\n",
        "    )\n",
        "\n",
        "    return inputs * se\n",
        "\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "  \"\"\"Main building component of Efficientnet.\n",
        "\n",
        "  Attributes:\n",
        "    block: BlockConfig, arguments to create a Block.\n",
        "    config: ModelConfig, a set of model parameters.\n",
        "    train: Whether we are training or predicting.\n",
        "  \"\"\"\n",
        "\n",
        "  block: BlockConfig\n",
        "  config: ModelConfig\n",
        "  train: bool = False\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Mobile Inverted Residual Bottleneck.\n",
        "\n",
        "    Args:\n",
        "      inputs: Input to the block.\n",
        "\n",
        "    Returns:\n",
        "      The output of the block.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "    block = self.block\n",
        "    train = self.train\n",
        "    use_se = config.use_se\n",
        "    activation = config.activation\n",
        "    drop_connect_rate = config.drop_connect_rate\n",
        "    use_depthwise = block.conv_type != 'no_depthwise'\n",
        "    dtype = config.dtype\n",
        "\n",
        "    rng = self.make_rng('random')\n",
        "\n",
        "    filters = block.input_filters * block.expand_ratio\n",
        "\n",
        "    x = inputs\n",
        "    bn_index = 0\n",
        "\n",
        "    if block.fused_conv:\n",
        "      # If we use fused mbconv, skip expansion and use regular conv.\n",
        "      x = conv2d(\n",
        "          x,\n",
        "          filters,\n",
        "          config,\n",
        "          kernel_size=block.kernel_size,\n",
        "          strides=block.strides,\n",
        "          activation=activation,\n",
        "          conv_name='fused_conv2d_0',\n",
        "          bn_name='batch_normalization_' + str(bn_index),\n",
        "          train=train,\n",
        "          dtype=dtype,\n",
        "      )\n",
        "      bn_index += 1\n",
        "    else:\n",
        "      if block.expand_ratio != 1:\n",
        "        # Expansion phase\n",
        "        kernel_size = (1, 1) if use_depthwise else (3, 3)\n",
        "        x = conv2d(\n",
        "            x,\n",
        "            filters,\n",
        "            config,\n",
        "            kernel_size=kernel_size,\n",
        "            activation=activation,\n",
        "            conv_name='expand_conv2d_0',\n",
        "            bn_name='batch_normalization_' + str(bn_index),\n",
        "            train=train,\n",
        "            dtype=dtype,\n",
        "        )\n",
        "        bn_index += 1\n",
        "      # Depthwise Convolution\n",
        "      if use_depthwise:\n",
        "        x = conv2d(\n",
        "            x,\n",
        "            num_filters=x.shape[-1],  # Depthwise conv\n",
        "            config=config,\n",
        "            kernel_size=block.kernel_size,\n",
        "            strides=block.strides,\n",
        "            activation=activation,\n",
        "            depthwise=True,\n",
        "            conv_name='depthwise_conv2d',\n",
        "            bn_name='batch_normalization_' + str(bn_index),\n",
        "            train=train,\n",
        "            dtype=dtype,\n",
        "        )\n",
        "        bn_index += 1\n",
        "\n",
        "    # Squeeze and Excitation phase\n",
        "    if use_se:\n",
        "      assert block.se_ratio is not None\n",
        "      assert 0 < block.se_ratio <= 1\n",
        "      x = SqueezeExcite(\n",
        "          num_filters=filters, block=block, config=config, train=train\n",
        "      )(x)\n",
        "\n",
        "    # Output phase\n",
        "    x = conv2d(\n",
        "        x,\n",
        "        block.output_filters,\n",
        "        config,\n",
        "        activation=None,\n",
        "        conv_name='project_conv2d_0',\n",
        "        bn_name='batch_normalization_' + str(bn_index),\n",
        "        train=train,\n",
        "        dtype=dtype,\n",
        "    )\n",
        "\n",
        "    if (\n",
        "        block.id_skip\n",
        "        and all(s == 1 for s in block.strides)\n",
        "        and block.input_filters == block.output_filters\n",
        "    ):\n",
        "      if drop_connect_rate and drop_connect_rate > 0:\n",
        "        survival_probability = 1 - drop_connect_rate\n",
        "        x = stochastic_depth(\n",
        "            x, rng, survival_probability, deterministic=not train\n",
        "        )\n",
        "      x = x + inputs\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Stem(nn.Module):\n",
        "  \"\"\"Initial block of Efficientnet.\n",
        "\n",
        "  Attributes:\n",
        "    config: ModelConfig, a set of model parameters.\n",
        "    train: Whether we are training or predicting.\n",
        "  \"\"\"\n",
        "\n",
        "  config: ModelConfig\n",
        "  train: bool = False\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Returns the output of the stem block.\n",
        "\n",
        "    Args:\n",
        "      inputs: The input to the block.\n",
        "\n",
        "    Returns:\n",
        "      Output of the block\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "    train = self.train\n",
        "    x = conv2d(\n",
        "        inputs,\n",
        "        round_filters(config.stem_base_filters, config),\n",
        "        config,\n",
        "        kernel_size=(3, 3),\n",
        "        strides=(2, 2),\n",
        "        activation=config.activation,\n",
        "        train=train,\n",
        "        dtype=config.dtype,\n",
        "    )\n",
        "    return x\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\"Final block of Efficientnet.\n",
        "\n",
        "  Attributes:\n",
        "    config: A set of model parameters.\n",
        "    train: Whether we are training or predicting.\n",
        "  \"\"\"\n",
        "\n",
        "  config: Any\n",
        "  train: bool = True\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Returns the output of the head block.\n",
        "\n",
        "    Args:\n",
        "      inputs: The input to the block.\n",
        "\n",
        "    Returns:\n",
        "      x: Classifier logits.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "    train = self.train\n",
        "    dtype = config.dtype\n",
        "    # Build top.\n",
        "    x = conv2d(\n",
        "        inputs,\n",
        "        round_filters(config.top_base_filters, config),\n",
        "        config,\n",
        "        activation=config.activation,\n",
        "        train=train,\n",
        "        dtype=dtype,\n",
        "    )\n",
        "    return x\n",
        "# pytype: enable=attribute-error\n",
        "\n",
        "\n",
        "class EfficientNetWithFilm(nn.Module):\n",
        "  \"\"\"EfficientNet with FiLM conditioning.\"\"\"\n",
        "\n",
        "  config: Any\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self, inputs: jnp.ndarray, context_input: jnp.ndarray, *, train: bool\n",
        "  ):\n",
        "    \"\"\"Returns the output of the EfficientNet model.\"\"\"\n",
        "    config = copy.deepcopy(self.config)\n",
        "    config.dtype = self.dtype\n",
        "    depth_coefficient = config.depth_coefficient\n",
        "    blocks = config.blocks\n",
        "    drop_connect_rate = config.drop_connect_rate\n",
        "\n",
        "    inputs = jnp.asarray(inputs, self.dtype)\n",
        "\n",
        "    # Build stem.\n",
        "    x = Stem(config=config, train=train)(inputs)\n",
        "\n",
        "    # Build blocks.\n",
        "    num_blocks_total = sum(\n",
        "        round_repeats(block.num_repeat, depth_coefficient) for block in blocks\n",
        "    )\n",
        "    block_num = 0\n",
        "\n",
        "    for _, block in enumerate(blocks):\n",
        "      assert block.num_repeat > 0\n",
        "      # Update block input and output filters based on depth multiplier.\n",
        "      block.input_filters = round_filters(block.input_filters, config)\n",
        "      block.output_filters = round_filters(block.output_filters, config)\n",
        "      block.num_repeat = round_repeats(block.num_repeat, depth_coefficient)\n",
        "\n",
        "      # The first block needs to take care of stride and filter size increase\n",
        "      drop_rate = drop_connect_rate * float(block_num) / num_blocks_total\n",
        "      config.drop_connect_rate = drop_rate\n",
        "\n",
        "      x = MBConvBlock(block=block, config=config, train=train)(x)\n",
        "\n",
        "      x = FilmConditioning(num_channels=x.shape[-1])(\n",
        "          x, context_input\n",
        "      )\n",
        "\n",
        "      block_num += 1\n",
        "      if block.num_repeat > 1:\n",
        "        block.input_filters = block.output_filters\n",
        "        block.strides = [1, 1]\n",
        "\n",
        "        for _ in range(block.num_repeat - 1):\n",
        "          drop_rate = drop_connect_rate * float(block_num) / num_blocks_total\n",
        "          config.drop_connect_rate = drop_rate\n",
        "          x = MBConvBlock(block=block, config=config, train=train)(x)\n",
        "          x = FilmConditioning(num_channels=x.shape[-1])(\n",
        "              x, context_input\n",
        "          )\n",
        "\n",
        "          block_num += 1\n",
        "\n",
        "    x = Head(self.config, train=train)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "  \"\"\"Identity layer, convenient for giving a name to an array.\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "    return x\n",
        "\n",
        "\n",
        "class MlpBlock(nn.Module):\n",
        "  \"\"\"Transformer MLP / feed-forward block.\"\"\"\n",
        "\n",
        "  mlp_dim: int\n",
        "  out_dim: Optional[int] = None\n",
        "  dropout_rate: float = 0.1\n",
        "  use_bias: bool = True\n",
        "  kernel_init: Initializer = nn.initializers.xavier_uniform()\n",
        "  bias_init: Initializer = nn.initializers.normal(stddev=1e-6)\n",
        "  activation_fn: Callable[[jnp.ndarray], jnp.ndarray] = nn.gelu\n",
        "  precision: Optional[jax.lax.Precision] = None\n",
        "  dtype: jnp.ndarray = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray, *, deterministic: bool):\n",
        "    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n",
        "    actual_out_dim = inputs.shape[-1] if self.out_dim is None else self.out_dim\n",
        "    x = nn.Dense(\n",
        "        self.mlp_dim,\n",
        "        dtype=self.dtype,\n",
        "        use_bias=self.use_bias,\n",
        "        kernel_init=self.kernel_init,\n",
        "        bias_init=self.bias_init,\n",
        "        precision=self.precision)(\n",
        "            inputs)\n",
        "    x = IdentityLayer(name='mlp1')(self.activation_fn(x))\n",
        "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "    output = nn.Dense(\n",
        "        actual_out_dim,\n",
        "        dtype=self.dtype,\n",
        "        use_bias=self.use_bias,\n",
        "        kernel_init=self.kernel_init,\n",
        "        bias_init=self.bias_init,\n",
        "        precision=self.precision)(\n",
        "            x)\n",
        "    output = IdentityLayer(name='mlp2')(output)\n",
        "    output = nn.Dropout(rate=self.dropout_rate)(\n",
        "        output, deterministic=deterministic)\n",
        "    return output\n",
        "\n",
        "\n",
        "class TokenLearnerModuleV11(nn.Module):\n",
        "  \"\"\"TokenLearner module Version 1.1, using slightly different conv. layers.\n",
        "\n",
        "  Instead of using 4 conv. layers with small channels to implement spatial\n",
        "  attention, this version uses a MLP with gelu inbetween. It also uses softmax\n",
        "  instead of sigmoid. We confirmed that this version works better in general.\n",
        "\n",
        "  Attributes:\n",
        "    num_tokens: Number of tokens.\n",
        "    bottleneck_dim: The size of hidden units in the MLP for spatial attention.\n",
        "    dropout_rate: Dropout rate.\n",
        "  \"\"\"\n",
        "  num_tokens: int\n",
        "  bottleneck_dim: int = 64\n",
        "  dropout_rate: float = 0.\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray, deterministic: bool) -> jnp.ndarray:\n",
        "    \"\"\"Applies learnable tokenization to the 2D inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Inputs of shape `[bs, h, w, c]`.\n",
        "      deterministic: Weather we are in the deterministic mode (e.g inference\n",
        "        time) or not.\n",
        "\n",
        "    Returns:\n",
        "      Output of shape `[bs, n_token, c]`.\n",
        "    \"\"\"\n",
        "    if inputs.ndim == 4:\n",
        "      n, h, w, c = inputs.shape\n",
        "      inputs = jnp.reshape(inputs, [n, h*w, c])\n",
        "\n",
        "    feature_shape = inputs.shape\n",
        "\n",
        "    selected = inputs\n",
        "\n",
        "    selected = nn.LayerNorm()(selected)\n",
        "\n",
        "    selected = MlpBlock(\n",
        "        mlp_dim=self.bottleneck_dim,\n",
        "        out_dim=self.num_tokens,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "        activation_fn=nn.gelu,\n",
        "        name='token_masking')(\n",
        "            selected, deterministic=deterministic)\n",
        "\n",
        "    selected = jnp.reshape(\n",
        "        selected,\n",
        "        [feature_shape[0], -1, self.num_tokens])  # Shape: [bs, h*w, n_token].\n",
        "    selected = jnp.transpose(selected, [0, 2, 1])  # Shape: [bs, n_token, h*w].\n",
        "    selected = jax.nn.softmax(selected, axis=-1)\n",
        "\n",
        "    feat = inputs\n",
        "    feat = jnp.reshape(\n",
        "        feat, [feature_shape[0], -1, feature_shape[-1]])  # Shape: [bs, h*w, c].\n",
        "\n",
        "    feat = jnp.einsum('...si,...id->...sd', selected, feat)\n",
        "\n",
        "    return feat\n",
        "\n",
        "\n",
        "class FFNOptions(enum.Enum):\n",
        "  \"\"\"Different choices of FFN block for ablation testing.\"\"\"\n",
        "\n",
        "  LINEAR = 'linear'  # RT-1 Legacy\n",
        "  SWIGLU = 'swiglu'  # Match LLaMa\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  \"\"\"A self-attention transformer block.\n",
        "\n",
        "  See the `_TransformerLayer` in\n",
        "  google-research/robotics_transformer/transformer.py for the original\n",
        "  tensorflow implementation.\n",
        "  \"\"\"\n",
        "  layer_size: int = 128\n",
        "  num_heads: int = 8\n",
        "  feed_forward_hidden_size: int = 512\n",
        "  feed_forward_output_size: int = 512\n",
        "  ffn_option: FFNOptions = FFNOptions.SWIGLU\n",
        "  dropout_rate: float = 0.1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, attn_mask: jnp.ndarray, *, train: bool):\n",
        "    x1 = nn.LayerNorm()(x)\n",
        "\n",
        "    x1 = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=self.num_heads,\n",
        "        qkv_features=(self.layer_size * self.num_heads),\n",
        "        dropout_rate=self.dropout_rate,\n",
        "    )(x1, x1, mask=attn_mask, deterministic=not train)\n",
        "\n",
        "    x = x + x1\n",
        "\n",
        "    y = nn.LayerNorm()(x)\n",
        "\n",
        "    if self.ffn_option == FFNOptions.SWIGLU:\n",
        "      h1 = nn.Dense(self.feed_forward_hidden_size, use_bias=False)(y)\n",
        "      h1 = nn.swish(h1)\n",
        "      gate = nn.Dense(self.feed_forward_hidden_size, use_bias=False)(y)\n",
        "      ff_y = nn.Dense(self.feed_forward_output_size, use_bias=False)(h1 * gate)\n",
        "    elif self.ffn_option == FFNOptions.LINEAR:\n",
        "      ff_y = nn.Dense(self.feed_forward_output_size, use_bias=False)(y)\n",
        "    else:\n",
        "      raise ValueError(f'Unknown FFN option: {self.ffn_option}')\n",
        "\n",
        "    ff_y = nn.Dropout(self.dropout_rate)(ff_y, deterministic=not train)\n",
        "    x = x + ff_y\n",
        "    return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  \"\"\"Transformer architecture with dense positional embedding.\n",
        "\n",
        "  See the `Transformer` in\n",
        "  google-research/robotics_transformer/transformer.py for the original\n",
        "  tensorflow implementation.\n",
        "  \"\"\"\n",
        "\n",
        "  num_layers: int = 8\n",
        "  layer_size: int = 128\n",
        "  num_heads: int = 8\n",
        "  feed_forward_hidden_size: int = 512\n",
        "  feed_forward_output_size: int = 512\n",
        "  ffn_option: FFNOptions = FFNOptions.SWIGLU\n",
        "  dropout_rate: float = 0.1\n",
        "  vocab_size: int = 256\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, attn_mask: jnp.ndarray, *, train: bool):\n",
        "    bs, seqlen, *_ = x.shape\n",
        "\n",
        "    pos = jnp.expand_dims(jnp.arange(0, seqlen, 1), 0)\n",
        "    pos = jnp.tile(pos, [bs, 1])\n",
        "    pos = jax.nn.one_hot(pos, seqlen)\n",
        "\n",
        "    x = nn.Dense(self.feed_forward_output_size)(x)\n",
        "    pos_emb = nn.Dense(self.feed_forward_output_size)(pos)\n",
        "    x += pos_emb\n",
        "\n",
        "    for _ in range(self.num_layers):\n",
        "      x = TransformerBlock(\n",
        "          layer_size=self.layer_size,\n",
        "          num_heads=self.num_heads,\n",
        "          feed_forward_hidden_size=self.feed_forward_hidden_size,\n",
        "          feed_forward_output_size=self.feed_forward_output_size,\n",
        "          dropout_rate=self.dropout_rate,\n",
        "          ffn_option=self.ffn_option,\n",
        "      )(x, attn_mask, train=train)\n",
        "\n",
        "    output_tokens = nn.Dense(self.vocab_size)(x)\n",
        "    return output_tokens\n",
        "\n",
        "\n",
        "class TokenLearnerModuleV11(nn.Module):\n",
        "  \"\"\"TokenLearner module Version 1.1, using slightly different conv. layers.\n",
        "\n",
        "  Instead of using 4 conv. layers with small channels to implement spatial\n",
        "  attention, this version uses a MLP with gelu inbetween. It also uses softmax\n",
        "  instead of sigmoid. We confirmed that this version works better in general.\n",
        "\n",
        "  From google-research/scenic/projects/token_learner/model.py.\n",
        "\n",
        "  Attributes:\n",
        "    num_tokens: Number of tokens.\n",
        "    bottleneck_dim: The size of hidden units in the MLP for spatial attention.\n",
        "    dropout_rate: Dropout rate.\n",
        "  \"\"\"\n",
        "  num_tokens: int\n",
        "  bottleneck_dim: int = 64\n",
        "  dropout_rate: float = 0.\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs: jnp.ndarray, deterministic: bool) -> jnp.ndarray:\n",
        "    \"\"\"Applies learnable tokenization to the 2D inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: Inputs of shape `[bs, h, w, c]`.\n",
        "      deterministic: Weather we are in the deterministic mode (e.g inference\n",
        "        time) or not.\n",
        "\n",
        "    Returns:\n",
        "      Output of shape `[bs, n_token, c]`.\n",
        "    \"\"\"\n",
        "    if inputs.ndim == 4:\n",
        "      n, h, w, c = inputs.shape\n",
        "      inputs = jnp.reshape(inputs, [n, h*w, c])\n",
        "\n",
        "    feature_shape = inputs.shape\n",
        "\n",
        "    selected = inputs\n",
        "\n",
        "    selected = nn.LayerNorm()(selected)\n",
        "\n",
        "    selected = MlpBlock(\n",
        "        mlp_dim=self.bottleneck_dim,\n",
        "        out_dim=self.num_tokens,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "        activation_fn=nn.gelu,\n",
        "        name='token_masking')(\n",
        "            selected, deterministic=deterministic)\n",
        "\n",
        "    selected = jnp.reshape(\n",
        "        selected,\n",
        "        [feature_shape[0], -1, self.num_tokens])  # Shape: [bs, h*w, n_token].\n",
        "    selected = jnp.transpose(selected, [0, 2, 1])  # Shape: [bs, n_token, h*w].\n",
        "    selected = jax.nn.softmax(selected, axis=-1)\n",
        "\n",
        "    feat = inputs\n",
        "    feat = jnp.reshape(\n",
        "        feat, [feature_shape[0], -1, feature_shape[-1]])  # Shape: [bs, h*w, c].\n",
        "\n",
        "    feat = jnp.einsum('...si,...id->...sd', selected, feat)\n",
        "\n",
        "    return feat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5XSY1Thyutjg"
      },
      "outputs": [],
      "source": [
        "# @title Main RT-1 model code\n",
        "\n",
        "class ImageTokenizer(nn.Module):\n",
        "  \"\"\"Tokenizes images with EfficientNet+FiLM.\n",
        "\n",
        "  This is based on the `RT1ImageTokenizer` implementation here:\n",
        "  google-research/robotics_transformer/tokenizers/image_tokenizer.py\n",
        "\n",
        "  The overall flow of the image tokenizer:\n",
        "  * The input image batch dimensions are squashed, and the image is normalized.\n",
        "  * The image is fed through the `EfficientNetWithFilm`.\n",
        "  * A 1x1 convolution is applied to project to `num_features`.\n",
        "  * Another final `FilmConditioning` layer is applied with the context.\n",
        "  * `TokenLearnerModuleV11` is applied to project the tokens to `num_tokens`.\n",
        "  \"\"\"\n",
        "\n",
        "  num_tokens: int = 8\n",
        "  num_features: int = 512\n",
        "\n",
        "  use_token_learner: bool = True\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self, image: jnp.ndarray, context_input: jnp.ndarray, *, train: bool\n",
        "  ):\n",
        "    \"\"\"Tokenizes the image using an EfficientNet.\n",
        "\n",
        "    Args:\n",
        "      image: jnp.Array with batch and seqlen leading dimensions. We assume the\n",
        "        input image is of size 300x300, since the EfficientNet takes in images\n",
        "        of that size.\n",
        "      context_input: jnp.Array with shape (batch * seqlen, size).\n",
        "      train: Training mode.\n",
        "\n",
        "    Returns:\n",
        "      shape (batch, seqlen, num_tokens, num_features) array.\n",
        "    \"\"\"\n",
        "    bs, seqlen, *_ = image.shape\n",
        "\n",
        "    # The efficientnet-b3 model uses 300x300 images.\n",
        "    efficientnet_config = EN_MODEL_CONFIGS['efficientnet-b3']\n",
        "    image = jnp.reshape(image, [bs * seqlen, 300, 300, 3])\n",
        "    image -= jnp.array(MEAN_RGB)\n",
        "    image /= jnp.array(STDDEV_RGB)\n",
        "\n",
        "    # Apply film in EfficientNet.\n",
        "    x = EfficientNetWithFilm(efficientnet_config)(\n",
        "        image, context_input=context_input, train=train\n",
        "    )\n",
        "\n",
        "    # 1x1 conv. This corresponds to the 1x1 conv here:\n",
        "    # google-research/robotics_transformer/film_efficientnet/pretrained_efficientnet_encoder.py\n",
        "    var_init = nn.initializers.variance_scaling(\n",
        "        scale=1.0,\n",
        "        mode='fan_in',\n",
        "        distribution='truncated_normal',\n",
        "    )\n",
        "    x = nn.Conv(\n",
        "        features=self.num_features,\n",
        "        kernel_size=(1, 1),\n",
        "        strides=(1, 1),\n",
        "        padding='SAME',\n",
        "        use_bias=False,\n",
        "        kernel_init=var_init,\n",
        "    )(x)\n",
        "\n",
        "    x = FilmConditioning(num_channels=self.num_features)(\n",
        "        x, context_input\n",
        "    )\n",
        "\n",
        "    if self.use_token_learner:\n",
        "      x = TokenLearnerModuleV11(num_tokens=self.num_tokens)(\n",
        "          x, deterministic=not train\n",
        "      )\n",
        "\n",
        "    x = jnp.reshape(x, [bs, seqlen, self.num_tokens, -1])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def tokenize_action(\n",
        "    actions: Dict[str, jnp.ndarray],\n",
        "    vocab_size: int,\n",
        "    world_vector_range: Tuple[float, float] = (-1.0, 1.0),\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Tokenizes the action for the RT-1 task.\n",
        "\n",
        "  <name>: <shape> <bounds>\n",
        "  terminate_episode: (3,) int32,\n",
        "    mode 0: terminate episode\n",
        "    mode 1: arm + gripper\n",
        "\n",
        "    mode 2: base\n",
        "  world_vector: (3,) [-1.0, 1.0] (RT-1) or [-2.0, 2.0] (RT-1-X)\n",
        "  rotation_delta: (3,) [-np.pi, np.pi]\n",
        "  gripper_closedness_action: (1,) [-1, 1]\n",
        "  base_displacement_vertical_rotation: (1,) [-np.pi, np.pi]\n",
        "  base_displacement_vector: (2,) [-1.0, 1.0]\n",
        "\n",
        "  Args:\n",
        "    actions: The raw action dictionary.\n",
        "    vocab_size: The vocab size of the tokenized actions.\n",
        "    world_vector_range: The bounds to use for the world_vector token.\n",
        "\n",
        "  Returns:\n",
        "    the tokenized action.\n",
        "  \"\"\"\n",
        "  action_tokens = []\n",
        "\n",
        "  # Handle the discrete one first.\n",
        "  terminate_episode = actions['terminate_episode']\n",
        "  terminate_episode = jnp.argmax(terminate_episode, axis=-1)\n",
        "  terminate_episode = jnp.expand_dims(terminate_episode, -1)\n",
        "  terminate_episode = terminate_episode.astype(jnp.int32)\n",
        "  action_tokens.append(terminate_episode)\n",
        "\n",
        "  for act_name, act_min, act_max in [\n",
        "      ('world_vector', world_vector_range[0], world_vector_range[1]),\n",
        "      ('rotation_delta', -np.pi / 2, np.pi / 2),\n",
        "      ('gripper_closedness_action', -1.0, 1.0),\n",
        "      ('base_displacement_vertical_rotation', -np.pi, np.pi),\n",
        "      ('base_displacement_vector', -1.0, 1.0),\n",
        "  ]:\n",
        "    act = actions[act_name]\n",
        "    act = jnp.clip(act, act_min, act_max)\n",
        "    act = (act - act_min) / (act_max - act_min)\n",
        "    act = act * (vocab_size - 1)\n",
        "    act = act.astype(jnp.int32)\n",
        "    action_tokens.append(act)\n",
        "\n",
        "  tokenized = jnp.concatenate(action_tokens, axis=-1)\n",
        "  return tokenized\n",
        "\n",
        "\n",
        "def detokenize_action(\n",
        "    tokenized_actions: jnp.ndarray,\n",
        "    vocab_size: int,\n",
        "    world_vector_range: Tuple[float, float] = (-1.0, 1.0),\n",
        ") -> Dict[str, jnp.ndarray]:\n",
        "  \"\"\"De-tokenizes the action for the RT-1 task.\n",
        "\n",
        "  See `tokenize_action` for information on the action structure.\n",
        "\n",
        "  Args:\n",
        "    tokenized_actions: The tokenized action vector.\n",
        "    vocab_size: The vocab size of the tokenized actions.\n",
        "    world_vector_range: The bounds to use for the world_vector token.\n",
        "\n",
        "  Returns:\n",
        "    the detokenized action dictionary.\n",
        "  \"\"\"\n",
        "  terminate_episode = tokenized_actions[:, 0]\n",
        "  terminate_episode = jax.nn.one_hot(terminate_episode, 3)\n",
        "\n",
        "  raw_actions = dict(\n",
        "      world_vector=tokenized_actions[:, 1:4].astype(jnp.float32),\n",
        "      rotation_delta=tokenized_actions[:, 4:7].astype(jnp.float32),\n",
        "      gripper_closedness_action=tokenized_actions[:, 7:8].astype(jnp.float32),\n",
        "      base_displacement_vertical_rotation=tokenized_actions[:, 8:9].astype(\n",
        "          jnp.float32\n",
        "      ),\n",
        "      base_displacement_vector=tokenized_actions[:, 9:11].astype(jnp.float32),\n",
        "  )\n",
        "\n",
        "  act_dict = {'terminate_episode': terminate_episode.astype(jnp.int32)}\n",
        "  for act_name, act_min, act_max in [\n",
        "      ('world_vector', world_vector_range[0], world_vector_range[1]),\n",
        "      ('rotation_delta', -np.pi / 2, np.pi / 2),\n",
        "      ('gripper_closedness_action', -1.0, 1.0),\n",
        "      ('base_displacement_vertical_rotation', -np.pi, np.pi),\n",
        "      ('base_displacement_vector', -1.0, 1.0),\n",
        "  ]:\n",
        "    act = raw_actions[act_name]\n",
        "    act = act / (vocab_size - 1)\n",
        "    act = act * (act_max - act_min)\n",
        "    act = act + act_min\n",
        "    act_dict[act_name] = act\n",
        "\n",
        "  return act_dict\n",
        "\n",
        "\n",
        "class RT1(nn.Module):\n",
        "  \"\"\"Full RT-1 and RT-1-X architecture.\"\"\"\n",
        "\n",
        "  num_layers: int = 8\n",
        "  layer_size: int = 128\n",
        "  num_heads: int = 8\n",
        "  feed_forward_hidden_size: int = 512\n",
        "  feed_forward_output_size: int = 512\n",
        "  ffn_option: FFNOptions = FFNOptions.SWIGLU\n",
        "  dropout_rate: float = 0.1\n",
        "  vocab_size: int = 256\n",
        "  num_image_tokens: int = 8\n",
        "  num_action_tokens: int = 11\n",
        "  image_num_features: int = 512\n",
        "\n",
        "  world_vector_range: Tuple[float, float] = (-1.0, 1.0)\n",
        "\n",
        "  use_token_learner: bool = True\n",
        "\n",
        "  # By default, mask out previous actions.\n",
        "  include_prev_timesteps_actions: bool = False\n",
        "\n",
        "  sow_intermediates: bool = False\n",
        "\n",
        "  def setup(self):\n",
        "    self.image_tokenizer = ImageTokenizer(\n",
        "        num_tokens=self.num_image_tokens,\n",
        "        num_features=self.image_num_features,\n",
        "        use_token_learner=self.use_token_learner,\n",
        "    )\n",
        "\n",
        "  def tokenize_image(\n",
        "      self, image: jnp.ndarray, context: jnp.ndarray, *, train: bool\n",
        "  ):\n",
        "    bs, seqlen, *_ = image.shape\n",
        "    context = jnp.reshape(context, [bs * seqlen, -1])\n",
        "    return self.image_tokenizer(image, context_input=context, train=train)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self,\n",
        "      obs: Dict[str, jnp.ndarray],\n",
        "      act: Dict[str, jnp.ndarray],\n",
        "      obs_tokens: Optional[jnp.ndarray] = None,\n",
        "      act_tokens: Optional[jnp.ndarray] = None,\n",
        "      *,\n",
        "      train: bool,\n",
        "  ):\n",
        "    bs = obs['image'].shape[0]\n",
        "    seqlen = obs['image'].shape[1]\n",
        "\n",
        "    # Depending on whether `obs_tokens` is passed, we either run the full\n",
        "    # sequence of images through the image tokenizer, or simply use the\n",
        "    # image tokens passed into this function. `obs_tokens` is usually passed\n",
        "    # during an inference call when caching tokens from previous elements of\n",
        "    # the input sequence.\n",
        "    if obs_tokens is None:\n",
        "      # Get image + language fused tokens.\n",
        "      image = obs['image']\n",
        "      lang = obs['natural_language_embedding']\n",
        "      lang = jnp.reshape(lang, [bs * seqlen, -1])\n",
        "      context_image_tokens = self.image_tokenizer(\n",
        "          image=image, context_input=lang, train=train\n",
        "      )\n",
        "    else:\n",
        "      context_image_tokens = obs_tokens\n",
        "\n",
        "    if self.sow_intermediates:\n",
        "      self.sow('intermediates', 'image_tokens', context_image_tokens)\n",
        "\n",
        "    # We either tokenize the action ourselves using `tokenize_action_fn` or\n",
        "    # use the tokens passed into this function. `act_tokens` is usually supplied\n",
        "    # during an inference call when caching tokens from previous actions.\n",
        "    if act_tokens is None:\n",
        "      action_tokens = tokenize_action(\n",
        "          act, self.vocab_size, self.world_vector_range\n",
        "      )  # pylint: disable=too-many-function-args\n",
        "    else:\n",
        "      action_tokens = act_tokens\n",
        "\n",
        "    if self.include_prev_timesteps_actions:\n",
        "      # Always zero out the final action tokens.\n",
        "      previous_action_tokens = action_tokens[:, : (seqlen - 1), :]\n",
        "      zero_action_tokens = jnp.zeros((bs, 1, self.num_action_tokens))\n",
        "      action_tokens = jnp.concatenate(\n",
        "          [previous_action_tokens, zero_action_tokens], axis=-2\n",
        "      )\n",
        "\n",
        "      # Project the actions to the token dimension.\n",
        "      action_tokens = jax.nn.one_hot(action_tokens, num_classes=self.vocab_size)\n",
        "      action_tokens = nn.Dense(self.image_num_features)(action_tokens)\n",
        "    else:\n",
        "      # If we're not including the previous actions, then we can zero out\n",
        "      # the action tokens. We do it here to ensure tokens are consistently\n",
        "      # zero regardless of the input actions passed to the function.\n",
        "      action_tokens = jnp.zeros(\n",
        "          (bs, seqlen, self.num_action_tokens, self.image_num_features)\n",
        "      )\n",
        "\n",
        "    # Assemble the input tokens into a single sequence.\n",
        "    full_tokens = jnp.concatenate(\n",
        "        [context_image_tokens, action_tokens], axis=-2\n",
        "    )\n",
        "\n",
        "    num_action_tokens = action_tokens.shape[-2]\n",
        "    full_tokens = jnp.reshape(\n",
        "        full_tokens,\n",
        "        [bs, seqlen * (self.num_image_tokens + num_action_tokens), -1],\n",
        "    )\n",
        "\n",
        "    attn_mask = self._construct_attn_mask(\n",
        "        seqlen * (self.num_image_tokens + self.num_action_tokens)\n",
        "    )\n",
        "    output_tokens = Transformer(\n",
        "        num_layers=self.num_layers,\n",
        "        layer_size=self.layer_size,\n",
        "        num_heads=self.num_heads,\n",
        "        feed_forward_hidden_size=self.feed_forward_hidden_size,\n",
        "        feed_forward_output_size=self.feed_forward_output_size,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "        vocab_size=self.vocab_size,\n",
        "        ffn_option=self.ffn_option,\n",
        "    )(full_tokens, attn_mask=attn_mask, train=train)\n",
        "\n",
        "    return output_tokens\n",
        "\n",
        "  def _get_action_index_for_token(self, k: int, num_tokens: int):\n",
        "    \"\"\"Returns action associated with the token at given position `k`.\n",
        "\n",
        "    If k is not an action token then it returns -1.\n",
        "    If k is part of the first action in the sequence then returns 0 etc.\n",
        "\n",
        "    Based on `_get_action_index_for_token` here:\n",
        "    google-research/robotics_transformer/transformer_network.py\n",
        "\n",
        "    Args:\n",
        "      k: an int that represents the position in the sequence.\n",
        "      num_tokens: The total number of tokens in the sequence.\n",
        "    Returns:\n",
        "      The index of the action that this position belongs to, or if this\n",
        "      position is part of an image token then returns -1.\n",
        "    \"\"\"\n",
        "    if k < 0 or k >= num_tokens:\n",
        "      return -1\n",
        "\n",
        "    single_time_step_num_tokens = self.num_image_tokens + self.num_action_tokens\n",
        "    n = k\n",
        "    if n % single_time_step_num_tokens < self.num_image_tokens:\n",
        "      return -1\n",
        "\n",
        "    return int(n / single_time_step_num_tokens)\n",
        "\n",
        "  def _construct_attn_mask(self, num_tokens: ...):\n",
        "    \"\"\"Generate mask for action prediction loss.\n",
        "\n",
        "    This masks out all action tokens.\n",
        "\n",
        "    Based on `_generate_masks` here:\n",
        "    google-research/robotics_transformer/transformer_network.py\n",
        "\n",
        "    Args:\n",
        "      num_tokens: The number of tokens with which to construct the input mask.\n",
        "\n",
        "    Returns:\n",
        "      A (num_tokens, num_tokens) attention mask.\n",
        "    \"\"\"\n",
        "    default_attn_mask = np.tril(np.ones((num_tokens, num_tokens), np.int32))\n",
        "    action_mask = np.zeros(shape=(num_tokens, num_tokens), dtype=np.int32)\n",
        "\n",
        "    for i in range(num_tokens):\n",
        "      for j in range(num_tokens):\n",
        "        action_i = self._get_action_index_for_token(i, num_tokens)\n",
        "        action_j = self._get_action_index_for_token(j, num_tokens)\n",
        "        mask = 0\n",
        "        if action_i != -1 and action_j != -1:\n",
        "          # Ignore actions of previous steps.\n",
        "          if action_j < action_i:\n",
        "            mask = 1\n",
        "          # If we're not auto-regression, ignore action dimensions of current\n",
        "          # step.\n",
        "          if action_j == action_i and j <= i:\n",
        "            mask = 1\n",
        "        # i not is an action, but j is an action token.\n",
        "        # Hence, also mask j when predicting i, to prevent accidental\n",
        "        # dependency between output and masked dimensions because the output\n",
        "        # can still depend on the masked dimensions when predictions of the\n",
        "        # transformer layers after the first layer depends on the masked\n",
        "        # dimensions.\n",
        "        elif action_j != -1:\n",
        "          if not self.include_prev_timesteps_actions and j < i:\n",
        "            mask = 1\n",
        "        action_mask[i, j] = mask\n",
        "    return default_attn_mask - action_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 95728,
          "status": "ok",
          "timestamp": 1702072619026,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "AsJBedCbv_cl",
        "outputId": "ad37c985-10ea-4f60-c6cb-81b996690119"
      },
      "outputs": [],
      "source": [
        "SEQUENCE_LENGTH = 15\n",
        "NUM_ACTION_TOKENS = 11\n",
        "LAYER_SIZE = 256\n",
        "VOCAB_SIZE = 512\n",
        "NUM_IMAGE_TOKENS = 81\n",
        "\n",
        "rt1x_model = RT1(\n",
        "    num_image_tokens=NUM_IMAGE_TOKENS,\n",
        "    num_action_tokens=NUM_ACTION_TOKENS,\n",
        "    layer_size=LAYER_SIZE,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    # Use token learner to reduce tokens per image to 81.\n",
        "    use_token_learner=True,\n",
        "    # RT-1-X uses (-2.0, 2.0) instead of (-1.0, 1.0).\n",
        "    world_vector_range=(-2.0, 2.0)\n",
        ")\n",
        "\n",
        "# Initialize random weights for the model and run a forward pass.\n",
        "obs = {\n",
        "    \"image\": jnp.ones((1, 15, 300, 300, 3)),\n",
        "    \"natural_language_embedding\": jnp.ones((1, 15, 512)),\n",
        "}\n",
        "act = {\n",
        "    \"world_vector\": jnp.ones((1, 15, 3)),\n",
        "    \"rotation_delta\": jnp.ones((1, 15, 3)),\n",
        "    \"gripper_closedness_action\": jnp.ones((1, 15, 1)),\n",
        "    \"base_displacement_vertical_rotation\": jnp.ones((1, 15, 1)),\n",
        "    \"base_displacement_vector\": jnp.ones((1, 15, 2)),\n",
        "    \"terminate_episode\": jnp.ones((1, 15, 3), dtype=jnp.int32),\n",
        "}\n",
        "\n",
        "variables = rt1x_model.init(\n",
        "    {\n",
        "        \"params\": jax.random.PRNGKey(0),\n",
        "        \"random\": jax.random.PRNGKey(0),\n",
        "    },\n",
        "    obs,\n",
        "    act,\n",
        "    train=False,\n",
        ")\n",
        "model_output = rt1x_model.apply(\n",
        "    variables,\n",
        "    obs,\n",
        "    act,\n",
        "    train=False,\n",
        "    rngs={\"random\": jax.random.PRNGKey(0)},\n",
        ")\n",
        "\n",
        "# Inspect the model weights and output.\n",
        "\n",
        "param_count = sum(x.size for x in jax.tree_util.tree_leaves(variables[\"params\"]))\n",
        "print(f\"Number of parameters: {param_count}\")\n",
        "\n",
        "print(f\"Output shape: {model_output.shape}.\")\n",
        "\n",
        "# Extract the actions from the model.\n",
        "time_step_tokens = (\n",
        "    NUM_IMAGE_TOKENS + NUM_ACTION_TOKENS\n",
        ")\n",
        "output_logits = jnp.reshape(\n",
        "    model_output, (1, SEQUENCE_LENGTH, time_step_tokens, -1)\n",
        ")\n",
        "action_logits = output_logits[:, -1, ...]\n",
        "action_logits = action_logits[:, NUM_IMAGE_TOKENS - 1 : -1]\n",
        "\n",
        "action_logp = jax.nn.softmax(action_logits)\n",
        "action_token = jnp.argmax(action_logp, axis=-1)\n",
        "\n",
        "action_detokenized = detokenize_action(action_token, VOCAB_SIZE, world_vector_range=(-2.0, 2.0))\n",
        "print(f\"Detokenized actions: {action_detokenized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkRGdUx5lhg8"
      },
      "source": [
        "# Training Setup and Loop\n",
        "\n",
        "* First, add some additional preprocessors in the data pipeline. This crops and resizes images and filters out terminal steps from the episode.\n",
        "* We set up the parallelism. We use a data parallel mesh and shard the batch across all of the available devices.\n",
        "* Create the `create_train_state` and `train` functions. `create_train_state` initializes the train state, while `train` is the main step function called on each train iteration. `rt1_loss` implements the cross-entropy loss for the RT-1 model.\n",
        "* We `jit` the functions above, and initialize the train state and place the input arrays on the available devices.\n",
        "* Run the train loop which iterates through the batches and calls the train step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nofx5oWB0DqY"
      },
      "outputs": [],
      "source": [
        "# @title Additional data preprocessing\n",
        "\n",
        "def _is_not_terminal(trajectory):\n",
        "  # -1 selects the final step in a trajectory\n",
        "  if trajectory[rlds.IS_TERMINAL][-1]:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def convert_dtype_and_crop_images(\n",
        "    images,\n",
        "    resize_size,\n",
        "    training: bool = True,\n",
        "    convert_dtype: bool = True,\n",
        "    seed: Optional[tf.Tensor] = None,\n",
        "):\n",
        "  \"\"\"Convert uint8 images to float32 and square crop.\n",
        "\n",
        "  Args:\n",
        "    images: [B, H, W, 3] uint8 tensor of images.\n",
        "    resize_size: (H, W) of resize.\n",
        "    training: If we are in training (random crop) or not-training (fixed crop).\n",
        "    convert_dtype: whether or not to convert the image to float32 in the range\n",
        "      of (0, 1).\n",
        "    seed: Optional seed of shape (2,) for giving to tf.random.stateless_uniform\n",
        "\n",
        "  Returns:\n",
        "    [B, crop_size, crop_size, 3] images of dtype float32.\n",
        "  \"\"\"\n",
        "\n",
        "  if seed is None:\n",
        "    seed = tf.random.uniform(shape=(2,), maxval=2**30, dtype=tf.int32)\n",
        "\n",
        "  seed2 = tf.random.experimental.stateless_split(seed, num=1)[0]\n",
        "\n",
        "  if convert_dtype:\n",
        "    images = tf.image.convert_image_dtype(images, tf.float32)\n",
        "  image_height = images.get_shape().as_list()[-3]\n",
        "  image_width = images.get_shape().as_list()[-2]\n",
        "\n",
        "  if training:\n",
        "    if image_height == 512:\n",
        "      ud_pad = 40\n",
        "      lr_pad = 100\n",
        "    elif image_height == 256:\n",
        "      ud_pad = 20\n",
        "      lr_pad = 50\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          'convert_dtype_and_crop_images only supports image height 512 or 256.'\n",
        "      )\n",
        "    max_y = 2 * ud_pad\n",
        "    max_x = 2 * lr_pad\n",
        "    images = tf.image.pad_to_bounding_box(\n",
        "        images,\n",
        "        offset_height=ud_pad,\n",
        "        offset_width=lr_pad,\n",
        "        target_height=image_height + 2 * ud_pad,\n",
        "        target_width=image_width + 2 * lr_pad,\n",
        "    )\n",
        "    offset_y = tf.random.stateless_uniform(\n",
        "        (), maxval=max_y + 1, dtype=tf.int32, seed=seed\n",
        "    )\n",
        "    offset_x = tf.random.stateless_uniform(\n",
        "        (), maxval=max_x + 1, dtype=tf.int32, seed=seed2\n",
        "    )\n",
        "    images = tf.image.crop_to_bounding_box(\n",
        "        images, offset_y, offset_x, image_height, image_width\n",
        "    )\n",
        "\n",
        "  # Add resize in pipeline for jax.\n",
        "  images = tf.image.resize(images, size=resize_size)\n",
        "  return images\n",
        "\n",
        "\n",
        "def prepare_for_model_input(\n",
        "    ds, target_height, target_width, training\n",
        "):\n",
        "  \"\"\"Removes terminal trajectory, string from features and crops image.\"\"\"\n",
        "  ds = ds.filter(_is_not_terminal)\n",
        "\n",
        "  # Remove non-jax types.\n",
        "  def _remove_str(steps):\n",
        "    if 'natural_language_instruction' in steps['observation']:\n",
        "      del steps['observation']['natural_language_instruction']\n",
        "    return steps\n",
        "\n",
        "  ds = ds.map(_remove_str, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  # Cropping augmentation.\n",
        "  def _add_crop_augmentation(step):\n",
        "    # Crop and pad augmentation. Added for jax.\n",
        "    image = step['observation']['image']\n",
        "    step['observation']['image'] = (\n",
        "        convert_dtype_and_crop_images(\n",
        "            image, (target_height, target_width), training=training\n",
        "        )\n",
        "    )\n",
        "    return step\n",
        "\n",
        "  ds = ds.map(_add_crop_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "executionInfo": {
          "elapsed": 9679,
          "status": "ok",
          "timestamp": 1702072629279,
          "user": {
            "displayName": "Ayzaan Wahid",
            "userId": "15944556989858923963"
          },
          "user_tz": 480
        },
        "id": "D_fZErEk0l9Z",
        "outputId": "c70419c8-2d5f-4554-aba9-2d568ee443d5"
      },
      "outputs": [],
      "source": [
        "# @title Set up sharding and data parallel mesh\n",
        "\n",
        "# Actual global batch size is 1024. Use a smaller batch size for this colab\n",
        "# example.\n",
        "# TODO: Set this to actual batch size.\n",
        "PER_DEVICE_BATCH_SIZE = 2\n",
        "\n",
        "def reshard(tree, shardings):\n",
        "  \"\"\"Take an arbitrarily sharded pytree and shard it according to `shardings`.\n",
        "\n",
        "  From `big_vision.utils.reshard`. See that doc for full details.\n",
        "\n",
        "  Args:\n",
        "    tree: a pytree of arrays.\n",
        "    shardings: a (prefix) pytree of jax array shardings.\n",
        "\n",
        "  Returns:\n",
        "    A pytree of global jax arrays that follows provided shardings.\n",
        "  \"\"\"\n",
        "\n",
        "  def _make_global_arr(x, shard, shape):\n",
        "    # Avoid unnecessary copies and transfers:\n",
        "    if hasattr(x, \"sharding\") and x.sharding.is_equivalent_to(\n",
        "        shard, len(shape)\n",
        "    ):  # pylint: disable=line-too-long\n",
        "      return x\n",
        "    if not getattr(x, \"is_fully_addressable\", True):\n",
        "      raise RuntimeError(\n",
        "          \"Trying to reshard a non-fully-addressable array. \"\n",
        "          \"Please see the doc-comment for detailed explanation.\"\n",
        "      )\n",
        "    x = jax.device_get(x)  # Might be on local devices.\n",
        "    xs = [\n",
        "        jax.device_put(x[s], device=d)\n",
        "        for d, s in shard.addressable_devices_indices_map(shape).items()\n",
        "    ]\n",
        "    return jax.make_array_from_single_device_arrays(shape, shard, xs)\n",
        "\n",
        "  shapes = jax.tree_map(np.shape, tree)\n",
        "  shardings = tree_broadcast(shardings, tree)\n",
        "  return jax.tree_map(_make_global_arr, tree, shardings, shapes)\n",
        "\n",
        "def tree_broadcast(prefix, target):\n",
        "  \"\"\"Broadcasts a prefix tree to a full tree.\n",
        "\n",
        "  See big_vision.utils.tree_broadcast.\n",
        "\n",
        "  Args:\n",
        "    prefix: prefix pytree.\n",
        "    target: boradcast target for a prefix tree.\n",
        "\n",
        "  Returns:\n",
        "    prefix tree broadcasted to a target tree.\n",
        "  \"\"\"\n",
        "\n",
        "  def _broadcast(leaf, subtree):\n",
        "    return jax.tree_map(lambda _: leaf, subtree)\n",
        "\n",
        "  return jax.tree_map(_broadcast, prefix, target)\n",
        "\n",
        "\n",
        "NamedSharding = jax.sharding.NamedSharding\n",
        "P = jax.sharding.PartitionSpec\n",
        "\n",
        "# TODO: Replace this with an UMI dataset.\n",
        "train_dataset = tf.data.Dataset.sample_from_datasets(datasets, weights=weights)\n",
        "\n",
        "train_dataset = prepare_for_model_input(\n",
        "    train_dataset, target_height=300, target_width=300, training=True\n",
        ")\n",
        "\n",
        "# Creating mesh and shardings.\n",
        "num_devices = len(jax.devices())\n",
        "mesh = jax.sharding.Mesh(\n",
        "    mesh_utils.create_device_mesh((num_devices,)), (\"data\",)\n",
        ")\n",
        "\n",
        "# Data parallel mesh.\n",
        "sharding = jax.sharding.NamedSharding(mesh, P(\"data\"))\n",
        "replicate_sharding = NamedSharding(mesh, P())\n",
        "\n",
        "global_batch_size = jax.device_count() * PER_DEVICE_BATCH_SIZE\n",
        "local_batch_size = jax.local_device_count() * PER_DEVICE_BATCH_SIZE\n",
        "train_dataset = train_dataset.batch(local_batch_size, drop_remainder=True)\n",
        "\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_iter = train_dataset.as_numpy_iterator()\n",
        "sample_batch = jax.tree_map(lambda x: x, next(train_iter))\n",
        "\n",
        "# shuffle and get new iterator for next epoch\n",
        "def get_new_iterator():\n",
        "  print(\"Shuffling dataset and getting new iterator.\")\n",
        "  shuffled_train_dataset = train_dataset.shuffle(10)\n",
        "  shuffled_train_iter = shuffled_train_dataset.as_numpy_iterator()\n",
        "  return shuffled_train_iter\n",
        "\n",
        "print(f\"Local batch size: {local_batch_size}\")\n",
        "print(f\"Global batch size: {global_batch_size}\")\n",
        "print(f\"Devices: {jax.devices()}\")\n",
        "print(f\"Sample batch keys: {sample_batch.keys()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBJwdoo22Oed"
      },
      "outputs": [],
      "source": [
        "# @title Create the train init fn, train step fn, and loss function.\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class TrainState:\n",
        "  step: int\n",
        "  params: Any\n",
        "  opt_state: optax.OptState\n",
        "  batch_stats: Any\n",
        "\n",
        "\n",
        "def create_train_state(model, batch, rng, optimizer):\n",
        "    \"\"\"Creates the train state and initial metrics for agent.\"\"\"\n",
        "    obs_input = batch[\"observation\"]\n",
        "    act_input = batch[\"action\"]\n",
        "\n",
        "    rng, rng2, rng3 = jax.random.split(rng, 3)\n",
        "    variables = model.init(\n",
        "        {\"params\": rng, \"random\": rng3},\n",
        "        obs=obs_input,\n",
        "        act=act_input,\n",
        "        train=False,\n",
        "    )\n",
        "\n",
        "    params = flax.core.unfreeze(variables[\"params\"])\n",
        "    batch_stats = flax.core.unfreeze(variables[\"batch_stats\"])\n",
        "\n",
        "    train_state = TrainState(\n",
        "        step=0,\n",
        "        params=flax.core.unfreeze(params),\n",
        "        opt_state=optimizer.init(params),\n",
        "        batch_stats=batch_stats,\n",
        "    )\n",
        "    return train_state\n",
        "\n",
        "\n",
        "def train(batch, state, model, optimizer, rng):\n",
        "  \"\"\"Performs a single training step.\"\"\"\n",
        "  rng, loss_rng = jax.random.split(rng)\n",
        "\n",
        "  def loss_fn(params):\n",
        "    variables = {\"params\": params, \"batch_stats\": state.batch_stats}\n",
        "    per_example_loss, new_variables = rt1_loss(\n",
        "        model, batch=batch, variables=variables, rng=loss_rng\n",
        "    )\n",
        "    loss = jnp.mean(per_example_loss)\n",
        "    return loss, new_variables[\"batch_stats\"]\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, new_batch_stats), grad = grad_fn(state.params)\n",
        "\n",
        "  loss = jnp.mean(loss)\n",
        "\n",
        "  updates, new_opt_state = optimizer.update(\n",
        "      grad, state.opt_state, state.params\n",
        "  )\n",
        "\n",
        "  new_params = optax.apply_updates(state.params, updates)\n",
        "  new_state = state.replace(\n",
        "      step=state.step + 1,\n",
        "      params=flax.core.unfreeze(new_params),\n",
        "      opt_state=flax.core.unfreeze(new_opt_state),\n",
        "      batch_stats=flax.core.unfreeze(new_batch_stats),\n",
        "  )\n",
        "\n",
        "  metrics_update = {\n",
        "      \"loss\": loss,\n",
        "  }\n",
        "  return new_state, metrics_update\n",
        "\n",
        "\n",
        "def rt1_loss(\n",
        "      model,\n",
        "      batch,\n",
        "      variables,\n",
        "      rng,\n",
        "  ):\n",
        "  \"\"\"Implements the RT-1 loss.\"\"\"\n",
        "  observation = batch[\"observation\"]\n",
        "  action = batch[\"action\"]\n",
        "\n",
        "  bs = observation[\"image\"].shape[0]\n",
        "  seqlen = observation[\"image\"].shape[1]\n",
        "\n",
        "  # First, we encode the observations using the model.encode method.\n",
        "  # This will give us an observation encoding (for the entire sequence).\n",
        "  rng, params_rng = jax.random.split(rng)\n",
        "  rng, dropout_rng = jax.random.split(rng)\n",
        "  rng, sd_rng = jax.random.split(rng)\n",
        "  rng, random_rng = jax.random.split(rng)\n",
        "  logits, new_variables = model.apply(\n",
        "      variables,\n",
        "      obs=observation,\n",
        "      act=action,\n",
        "      train=True,\n",
        "      mutable=[\"batch_stats\"],\n",
        "      rngs={\n",
        "          \"params\": params_rng,\n",
        "          \"dropout\": dropout_rng,\n",
        "          \"random\": random_rng,\n",
        "      },\n",
        "  )\n",
        "\n",
        "  vocab_size = model.vocab_size\n",
        "\n",
        "  # `action` is dict of (B, T, ...), we combine actions into B*T batch to\n",
        "  # tokenize.\n",
        "  action = jax.tree_map(lambda x: jnp.reshape(x, (bs * seqlen, -1)), action)\n",
        "  labels = tokenize_action(action, vocab_size=vocab_size)\n",
        "  labels = jax.tree_map(lambda x: jnp.reshape(x, (bs, seqlen, -1)), labels)\n",
        "  labels = labels[:, :, :, None]  # labels should be (B, seqlen, 11, 1)\n",
        "\n",
        "  # Get num_action_tokens tokens for the action prediction. By default,\n",
        "  # RT-1 computes the loss for all `seqlen * num_action_tokens`, not just\n",
        "  # the final timestep's action.\n",
        "  # In the default RT-1 setup (8 img, 11 act tokens), we have:\n",
        "  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
        "  # |-----image tokens------|-------------action tokens--------------|\n",
        "  #                      |----------------logits------------------|\n",
        "  # For each time step, we want [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17] for\n",
        "  # the logits, for the \"next token\" prediction.\n",
        "  num_image_tokens = model.num_image_tokens\n",
        "  num_action_tokens = model.num_action_tokens\n",
        "  time_step_tokens = num_image_tokens + num_action_tokens\n",
        "  logits = jnp.reshape(logits, (bs, seqlen, time_step_tokens, vocab_size))\n",
        "  logits = logits[:, :, num_image_tokens - 1 : -1]\n",
        "\n",
        "  logp = jax.nn.log_softmax(logits)\n",
        "  loglik = jnp.take_along_axis(logp, labels, axis=-1)\n",
        "  loglik = jnp.mean(loglik, axis=(1, 2, 3))\n",
        "\n",
        "  return -loglik, new_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N9PgzbdS2mEN"
      },
      "outputs": [],
      "source": [
        "# @title Set up the functions for training\n",
        "\n",
        "optimizer = optax.adam(learning_rate=1e-4, eps=1e-7)\n",
        "\n",
        "# Create the train state.\n",
        "# input: batch, rng, ds_info\n",
        "# output: state\n",
        "agent_create_train_state = functools.partial(\n",
        "    create_train_state, model=rt1x_model, optimizer=optimizer\n",
        ")\n",
        "create_train_state_jit = jax.jit(\n",
        "    agent_create_train_state,\n",
        "    out_shardings=replicate_sharding,\n",
        ")\n",
        "\n",
        "global_data_shape = jax.tree_map(\n",
        "    lambda x: (global_batch_size,) + x.shape[1:], sample_batch\n",
        ")\n",
        "\n",
        "local_devices = mesh.local_devices\n",
        "local_device_count = jax.local_device_count()\n",
        "\n",
        "def _put_to_devices(x):\n",
        "  per_device_arrays = np.split(x, local_device_count, axis=0)\n",
        "  return jax.device_put(per_device_arrays, local_devices)\n",
        "\n",
        "def _form_gda(local_data, global_shape):\n",
        "  arrays = _put_to_devices(local_data)\n",
        "  return jax.make_array_from_single_device_arrays(\n",
        "      global_shape, sharding, arrays\n",
        "  )\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "sample_batch = jax.tree_map(_form_gda, sample_batch, global_data_shape)\n",
        "rng, agent_rng = jax.random.split(rng)\n",
        "state = create_train_state_jit(\n",
        "    batch=sample_batch, rng=agent_rng\n",
        ")\n",
        "\n",
        "# Create the train step.\n",
        "agent_train = functools.partial(train, model=rt1x_model, optimizer=optimizer)\n",
        "jitted_train_step = jax.jit(\n",
        "    agent_train,\n",
        "    out_shardings=(replicate_sharding, replicate_sharding),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ3mdV4D5C9c"
      },
      "outputs": [],
      "source": [
        "# @title Run the train loop\n",
        "\n",
        "num_train_steps = 1_000_000  # 1k for example, actual should be > 1M\n",
        "log_loss_every_steps = 10\n",
        "epoch_count = 1\n",
        "epoch_step = 1\n",
        "\n",
        "\n",
        "# The state should be resharded since we may have loaded pretrained weights\n",
        "# that need to be converted to jax.Arrays.\n",
        "state_repl = reshard(state, shardings=replicate_sharding)\n",
        "# The RNG must be replicated.\n",
        "rng_repl = reshard(rng, shardings=replicate_sharding)\n",
        "\n",
        "train_iter = get_new_iterator()\n",
        "\n",
        "for step in range(num_train_steps):\n",
        "  is_last_step = step == num_train_steps\n",
        "\n",
        "  rng_repl = jax.random.fold_in(rng_repl, step)\n",
        "\n",
        "  # check if there is next in train_iter\n",
        "  try:\n",
        "    batch = next(train_iter)\n",
        "    batch = jax.tree_map(_form_gda, batch, global_data_shape)\n",
        "    epoch_step += 1\n",
        "  except StopIteration:\n",
        "    train_iter = get_new_iterator()\n",
        "    epoch_count += 1\n",
        "    epoch_step = 1\n",
        "    batch = next(train_iter)\n",
        "    batch = jax.tree_map(_form_gda, batch, global_data_shape)\n",
        "\n",
        "  state_repl, metrics_update = jitted_train_step(\n",
        "      state=state_repl, batch=batch, rng=rng_repl\n",
        "  )\n",
        "\n",
        "  if step % log_loss_every_steps == 0 or is_last_step:\n",
        "    metrics_update = jax.device_get(metrics_update)\n",
        "    print(f\"Metrics: step={step} ({epoch_step}), epoch={epoch_count} {metrics_update}\")\n",
        "\n",
        "  if step % 10_000 == 0:\n",
        "    # Save the current state.\n",
        "    checkpoint_path = f'/home/jonathan/Thesis/open_x_embodiment/custom_rt1x_checkpoint_step{step}'\n",
        "    checkpoints.save_checkpoint(ckpt_dir=checkpoint_path, target=state_repl, step=step, overwrite=True)\n",
        "    print(f\"Saved checkpoint at step {step}\")\n",
        "\n",
        "# Save the final trained state\n",
        "checkpoint_path = '/home/jonathan/Thesis/open_x_embodiment/custom_rt1x_checkpoint_final'\n",
        "checkpoints.save_checkpoint(ckpt_dir=checkpoint_path, target=state_repl, step=num_train_steps, overwrite=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "provenance": [
        {
          "file_id": "1pTVYsQBR0dBJrfDJ1A3p143ZRrSNasxn",
          "timestamp": 1702073007553
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
